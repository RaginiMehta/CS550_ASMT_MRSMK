{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Bnpd7Zh0NPl2"
   },
   "source": [
    "## **Medical Report Summarisation using Medical Knowledge**\n",
    "\n",
    "### **References**\n",
    "\n",
    "**Main Reference**\n",
    "- Radiology report generation with medical knowledge and multilevel image-report alignment: A new method and its verification\n",
    "https://www.sciencedirect.com/science/article/pii/S0933365723002282#bib1\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oKdcyk1sMEtD"
   },
   "source": [
    "## **Data Collection**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p81rfgckMR9a"
   },
   "source": [
    "### **Collect Datasets**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AYR_B6u1ojJt",
    "outputId": "faaac1ed-b54b-4cfd-b3e4-cca51372318e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: Pillow in /home/raginivi/anaconda3/lib/python3.12/site-packages (10.3.0)\n",
      "Requirement already satisfied: torchvision in /home/raginivi/anaconda3/lib/python3.12/site-packages (0.19.1)\n",
      "Requirement already satisfied: numpy in /home/raginivi/anaconda3/lib/python3.12/site-packages (from torchvision) (1.26.4)\n",
      "Requirement already satisfied: torch==2.4.1 in /home/raginivi/anaconda3/lib/python3.12/site-packages (from torchvision) (2.4.1)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /home/raginivi/anaconda3/lib/python3.12/site-packages (from torchvision) (10.3.0)\n",
      "Requirement already satisfied: filelock in /home/raginivi/anaconda3/lib/python3.12/site-packages (from torch==2.4.1->torchvision) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /home/raginivi/anaconda3/lib/python3.12/site-packages (from torch==2.4.1->torchvision) (4.11.0)\n",
      "Requirement already satisfied: sympy in /home/raginivi/anaconda3/lib/python3.12/site-packages (from torch==2.4.1->torchvision) (1.12)\n",
      "Requirement already satisfied: networkx in /home/raginivi/anaconda3/lib/python3.12/site-packages (from torch==2.4.1->torchvision) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /home/raginivi/anaconda3/lib/python3.12/site-packages (from torch==2.4.1->torchvision) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /home/raginivi/anaconda3/lib/python3.12/site-packages (from torch==2.4.1->torchvision) (2024.3.1)\n",
      "Requirement already satisfied: setuptools in /home/raginivi/anaconda3/lib/python3.12/site-packages (from torch==2.4.1->torchvision) (69.5.1)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /home/raginivi/anaconda3/lib/python3.12/site-packages (from torch==2.4.1->torchvision) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /home/raginivi/anaconda3/lib/python3.12/site-packages (from torch==2.4.1->torchvision) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /home/raginivi/anaconda3/lib/python3.12/site-packages (from torch==2.4.1->torchvision) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /home/raginivi/anaconda3/lib/python3.12/site-packages (from torch==2.4.1->torchvision) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /home/raginivi/anaconda3/lib/python3.12/site-packages (from torch==2.4.1->torchvision) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /home/raginivi/anaconda3/lib/python3.12/site-packages (from torch==2.4.1->torchvision) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /home/raginivi/anaconda3/lib/python3.12/site-packages (from torch==2.4.1->torchvision) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /home/raginivi/anaconda3/lib/python3.12/site-packages (from torch==2.4.1->torchvision) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /home/raginivi/anaconda3/lib/python3.12/site-packages (from torch==2.4.1->torchvision) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /home/raginivi/anaconda3/lib/python3.12/site-packages (from torch==2.4.1->torchvision) (2.20.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /home/raginivi/anaconda3/lib/python3.12/site-packages (from torch==2.4.1->torchvision) (12.1.105)\n",
      "Requirement already satisfied: triton==3.0.0 in /home/raginivi/anaconda3/lib/python3.12/site-packages (from torch==2.4.1->torchvision) (3.0.0)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /home/raginivi/anaconda3/lib/python3.12/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch==2.4.1->torchvision) (12.6.77)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/raginivi/anaconda3/lib/python3.12/site-packages (from jinja2->torch==2.4.1->torchvision) (2.1.3)\n",
      "Requirement already satisfied: mpmath>=0.19 in /home/raginivi/anaconda3/lib/python3.12/site-packages (from sympy->torch==2.4.1->torchvision) (1.3.0)\n",
      "Requirement already satisfied: nltk in /home/raginivi/anaconda3/lib/python3.12/site-packages (3.8.1)\n",
      "Requirement already satisfied: click in /home/raginivi/anaconda3/lib/python3.12/site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in /home/raginivi/anaconda3/lib/python3.12/site-packages (from nltk) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /home/raginivi/anaconda3/lib/python3.12/site-packages (from nltk) (2023.10.3)\n",
      "Requirement already satisfied: tqdm in /home/raginivi/anaconda3/lib/python3.12/site-packages (from nltk) (4.66.4)\n",
      "Requirement already satisfied: pyspellchecker in /home/raginivi/anaconda3/lib/python3.12/site-packages (0.8.1)\n",
      "Requirement already satisfied: tqdm in /home/raginivi/anaconda3/lib/python3.12/site-packages (4.66.4)\n",
      "Requirement already satisfied: opencv-python in /home/raginivi/anaconda3/lib/python3.12/site-packages (4.10.0.84)\n",
      "Requirement already satisfied: numpy>=1.21.2 in /home/raginivi/anaconda3/lib/python3.12/site-packages (from opencv-python) (1.26.4)\n",
      "Requirement already satisfied: transformers in /home/raginivi/anaconda3/lib/python3.12/site-packages (4.45.2)\n",
      "Requirement already satisfied: filelock in /home/raginivi/anaconda3/lib/python3.12/site-packages (from transformers) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /home/raginivi/anaconda3/lib/python3.12/site-packages (from transformers) (0.25.2)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/raginivi/anaconda3/lib/python3.12/site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/raginivi/anaconda3/lib/python3.12/site-packages (from transformers) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/raginivi/anaconda3/lib/python3.12/site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/raginivi/anaconda3/lib/python3.12/site-packages (from transformers) (2023.10.3)\n",
      "Requirement already satisfied: requests in /home/raginivi/anaconda3/lib/python3.12/site-packages (from transformers) (2.32.2)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /home/raginivi/anaconda3/lib/python3.12/site-packages (from transformers) (0.4.5)\n",
      "Requirement already satisfied: tokenizers<0.21,>=0.20 in /home/raginivi/anaconda3/lib/python3.12/site-packages (from transformers) (0.20.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/raginivi/anaconda3/lib/python3.12/site-packages (from transformers) (4.66.4)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/raginivi/anaconda3/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.3.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/raginivi/anaconda3/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.11.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/raginivi/anaconda3/lib/python3.12/site-packages (from requests->transformers) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/raginivi/anaconda3/lib/python3.12/site-packages (from requests->transformers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/raginivi/anaconda3/lib/python3.12/site-packages (from requests->transformers) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/raginivi/anaconda3/lib/python3.12/site-packages (from requests->transformers) (2024.8.30)\n"
     ]
    }
   ],
   "source": [
    "'''Libraries Installation and Import'''\n",
    "\n",
    "# install necessary libraries\n",
    "!pip install Pillow\n",
    "!pip install torchvision\n",
    "!pip install nltk\n",
    "!pip install pyspellchecker\n",
    "!pip install tqdm\n",
    "!pip install opencv-python\n",
    "!pip install transformers\n",
    "\n",
    "\n",
    "\n",
    "# importing required libraries\n",
    "import os\n",
    "import re\n",
    "import requests\n",
    "import tarfile\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xml.etree.ElementTree as ET\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "import cv2\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from spellchecker import SpellChecker\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.models as models\n",
    "from torchvision import datasets\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from transformers import BertTokenizer, BertModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "49-0xiXplwH8",
    "outputId": "ea63eda3-908a-4f5f-eeec-a4019130e57f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://openi.nlm.nih.gov/imgs/collections/NLMCXR_png.tgz already exists at: ./datasets/iu_xray/images\n",
      "https://openi.nlm.nih.gov/imgs/collections/NLMCXR_reports.tgz already exists at: ./datasets/iu_xray/reports\n"
     ]
    }
   ],
   "source": [
    "'''Setup - Generalized'''\n",
    "\n",
    "# setup to download the IU X-Ray Dataset\n",
    "dataset = 'iu_xray/'\n",
    "download_path = os.path.join('./datasets', dataset)\n",
    "\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')\n",
    "# download_path = os.path.join('/content/drive/Othercomputers/My Laptop/CS550_ASMT_MRSMK/datasets', dataset)\n",
    "# download_path = os.path.join('/content/drive/MyDrive/Academics/CS550 Machine Learning/CS550 ASMT MRSMK/datasets', dataset)\n",
    "\n",
    "images_dir = os.path.join(download_path, \"images\")\n",
    "reports_dir = os.path.join(download_path, \"reports\")\n",
    "\n",
    "images_url = \"https://openi.nlm.nih.gov/imgs/collections/NLMCXR_png.tgz\"\n",
    "reports_url = \"https://openi.nlm.nih.gov/imgs/collections/NLMCXR_reports.tgz\"\n",
    "\n",
    "\n",
    "# function to check the file size of a given URL\n",
    "def get_file_size(url):\n",
    "    response = requests.head(url)\n",
    "    size_in_bytes = int(response.headers.get('Content-Length', 0))\n",
    "    size_in_mb = size_in_bytes / (1024 * 1024)\n",
    "    return size_in_mb\n",
    "\n",
    "\n",
    "# function to download and extract from a given url to a given directory\n",
    "def download_and_extract(url, save_dir):\n",
    "    file_name = url.split('/')[-1]\n",
    "    file_path = os.path.join(save_dir, file_name)\n",
    "\n",
    "    response = requests.get(url, stream=True)\n",
    "    total_size = int(response.headers.get('Content-Length', 0))\n",
    "    downloaded_size = 0\n",
    "\n",
    "    with open(file_path, 'wb') as file:\n",
    "        for chunk in response.iter_content(chunk_size=1024):\n",
    "            if chunk:\n",
    "                file.write(chunk)\n",
    "                downloaded_size += len(chunk)\n",
    "                percent_complete = (downloaded_size / total_size) * 100\n",
    "                print(f\"Downloaded {downloaded_size / (1024*1024):.2f} MB out of {total_size / (1024*1024):.2f} MB: {percent_complete:.2f}% complete\")\n",
    "\n",
    "    print(\"\\nDownload complete!\")\n",
    "\n",
    "    with tarfile.open(file_path, 'r:gz') as tar:\n",
    "        members = tar.getmembers()\n",
    "        total_files = len(members)\n",
    "\n",
    "        for idx, member in enumerate(members, start=1):\n",
    "            tar.extract(member, path=save_dir)\n",
    "            print(f\"Extracting File {idx} out of {total_files}: {member.name}\")\n",
    "\n",
    "    os.remove(file_path)\n",
    "\n",
    "\n",
    "# downloading  IU X-Ray dataset\n",
    "if not os.path.exists(images_dir):\n",
    "    images_size = get_file_size(images_url)\n",
    "    print(f\"Downloading {images_url} to: {images_dir} ({images_size:.2f} MB)\")\n",
    "    os.makedirs(images_dir, exist_ok=True)\n",
    "    download_and_extract(images_url, images_dir)\n",
    "    print(f\"Downloaded {images_url} to: {images_dir}\")\n",
    "else:\n",
    "    print(f\"{images_url} already exists at: {images_dir}\")\n",
    "\n",
    "if not os.path.exists(reports_dir):\n",
    "    reports_size = get_file_size(reports_url)\n",
    "    print(f\"Downloading {reports_url} to: {reports_dir} ({reports_size:.2f} MB)\")\n",
    "    os.makedirs(reports_dir, exist_ok=True)\n",
    "    download_and_extract(reports_url, reports_dir)\n",
    "    print(f\"Downloaded {reports_url} to: {reports_dir}\")\n",
    "else:\n",
    "    print(f\"{reports_url} already exists at: {reports_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5ZSA8Jyowaoh",
    "outputId": "dffa4899-00a7-4d49-a118-8dd85400ba06"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Path:  ./datasets/iu_xray/\n",
      "Directory Contents: ['images', 'reports', 'iu_xray_images_df.csv', 'iu_xray_data.csv', 'iu_xray_reports_df_preprocessed.csv', 'images_preprocessed.tar.xz', 'iu_xray_reports_df.csv', 'images_preprocessed']\n",
      "\n",
      "Path:  ./datasets/iu_xray/images\n",
      "Directory Contents: 7471 Images\n",
      "\n",
      "Path:  ./datasets/iu_xray/reports/ecgen-radiology\n",
      "Directory Contents: 3955 Reports\n"
     ]
    }
   ],
   "source": [
    "'''Exploring the IU X-Ray Dataset Contents'''\n",
    "\n",
    "# displaying directory and subdirectory contents\n",
    "iu_xray = download_path\n",
    "print(\"\\nPath: \", iu_xray)\n",
    "print(f\"Directory Contents: {os.listdir(iu_xray)}\")\n",
    "\n",
    "iu_xray_images = images_dir\n",
    "print(\"\\nPath: \", iu_xray_images)\n",
    "print(f\"Directory Contents: {len(os.listdir(iu_xray_images))} Images\")\n",
    "\n",
    "iu_xray_reports = os.path.join(reports_dir, 'ecgen-radiology')\n",
    "print(\"\\nPath: \", iu_xray_reports)\n",
    "print(f\"Directory Contents: {len(os.listdir(iu_xray_reports))} Reports\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 822
    },
    "id": "qGczexPLUaN5",
    "outputId": "8c7528a1-7064-4802-c276-edef004f0d5b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataframe already exists at ./datasets/iu_xray/iu_xray_images_df.csv\n",
      "\n",
      "\n",
      "Dataframe Shape: (7470, 9)\n",
      "\n",
      "\n",
      "Dataframe Information:\n",
      "\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 7470 entries, 0 to 7469\n",
      "Data columns (total 9 columns):\n",
      " #   Column          Non-Null Count  Dtype \n",
      "---  ------          --------------  ----- \n",
      " 0   pmc_id          7470 non-null   int64 \n",
      " 1   image_filename  7470 non-null   object\n",
      " 2   caption         7468 non-null   object\n",
      " 3   comparison      5210 non-null   object\n",
      " 4   indication      7311 non-null   object\n",
      " 5   findings        6473 non-null   object\n",
      " 6   impression      7418 non-null   object\n",
      " 7   height          7470 non-null   int64 \n",
      " 8   width           7470 non-null   int64 \n",
      "dtypes: int64(3), object(6)\n",
      "memory usage: 525.4+ KB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Displaying Dataframe:\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pmc_id</th>\n",
       "      <th>image_filename</th>\n",
       "      <th>caption</th>\n",
       "      <th>comparison</th>\n",
       "      <th>indication</th>\n",
       "      <th>findings</th>\n",
       "      <th>impression</th>\n",
       "      <th>height</th>\n",
       "      <th>width</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>779</td>\n",
       "      <td>CXR779_IM-2321-1001.png</td>\n",
       "      <td>Radiographs of the chest, 2 views, dated XXXX,...</td>\n",
       "      <td>CT chest, dated XXXX, XXXX.</td>\n",
       "      <td>XXXX-year-old female. Pain after XXXX.</td>\n",
       "      <td>The cardiomediastinal silhouette is normal in ...</td>\n",
       "      <td>Negative for acute abnormality.</td>\n",
       "      <td>420</td>\n",
       "      <td>512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>779</td>\n",
       "      <td>CXR779_IM-2321-2001.png</td>\n",
       "      <td>Radiographs of the chest, 2 views, dated XXXX,...</td>\n",
       "      <td>CT chest, dated XXXX, XXXX.</td>\n",
       "      <td>XXXX-year-old female. Pain after XXXX.</td>\n",
       "      <td>The cardiomediastinal silhouette is normal in ...</td>\n",
       "      <td>Negative for acute abnormality.</td>\n",
       "      <td>624</td>\n",
       "      <td>512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1102</td>\n",
       "      <td>CXR1102_IM-0069-12012.png</td>\n",
       "      <td>AP and lateral views of the chest dated XXXX, ...</td>\n",
       "      <td>XXXX, XXXX.</td>\n",
       "      <td>Shortness of breath. Unable to XXXX XXXX for l...</td>\n",
       "      <td>There is stable cardiomegaly with XXXX pulmona...</td>\n",
       "      <td>1. Cardiomegaly, vascular congestion and proba...</td>\n",
       "      <td>420</td>\n",
       "      <td>512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1102</td>\n",
       "      <td>CXR1102_IM-0069-2001.png</td>\n",
       "      <td>AP and lateral views of the chest dated XXXX, ...</td>\n",
       "      <td>XXXX, XXXX.</td>\n",
       "      <td>Shortness of breath. Unable to XXXX XXXX for l...</td>\n",
       "      <td>There is stable cardiomegaly with XXXX pulmona...</td>\n",
       "      <td>1. Cardiomegaly, vascular congestion and proba...</td>\n",
       "      <td>512</td>\n",
       "      <td>512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1102</td>\n",
       "      <td>CXR1102_IM-0069-3001.png</td>\n",
       "      <td>AP and lateral views of the chest dated XXXX, ...</td>\n",
       "      <td>XXXX, XXXX.</td>\n",
       "      <td>Shortness of breath. Unable to XXXX XXXX for l...</td>\n",
       "      <td>There is stable cardiomegaly with XXXX pulmona...</td>\n",
       "      <td>1. Cardiomegaly, vascular congestion and proba...</td>\n",
       "      <td>512</td>\n",
       "      <td>512</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   pmc_id             image_filename  \\\n",
       "0     779    CXR779_IM-2321-1001.png   \n",
       "1     779    CXR779_IM-2321-2001.png   \n",
       "2    1102  CXR1102_IM-0069-12012.png   \n",
       "3    1102   CXR1102_IM-0069-2001.png   \n",
       "4    1102   CXR1102_IM-0069-3001.png   \n",
       "\n",
       "                                             caption  \\\n",
       "0  Radiographs of the chest, 2 views, dated XXXX,...   \n",
       "1  Radiographs of the chest, 2 views, dated XXXX,...   \n",
       "2  AP and lateral views of the chest dated XXXX, ...   \n",
       "3  AP and lateral views of the chest dated XXXX, ...   \n",
       "4  AP and lateral views of the chest dated XXXX, ...   \n",
       "\n",
       "                    comparison  \\\n",
       "0  CT chest, dated XXXX, XXXX.   \n",
       "1  CT chest, dated XXXX, XXXX.   \n",
       "2                  XXXX, XXXX.   \n",
       "3                  XXXX, XXXX.   \n",
       "4                  XXXX, XXXX.   \n",
       "\n",
       "                                          indication  \\\n",
       "0             XXXX-year-old female. Pain after XXXX.   \n",
       "1             XXXX-year-old female. Pain after XXXX.   \n",
       "2  Shortness of breath. Unable to XXXX XXXX for l...   \n",
       "3  Shortness of breath. Unable to XXXX XXXX for l...   \n",
       "4  Shortness of breath. Unable to XXXX XXXX for l...   \n",
       "\n",
       "                                            findings  \\\n",
       "0  The cardiomediastinal silhouette is normal in ...   \n",
       "1  The cardiomediastinal silhouette is normal in ...   \n",
       "2  There is stable cardiomegaly with XXXX pulmona...   \n",
       "3  There is stable cardiomegaly with XXXX pulmona...   \n",
       "4  There is stable cardiomegaly with XXXX pulmona...   \n",
       "\n",
       "                                          impression  height  width  \n",
       "0                    Negative for acute abnormality.     420    512  \n",
       "1                    Negative for acute abnormality.     624    512  \n",
       "2  1. Cardiomegaly, vascular congestion and proba...     420    512  \n",
       "3  1. Cardiomegaly, vascular congestion and proba...     512    512  \n",
       "4  1. Cardiomegaly, vascular congestion and proba...     512    512  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "'''Processing Textual Data from each .xml Report File and Storing it in a .csv File'''\n",
    "\n",
    "# function to iterate through all .xml report files and storing them in a dataframe\n",
    "def save_images_df():\n",
    "    data = []\n",
    "    cnt = 0\n",
    "    for file in os.listdir(iu_xray_reports):\n",
    "        if file.endswith(\".xml\"):\n",
    "            cnt += 1\n",
    "            print(f\"Processing .xml File {cnt} out of {len(os.listdir(iu_xray_reports))}: {file}\")\n",
    "\n",
    "            file_path = os.path.join(iu_xray_reports, file)\n",
    "            try:\n",
    "                tree = ET.parse(file_path)\n",
    "                root = tree.getroot()\n",
    "\n",
    "                pmc_id = root.find('.//pmcId').attrib.get('id')\n",
    "\n",
    "                comparison = indication = findings = impression = None\n",
    "\n",
    "                for abstract in root.findall('.//AbstractText'):\n",
    "                    if abstract.attrib.get('Label') == 'COMPARISON':\n",
    "                        comparison = abstract.text\n",
    "                    elif abstract.attrib.get('Label') == 'INDICATION':\n",
    "                        indication = abstract.text\n",
    "                    elif abstract.attrib.get('Label') == 'FINDINGS':\n",
    "                        findings = abstract.text\n",
    "                    elif abstract.attrib.get('Label') == 'IMPRESSION':\n",
    "                        impression = abstract.text\n",
    "\n",
    "                for parent_image in root.findall('parentImage'):\n",
    "                    image_file = parent_image.attrib['id'] + \".png\"\n",
    "                    image_path = os.path.join(iu_xray_images, image_file)\n",
    "                    image = cv2.imread(image_path)\n",
    "\n",
    "                    if image is not None:\n",
    "                        height, width, channels = image.shape\n",
    "                        caption = parent_image.find('caption').text if parent_image.find('caption') is not None else None\n",
    "                        data.append([pmc_id, image_file, caption, comparison, indication, findings, impression, height, width])\n",
    "                    else:\n",
    "                        print(f\"Warning: Unable to read image {image_path}\")\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing file {file}: {e}\")\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "# create a dataframe and save it as csv\n",
    "iu_xray_images_df_path = os.path.join(iu_xray, 'iu_xray_images_df.csv')\n",
    "if not os.path.exists(iu_xray_images_df_path):\n",
    "    data = save_images_df()\n",
    "    columns = ['pmc_id', 'image_filename', 'caption', 'comparison', 'indication', 'findings', 'impression', 'height', 'width']\n",
    "    iu_xray_images_df = pd.DataFrame(data, columns=columns)\n",
    "    iu_xray_images_df.to_csv(iu_xray_images_df_path, index=False)\n",
    "    print(f\"Dataframe saved to {iu_xray_images_df_path}\")\n",
    "else:\n",
    "    print(f\"Dataframe already exists at {iu_xray_images_df_path}\")\n",
    "    iu_xray_images_df = pd.read_csv(iu_xray_images_df_path)\n",
    "\n",
    "\n",
    "# display the stored dataframe\n",
    "print(\"\\n\\nDataframe Shape:\", iu_xray_images_df.shape)\n",
    "\n",
    "print(\"\\n\\nDataframe Information:\\n\")\n",
    "display(iu_xray_images_df.info())\n",
    "\n",
    "print(\"\\n\\nDisplaying Dataframe:\\n\")\n",
    "display(iu_xray_images_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 893
    },
    "id": "vYnfzXXT0O6w",
    "outputId": "d2692046-ebe0-45ac-8291-37641f6d5958"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataframe already exists at ./datasets/iu_xray/iu_xray_reports_df.csv\n",
      "\n",
      "\n",
      "Dataframe Shape: (3955, 11)\n",
      "\n",
      "\n",
      "Dataframe Information:\n",
      "\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 3955 entries, 0 to 3954\n",
      "Data columns (total 11 columns):\n",
      " #   Column       Non-Null Count  Dtype \n",
      "---  ------       --------------  ----- \n",
      " 0   pmc_id       3955 non-null   int64 \n",
      " 1   findings     3425 non-null   object\n",
      " 2   impression   3921 non-null   object\n",
      " 3   comparison   2757 non-null   object\n",
      " 4   indication   3865 non-null   object\n",
      " 5   image_count  3955 non-null   int64 \n",
      " 6   image_1      3851 non-null   object\n",
      " 7   image_2      3405 non-null   object\n",
      " 8   image_3      197 non-null    object\n",
      " 9   image_4      16 non-null     object\n",
      " 10  image_5      1 non-null      object\n",
      "dtypes: int64(2), object(9)\n",
      "memory usage: 340.0+ KB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Displaying Dataframe:\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pmc_id</th>\n",
       "      <th>findings</th>\n",
       "      <th>impression</th>\n",
       "      <th>comparison</th>\n",
       "      <th>indication</th>\n",
       "      <th>image_count</th>\n",
       "      <th>image_1</th>\n",
       "      <th>image_2</th>\n",
       "      <th>image_3</th>\n",
       "      <th>image_4</th>\n",
       "      <th>image_5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>941</td>\n",
       "      <td>Surgical XXXX at the distal left clavicle. No ...</td>\n",
       "      <td>1. No acute radiographic cardiopulmonary process.</td>\n",
       "      <td>Chest radiograph XXXX.</td>\n",
       "      <td>XXXX-year-old XXXX with chest pain</td>\n",
       "      <td>2</td>\n",
       "      <td>CXR941_IM-2438-1001.jpg: PA and lateral chest ...</td>\n",
       "      <td>CXR941_IM-2438-2001.jpg: PA and lateral chest ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>554</td>\n",
       "      <td>The cardiomediastinal silhouette is normal in ...</td>\n",
       "      <td>Negative.</td>\n",
       "      <td>XXXX, XXXX.</td>\n",
       "      <td>XXXX-year-old female. Dyspnea.</td>\n",
       "      <td>2</td>\n",
       "      <td>CXR554_IM-2155-1001.jpg: Xray Chest PA and Lat...</td>\n",
       "      <td>CXR554_IM-2155-2001.jpg: Xray Chest PA and Lat...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>733</td>\n",
       "      <td>There is a dual-lumen right internal jugular c...</td>\n",
       "      <td>1. Distal tip of the dual-lumen right internal...</td>\n",
       "      <td>Multiple chest x-XXXX, the most recent dated X...</td>\n",
       "      <td>History of central line placement.</td>\n",
       "      <td>2</td>\n",
       "      <td>CXR733_IM-2293-0001-0001.jpg: PA and lateral c...</td>\n",
       "      <td>CXR733_IM-2293-0001-0002.jpg: PA and lateral c...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3802</td>\n",
       "      <td>The cardiomediastinal silhouette is normal siz...</td>\n",
       "      <td>No acute cardiopulmonary disease. .</td>\n",
       "      <td>NaN</td>\n",
       "      <td>XXXX x2 weeks</td>\n",
       "      <td>2</td>\n",
       "      <td>CXR3802_IM-1912-1001-0001.jpg: Xray Chest PA a...</td>\n",
       "      <td>CXR3802_IM-1912-1001-0002.jpg: Xray Chest PA a...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2190</td>\n",
       "      <td>Lungs are clear. No pleural effusions or pneum...</td>\n",
       "      <td>Clear lungs.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>XXXX</td>\n",
       "      <td>2</td>\n",
       "      <td>CXR2190_IM-0800-2001.jpg: PA and lateral views...</td>\n",
       "      <td>CXR2190_IM-0800-3001.jpg: PA and lateral views...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   pmc_id                                           findings  \\\n",
       "0     941  Surgical XXXX at the distal left clavicle. No ...   \n",
       "1     554  The cardiomediastinal silhouette is normal in ...   \n",
       "2     733  There is a dual-lumen right internal jugular c...   \n",
       "3    3802  The cardiomediastinal silhouette is normal siz...   \n",
       "4    2190  Lungs are clear. No pleural effusions or pneum...   \n",
       "\n",
       "                                          impression  \\\n",
       "0  1. No acute radiographic cardiopulmonary process.   \n",
       "1                                          Negative.   \n",
       "2  1. Distal tip of the dual-lumen right internal...   \n",
       "3                No acute cardiopulmonary disease. .   \n",
       "4                                       Clear lungs.   \n",
       "\n",
       "                                          comparison  \\\n",
       "0                             Chest radiograph XXXX.   \n",
       "1                                        XXXX, XXXX.   \n",
       "2  Multiple chest x-XXXX, the most recent dated X...   \n",
       "3                                                NaN   \n",
       "4                                                NaN   \n",
       "\n",
       "                           indication  image_count  \\\n",
       "0  XXXX-year-old XXXX with chest pain            2   \n",
       "1      XXXX-year-old female. Dyspnea.            2   \n",
       "2  History of central line placement.            2   \n",
       "3                       XXXX x2 weeks            2   \n",
       "4                                XXXX            2   \n",
       "\n",
       "                                             image_1  \\\n",
       "0  CXR941_IM-2438-1001.jpg: PA and lateral chest ...   \n",
       "1  CXR554_IM-2155-1001.jpg: Xray Chest PA and Lat...   \n",
       "2  CXR733_IM-2293-0001-0001.jpg: PA and lateral c...   \n",
       "3  CXR3802_IM-1912-1001-0001.jpg: Xray Chest PA a...   \n",
       "4  CXR2190_IM-0800-2001.jpg: PA and lateral views...   \n",
       "\n",
       "                                             image_2 image_3 image_4 image_5  \n",
       "0  CXR941_IM-2438-2001.jpg: PA and lateral chest ...     NaN     NaN     NaN  \n",
       "1  CXR554_IM-2155-2001.jpg: Xray Chest PA and Lat...     NaN     NaN     NaN  \n",
       "2  CXR733_IM-2293-0001-0002.jpg: PA and lateral c...     NaN     NaN     NaN  \n",
       "3  CXR3802_IM-1912-1001-0002.jpg: Xray Chest PA a...     NaN     NaN     NaN  \n",
       "4  CXR2190_IM-0800-3001.jpg: PA and lateral views...     NaN     NaN     NaN  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "'''Processing Textual Data from each .xml Report File and Storing it in a .csv File'''\n",
    "\n",
    "# function to iterate through all .xml report files and storing them in a dataframe\n",
    "def save_reports_df():\n",
    "    data = []\n",
    "    cnt = 0\n",
    "    for file in os.listdir(iu_xray_reports):\n",
    "        if file.endswith(\".xml\"):\n",
    "            cnt += 1\n",
    "            print(f\"Processing .xml File {cnt} out of {len(os.listdir(iu_xray_reports))}: {file}\")\n",
    "\n",
    "            file_path = os.path.join(iu_xray_reports, file)\n",
    "            try:\n",
    "                tree = ET.parse(file_path)\n",
    "                root = tree.getroot()\n",
    "\n",
    "                pmc_id = root.find('.//pmcId').attrib.get('id')\n",
    "\n",
    "                comparison = indication = findings = impression = None\n",
    "\n",
    "                for abstract in root.findall('.//AbstractText'):\n",
    "                    if abstract.attrib.get('Label') == 'COMPARISON':\n",
    "                        comparison = abstract.text\n",
    "                    elif abstract.attrib.get('Label') == 'INDICATION':\n",
    "                        indication = abstract.text\n",
    "                    elif abstract.attrib.get('Label') == 'FINDINGS':\n",
    "                        findings = abstract.text\n",
    "                    elif abstract.attrib.get('Label') == 'IMPRESSION':\n",
    "                        impression = abstract.text\n",
    "\n",
    "                report_data = {\n",
    "                    'pmc_id': pmc_id,\n",
    "                    'findings': findings,\n",
    "                    'impression': impression,\n",
    "                    'comparison': comparison,\n",
    "                    'indication': indication,\n",
    "                }\n",
    "\n",
    "                parent_images = root.findall('parentImage')\n",
    "                report_data['image_count'] = len(parent_images)\n",
    "\n",
    "                for i, parent_image in enumerate(parent_images, start=1):\n",
    "                    image_file = parent_image.attrib['id'] + \".jpg\"\n",
    "                    caption = parent_image.find('caption').text if parent_image.find('caption') is not None else None\n",
    "                    report_data[f'image_{i}'] = f\"{image_file}: {caption}\" if caption else image_file\n",
    "\n",
    "                data.append(report_data)\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing file {file}: {e}\")\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "# create a dataframe and save it as csv\n",
    "iu_xray_reports_df_path = os.path.join(iu_xray, 'iu_xray_reports_df.csv')\n",
    "if not os.path.exists(iu_xray_reports_df_path):\n",
    "    data = save_reports_df()\n",
    "    iu_xray_reports_df = pd.DataFrame(data)\n",
    "    iu_xray_reports_df.to_csv(iu_xray_reports_df_path, index=False)\n",
    "    print(f\"Dataframe saved to {iu_xray_reports_df_path}\")\n",
    "else:\n",
    "    print(f\"Dataframe already exists at {iu_xray_reports_df_path}\")\n",
    "    iu_xray_reports_df = pd.read_csv(iu_xray_reports_df_path)\n",
    "\n",
    "\n",
    "# display the stored dataframe\n",
    "print(\"\\n\\nDataframe Shape:\", iu_xray_reports_df.shape)\n",
    "\n",
    "print(\"\\n\\nDataframe Information:\\n\")\n",
    "display(iu_xray_reports_df.info())\n",
    "\n",
    "print(\"\\n\\nDisplaying Dataframe:\\n\")\n",
    "display(iu_xray_reports_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 310
    },
    "id": "bw4Ylfa94M1o",
    "outputId": "5ef503a0-265c-423b-8cae-1d36b136134d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Number of Images per Report:\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>images_qty</th>\n",
       "      <th>reports_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>3208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>446</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   images_qty  reports_count\n",
       "0           2           3208\n",
       "1           1            446\n",
       "2           3            181\n",
       "3           0            104\n",
       "4           4             15\n",
       "5           5              1"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "'''Displaying the Number of Images per Report'''\n",
    "\n",
    "# displaying the distribution of number of images per report\n",
    "reports_count = iu_xray_reports_df['image_count'].value_counts().rename_axis('images_qty').reset_index(name='reports_count')\n",
    "print(\"\\n\\nNumber of Images per Report:\\n\")\n",
    "display(reports_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6UouFxQwMNeo"
   },
   "source": [
    "## **Data Preprocessing**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-cGf99_CMV47"
   },
   "source": [
    "### **Preprocess Images**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JTClOSxeSCOM",
    "outputId": "811cbc93-70e7-4d60-f213-26238488c44d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessed Images already exist at: ./datasets/iu_xray/images_preprocessed\n"
     ]
    }
   ],
   "source": [
    "'''Preprocessing Images - Resizing, Tensor Conversion and Normalization'''\n",
    "\n",
    "# function to preprocess and save images\n",
    "def preprocess_images(input_dir, output_dir):\n",
    "    preprocess = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]), \n",
    "    ])\n",
    "\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    cnt = 0\n",
    "    for filename in os.listdir(input_dir):\n",
    "        if filename.endswith('.png'):\n",
    "            cnt += 1\n",
    "            print(f\"Preprocessing File {cnt} out of {len(os.listdir(input_dir))}: {filename}\")\n",
    "\n",
    "            image_path = os.path.join(input_dir, filename)\n",
    "            image = Image.open(image_path).convert('RGB')\n",
    "            processed_image = preprocess(image)\n",
    "\n",
    "            processed_image_path = os.path.join(output_dir, filename)\n",
    "\n",
    "            processed_image_pil = transforms.ToPILImage()(processed_image)\n",
    "            processed_image_pil.save(processed_image_path)\n",
    "\n",
    "\n",
    "# preprocessing images\n",
    "iu_xray_images_preprocessed = os.path.join(iu_xray, 'images_preprocessed')\n",
    "if not os.path.exists(iu_xray_images_preprocessed):\n",
    "    print(f\"Preprocessing Images to: {iu_xray_images_preprocessed}\")\n",
    "    preprocess_images(iu_xray_images, iu_xray_images_preprocessed)\n",
    "    print(f\"Preprocessed Images saved to: {iu_xray_images_preprocessed}\")\n",
    "else:\n",
    "    print(f\"Preprocessed Images already exist at: {iu_xray_images_preprocessed}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lvdRoNqXMZlN",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### **Preprocess Text**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "id": "M8sW8uz8-plE"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/shivangi/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/shivangi/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/shivangi/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing Text DataFrame ./datasets/iu_xray/iu_xray_reports_df.csv to: ./datasets/iu_xray/iu_xray_reports_preprocessed_df.csv\n",
      "Preprocessing Column: comparison\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▉                                        | 87/3955 [00:04<03:24, 18.91it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[67], line 124\u001b[0m\n\u001b[1;32m    122\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(iu_xray_reports_preprocessed_df_path):\n\u001b[1;32m    123\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPreprocessing Text DataFrame \u001b[39m\u001b[38;5;132;01m{\u001b[39;00miu_xray_reports_df_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m to: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00miu_xray_reports_preprocessed_df_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 124\u001b[0m     iu_xray_reports_preprocessed_df \u001b[38;5;241m=\u001b[39m \u001b[43mpreprocess_and_save_dataframe\u001b[49m\u001b[43m(\u001b[49m\u001b[43miu_xray_reports_df\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43miu_xray_reports_preprocessed_df_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    125\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPreprocessed Text DataFrame \u001b[39m\u001b[38;5;132;01m{\u001b[39;00miu_xray_reports_df_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m saved to: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00miu_xray_reports_preprocessed_df_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    126\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "Cell \u001b[0;32mIn[67], line 113\u001b[0m, in \u001b[0;36mpreprocess_and_save_dataframe\u001b[0;34m(dataframe, path)\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPreprocessing Column: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcolumn\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    112\u001b[0m dataframe\u001b[38;5;241m.\u001b[39mloc[:, column] \u001b[38;5;241m=\u001b[39m dataframe[column]\u001b[38;5;241m.\u001b[39mfillna(fill_value)\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mstr\u001b[39m)\n\u001b[0;32m--> 113\u001b[0m dataframe\u001b[38;5;241m.\u001b[39mloc[:, column] \u001b[38;5;241m=\u001b[39m \u001b[43mpreprocess_text\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataframe\u001b[49m\u001b[43m[\u001b[49m\u001b[43mcolumn\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    114\u001b[0m dataframe\u001b[38;5;241m.\u001b[39mto_csv(path, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPreprocessed Dataframe Column \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcolumn\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m and saved Dataframe to: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[67], line 91\u001b[0m, in \u001b[0;36mpreprocess_text\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m     89\u001b[0m sentence \u001b[38;5;241m=\u001b[39m rem_stop_words(sentence)\n\u001b[1;32m     90\u001b[0m sentence \u001b[38;5;241m=\u001b[39m handle_negations(sentence)\n\u001b[0;32m---> 91\u001b[0m sentence \u001b[38;5;241m=\u001b[39m \u001b[43mcorrect_spelling\u001b[49m\u001b[43m(\u001b[49m\u001b[43msentence\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     92\u001b[0m sentence \u001b[38;5;241m=\u001b[39m rem_extra_spaces(sentence)\n\u001b[1;32m     94\u001b[0m preprocessed\u001b[38;5;241m.\u001b[39mappend(sentence)\n",
      "Cell \u001b[0;32mIn[67], line 69\u001b[0m, in \u001b[0;36mcorrect_spelling\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m     67\u001b[0m corrected \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     68\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m text\u001b[38;5;241m.\u001b[39msplit():\n\u001b[0;32m---> 69\u001b[0m     corrected_word \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(spell\u001b[38;5;241m.\u001b[39mcandidates(word))[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mspell\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcandidates\u001b[49m\u001b[43m(\u001b[49m\u001b[43mword\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01melse\u001b[39;00m word\n\u001b[1;32m     70\u001b[0m     corrected\u001b[38;5;241m.\u001b[39mappend(corrected_word)\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(corrected)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/spellchecker/spellchecker.py:185\u001b[0m, in \u001b[0;36mSpellChecker.candidates\u001b[0;34m(self, word)\u001b[0m\n\u001b[1;32m    183\u001b[0m \u001b[38;5;66;03m# if still not found, use the edit distance 1 to calc edit distance 2\u001b[39;00m\n\u001b[1;32m    184\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_distance \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[0;32m--> 185\u001b[0m     tmp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mknown(\u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__edit_distance_alt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mres\u001b[49m\u001b[43m)\u001b[49m))\n\u001b[1;32m    186\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m tmp:\n\u001b[1;32m    187\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m tmp\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/spellchecker/spellchecker.py:252\u001b[0m, in \u001b[0;36mSpellChecker.__edit_distance_alt\u001b[0;34m(self, words)\u001b[0m\n\u001b[1;32m    250\u001b[0m tmp_words \u001b[38;5;241m=\u001b[39m [ensure_unicode(w) \u001b[38;5;28;01mfor\u001b[39;00m w \u001b[38;5;129;01min\u001b[39;00m words]\n\u001b[1;32m    251\u001b[0m tmp \u001b[38;5;241m=\u001b[39m [w \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_case_sensitive \u001b[38;5;28;01melse\u001b[39;00m w\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;28;01mfor\u001b[39;00m w \u001b[38;5;129;01min\u001b[39;00m tmp_words \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_if_should_check(w)]\n\u001b[0;32m--> 252\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m [e2 \u001b[38;5;28;01mfor\u001b[39;00m e1 \u001b[38;5;129;01min\u001b[39;00m tmp \u001b[38;5;28;01mfor\u001b[39;00m e2 \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mknown(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39medit_distance_1(e1))]\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/spellchecker/spellchecker.py:252\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    250\u001b[0m tmp_words \u001b[38;5;241m=\u001b[39m [ensure_unicode(w) \u001b[38;5;28;01mfor\u001b[39;00m w \u001b[38;5;129;01min\u001b[39;00m words]\n\u001b[1;32m    251\u001b[0m tmp \u001b[38;5;241m=\u001b[39m [w \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_case_sensitive \u001b[38;5;28;01melse\u001b[39;00m w\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;28;01mfor\u001b[39;00m w \u001b[38;5;129;01min\u001b[39;00m tmp_words \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_if_should_check(w)]\n\u001b[0;32m--> 252\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m [e2 \u001b[38;5;28;01mfor\u001b[39;00m e1 \u001b[38;5;129;01min\u001b[39;00m tmp \u001b[38;5;28;01mfor\u001b[39;00m e2 \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mknown\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43medit_distance_1\u001b[49m\u001b[43m(\u001b[49m\u001b[43me1\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m]\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/spellchecker/spellchecker.py:199\u001b[0m, in \u001b[0;36mSpellChecker.known\u001b[0;34m(self, words)\u001b[0m\n\u001b[1;32m    197\u001b[0m tmp_words \u001b[38;5;241m=\u001b[39m [ensure_unicode(w) \u001b[38;5;28;01mfor\u001b[39;00m w \u001b[38;5;129;01min\u001b[39;00m words]\n\u001b[1;32m    198\u001b[0m tmp \u001b[38;5;241m=\u001b[39m [w \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_case_sensitive \u001b[38;5;28;01melse\u001b[39;00m w\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;28;01mfor\u001b[39;00m w \u001b[38;5;129;01min\u001b[39;00m tmp_words]\n\u001b[0;32m--> 199\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m {w \u001b[38;5;28;01mfor\u001b[39;00m w \u001b[38;5;129;01min\u001b[39;00m tmp \u001b[38;5;28;01mif\u001b[39;00m w \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_word_frequency\u001b[38;5;241m.\u001b[39mdictionary \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_if_should_check(w)}\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/spellchecker/spellchecker.py:199\u001b[0m, in \u001b[0;36m<setcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    197\u001b[0m tmp_words \u001b[38;5;241m=\u001b[39m [ensure_unicode(w) \u001b[38;5;28;01mfor\u001b[39;00m w \u001b[38;5;129;01min\u001b[39;00m words]\n\u001b[1;32m    198\u001b[0m tmp \u001b[38;5;241m=\u001b[39m [w \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_case_sensitive \u001b[38;5;28;01melse\u001b[39;00m w\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;28;01mfor\u001b[39;00m w \u001b[38;5;129;01min\u001b[39;00m tmp_words]\n\u001b[0;32m--> 199\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m {w \u001b[38;5;28;01mfor\u001b[39;00m w \u001b[38;5;129;01min\u001b[39;00m tmp \u001b[38;5;28;01mif\u001b[39;00m w \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_word_frequency\u001b[38;5;241m.\u001b[39mdictionary \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_if_should_check(w)}\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "'''Preprocessing Text - Lowercasing, Decontracting, Punctuation Removal, Number Removal, Two-Letter Word Removal, Stop Word Removal, Negation Handling, Spell Checking, Extra Space Removal'''\n",
    "\n",
    "# download nltk resources and initialize spell checker\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "spell = SpellChecker()\n",
    "\n",
    "\n",
    "# function to convert text to lowercase\n",
    "def lowercase(text):\n",
    "    return text.lower() if isinstance(text, str) else text\n",
    "\n",
    "\n",
    "# function to decontract words\n",
    "def decontracted(text):\n",
    "    if not isinstance(text, str):\n",
    "        return text\n",
    "    contractions = {\n",
    "        \"won't\": \"will not\", \"can't\": \"can not\", \"couldn't\": \"could not\",\n",
    "        \"shouldn't\": \"should not\", \"wouldn't\": \"would not\", \"n't\": \" not\",\n",
    "        \"'re\": \" are\", \"'s\": \" is\", \"'d\": \" would\", \"'ll\": \" will\",\n",
    "        \"'t\": \" not\", \"'ve\": \" have\", \"'m\": \" am\"\n",
    "    }\n",
    "    for contraction, full_form in contractions.items():\n",
    "        text = text.replace(contraction, full_form)\n",
    "    return text\n",
    "\n",
    "\n",
    "# function to remove punctuations\n",
    "def rem_punctuations(text):\n",
    "    return re.sub(r'[^\\w\\s]', '', text) if isinstance(text, str) else text\n",
    "\n",
    "\n",
    "# function to remove numbers\n",
    "def rem_numbers(text):\n",
    "    return re.sub(r'\\d+', '', text) if isinstance(text, str) else text\n",
    "\n",
    "\n",
    "# function to remove two-letter words except \"no\" and \"ct\"\n",
    "def rem_two_letter_words(text):\n",
    "    if not isinstance(text, str):\n",
    "        return text\n",
    "    return ' '.join(word for word in text.split() if len(word) > 2 or word in [\"no\", \"ct\"])\n",
    "\n",
    "\n",
    "# function to remove stop words\n",
    "def rem_stop_words(text):\n",
    "    if not isinstance(text, str):\n",
    "        return text\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    return ' '.join(word for word in text.split() if word not in stop_words)\n",
    "\n",
    "\n",
    "# function to handle negations\n",
    "def handle_negations(text):\n",
    "    if not isinstance(text, str):\n",
    "        return text\n",
    "    negations = {\"no\": \"not\", \"not\": \"not\"}\n",
    "    return ' '.join(negations.get(word, word) for word in text.split())\n",
    "\n",
    "\n",
    "# function to correct spelling\n",
    "def correct_spelling(text):\n",
    "    if not isinstance(text, str):\n",
    "        return text\n",
    "    corrected = []\n",
    "    for word in text.split():\n",
    "        corrected_word = list(spell.candidates(word))[0] if spell.candidates(word) else word\n",
    "        corrected.append(corrected_word)\n",
    "    return ' '.join(corrected)\n",
    "\n",
    "\n",
    "# function to remove extra spaces\n",
    "def rem_extra_spaces(text):\n",
    "    return ' '.join(text.split()) if isinstance(text, str) else text\n",
    "\n",
    "\n",
    "# function to preprocess text\n",
    "def preprocess_text(data):\n",
    "    preprocessed = []\n",
    "    for sentence in tqdm(data.values):\n",
    "        sentence = str(sentence)\n",
    "        sentence = lowercase(sentence)\n",
    "        sentence = decontracted(sentence)\n",
    "        sentence = rem_punctuations(sentence)\n",
    "        sentence = rem_numbers(sentence)\n",
    "        sentence = rem_two_letter_words(sentence)\n",
    "        sentence = rem_stop_words(sentence)\n",
    "        sentence = handle_negations(sentence)\n",
    "        sentence = correct_spelling(sentence)\n",
    "        sentence = rem_extra_spaces(sentence)\n",
    "\n",
    "        preprocessed.append(sentence)\n",
    "\n",
    "    return preprocessed\n",
    "\n",
    "\n",
    "# function to preprocess text and save the corresponding dataframe\n",
    "def preprocess_and_save_dataframe(dataframe, path):\n",
    "    columns_to_preprocess = {\n",
    "        'caption': 'unknown',\n",
    "        'comparison': 'no comparison',\n",
    "        'indication': 'no indication',\n",
    "        'findings': 'no findings',\n",
    "        'impression': 'no impression'\n",
    "    }\n",
    "\n",
    "    for column, fill_value in columns_to_preprocess.items():\n",
    "        if column in dataframe.columns:\n",
    "            print(f\"Preprocessing Column: {column}\")\n",
    "            dataframe.loc[:, column] = dataframe[column].fillna(fill_value).astype(str)\n",
    "            dataframe.loc[:, column] = preprocess_text(dataframe[column])\n",
    "            dataframe.to_csv(path, index=False)\n",
    "            print(f\"Preprocessed Dataframe Column {column} and saved Dataframe to: {path}\")\n",
    "\n",
    "    return dataframe\n",
    "\n",
    "\n",
    "# save and display the preprocessed dataframe for reports\n",
    "iu_xray_reports_preprocessed_df_path = os.path.join(iu_xray, 'iu_xray_reports_preprocessed_df.csv')\n",
    "if not os.path.exists(iu_xray_reports_preprocessed_df_path):\n",
    "    print(f\"Preprocessing Text DataFrame {iu_xray_reports_df_path} to: {iu_xray_reports_preprocessed_df_path}\")\n",
    "    iu_xray_reports_preprocessed_df = preprocess_and_save_dataframe(iu_xray_reports_df, iu_xray_reports_preprocessed_df_path)\n",
    "    print(f\"Preprocessed Text DataFrame {iu_xray_reports_df_path} saved to: {iu_xray_reports_preprocessed_df_path}\")\n",
    "else:\n",
    "    print(f\"Preprocessed Text DataFrame {iu_xray_reports_df_path} already exists at: {iu_xray_reports_preprocessed_df_path}\")\n",
    "    iu_xray_reports_preprocessed_df = pd.read_csv(iu_xray_reports_preprocessed_df_path)\n",
    "display(iu_xray_reports_preprocessed_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MlOl6I0rMa68"
   },
   "source": [
    "### **Create Data Loaders**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "class CustomImageDataset(Dataset):\n",
    "    def __init__(self, image_dir, transform=None):\n",
    "        self.image_dir = image_dir\n",
    "        self.transform = transform\n",
    "        self.image_files = [f for f in os.listdir(image_dir) if f.endswith(('.png', '.jpg', '.jpeg'))]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.image_dir, self.image_files[idx])\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image\n",
    "\n",
    "# Update your load_preprocessed_images function to use CustomImageDataset\n",
    "def load_preprocessed_images(image_dir, batch_size=32):\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),  # Resizing to fit the model input\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ])\n",
    "\n",
    "    dataset = CustomImageDataset(image_dir, transform=transform)\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n",
    "    return dataloader\n",
    "\n",
    "# Load images from preprocessed directory\n",
    "image_dir = iu_xray_images_preprocessed\n",
    "dataloader = load_preprocessed_images(image_dir)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wQ0H-X3HMgS1"
   },
   "source": [
    "## **Model Implementation**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PsELnlXpMjGX"
   },
   "source": [
    "### **Visual Extractor**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/raginivi/anaconda3/lib/python3.12/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/raginivi/anaconda3/lib/python3.12/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet101_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet101_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting features since they are not precomputed...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 234/234 [25:49<00:00,  6.62s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving features to ./datasets/iu_xray/patch_feats.pt\n",
      "Saving features to ./datasets/iu_xray/avg_feats.pt\n",
      "Saving features to ./datasets/iu_xray/final_embeddings.pt\n",
      "Patch Features Shape: torch.Size([7470, 49, 2048])\n",
      "Average Features Shape: torch.Size([7470, 2048])\n",
      "Final Embedding Shape: torch.Size([7470, 50, 2048])\n"
     ]
    }
   ],
   "source": [
    "'''Visual Extractor to Extract Data from Image and Encode it Accordingly'''\n",
    "\n",
    "# define device for gpu support\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "\n",
    "# define the visual extractor model using ResNet101 \n",
    "class VisualExtractor(nn.Module):\n",
    "    def __init__(self, args):\n",
    "        super(VisualExtractor, self).__init__()\n",
    "        self.visual_extractor = args.visual_extractor\n",
    "        self.pretrained = args.visual_extractor_pretrained\n",
    "        model = getattr(models, self.visual_extractor)(pretrained=self.pretrained)\n",
    "        modules = list(model.children())[:-2]  \n",
    "        self.model = nn.Sequential(*modules)\n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.fc_layer = nn.Linear(model.fc.in_features, 2048) \n",
    "        \n",
    "    def forward(self, images):\n",
    "        patch_feats = self.model(images)\n",
    "        avg_feats = self.avg_pool(patch_feats).squeeze() \n",
    "        avg_feats = self.fc_layer(avg_feats)\n",
    "\n",
    "        batch_size, feat_size, _, _ = patch_feats.shape\n",
    "        patch_feats = patch_feats.view(batch_size, feat_size, -1).permute(0, 2, 1)\n",
    "        \n",
    "        final_embedding = torch.cat((avg_feats.unsqueeze(1), patch_feats), dim=1) \n",
    "        \n",
    "        return patch_feats, avg_feats, final_embedding\n",
    "\n",
    "\n",
    "#arguments for the visual extractor\n",
    "class Args:\n",
    "    visual_extractor = 'resnet101' \n",
    "    visual_extractor_pretrained = True\n",
    "\n",
    "\n",
    "#initializing the model\n",
    "args = Args()\n",
    "visual_extractor = VisualExtractor(args).to(device)\n",
    "\n",
    "def load_preprocessed_images(preprocessed_image_dir, batch_size=32):\n",
    "    dataset = datasets.ImageFolder(root=preprocessed_image_dir) \n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n",
    "    return dataloader\n",
    "\n",
    "def extract_features(dataloader):\n",
    "    all_patch_feats = []\n",
    "    all_avg_feats = []\n",
    "    all_final_embeddings = []\n",
    "\n",
    "    visual_extractor.eval() \n",
    "    with torch.no_grad():\n",
    "        for images in tqdm(dataloader):\n",
    "            patch_feats, avg_feats, final_embedding = visual_extractor(images)\n",
    "            all_patch_feats.append(patch_feats.cpu()) \n",
    "            all_avg_feats.append(avg_feats.cpu())\n",
    "            all_final_embeddings.append(final_embedding.cpu())\n",
    "\n",
    "    all_patch_feats = torch.cat(all_patch_feats, dim=0)\n",
    "    all_avg_feats = torch.cat(all_avg_feats, dim=0)\n",
    "    all_final_embeddings = torch.cat(all_final_embeddings, dim=0)\n",
    "\n",
    "    return all_patch_feats, all_avg_feats, all_final_embeddings\n",
    "\n",
    "def save_features(file_path, features):\n",
    "    print(f\"Saving features to {file_path}\")\n",
    "    torch.save(features, file_path)\n",
    "\n",
    "def load_features(file_path):\n",
    "    if os.path.exists(file_path):\n",
    "        print(f\"Loading features from {file_path}\")\n",
    "        return torch.load(file_path)\n",
    "    return None\n",
    "feature_dir=iu_xray\n",
    "patch_feats_file = os.path.join(feature_dir, 'patch_feats.pt')\n",
    "avg_feats_file = os.path.join(feature_dir, 'avg_feats.pt')\n",
    "final_embeddings_file = os.path.join(feature_dir, 'final_embeddings.pt')\n",
    "if os.path.exists(patch_feats_file) and os.path.exists(avg_feats_file) and os.path.exists(final_embeddings_file):\n",
    "    print(\"All features are already precomputed and will be loaded.\")\n",
    "    patch_feats = load_features(patch_feats_file)\n",
    "    avg_feats = load_features(avg_feats_file)\n",
    "    final_embeddings = load_features(final_embeddings_file)\n",
    "else:\n",
    "    print(\"Extracting features since they are not precomputed...\")\n",
    "    patch_feats, avg_feats, final_embeddings = extract_features(dataloader)  # Your extraction function\n",
    "        \n",
    "    # Save features\n",
    "    os.makedirs(feature_dir, exist_ok=True)\n",
    "    save_features(patch_feats_file, patch_feats)\n",
    "    save_features(avg_feats_file, avg_feats)\n",
    "    save_features(final_embeddings_file, final_embeddings)\n",
    "\n",
    "# # extract features\n",
    "# patch_feats, avg_feats, final_embeddings = extract_features(dataloader)\n",
    "\n",
    "\n",
    "# patch_feats, avg_feats, and final_embeddings\n",
    "print(\"Patch Features Shape:\", patch_feats.shape)\n",
    "print(\"Average Features Shape:\", avg_feats.shape)\n",
    "print(\"Final Embedding Shape:\", final_embeddings.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original feature shape: (7470, 50, 2048)\n"
     ]
    }
   ],
   "source": [
    "#VISUALIZATION OF EXTRACTED FEATURES USING PLOTS \n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "def visualize_features(features, title):\n",
    "    # Reduce dimensionality\n",
    "    pca = PCA(n_components=2)\n",
    "    pca_result = pca.fit_transform(features)\n",
    "\n",
    "    # Reduce dimensionality\n",
    "    tsne = TSNE(n_components=2)\n",
    "    tsne_result = tsne.fit_transform(features)\n",
    "\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.scatter(pca_result[:, 0], pca_result[:, 1], alpha=0.5)\n",
    "    plt.title(f\"PCA - {title}\")\n",
    "    plt.xlabel(\"PCA Component 1\")\n",
    "    plt.ylabel(\"PCA Component 2\")\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.scatter(tsne_result[:, 0], tsne_result[:, 1], alpha=0.5)\n",
    "    plt.title(f\"t-SNE - {title}\")\n",
    "    plt.xlabel(\"t-SNE Component 1\")\n",
    "    plt.ylabel(\"t-SNE Component 2\")\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "    from sklearn.cluster import KMeans\n",
    "    \n",
    "    # K-Means Clustering\n",
    "    num_clusters = 3\n",
    "    kmeans = KMeans(n_clusters=num_clusters)\n",
    "    labels = kmeans.fit_predict(features)\n",
    "    \n",
    "    plt.figure(figsize=(8, 8))\n",
    "    plt.scatter(tsne_result[:, 0], tsne_result[:, 1], c=labels, cmap='viridis', alpha=0.5)\n",
    "    plt.title('t-SNE Result with K-Means Clusters')\n",
    "    plt.colorbar()\n",
    "    plt.show()\n",
    "    plt.figure(figsize=(8, 8))\n",
    "    plt.scatter(pca_result[:, 0], pca_result[:, 1], c=labels, cmap='viridis', alpha=0.5)\n",
    "    plt.title('PCA Result with K-Means Clusters')\n",
    "    plt.colorbar()\n",
    "    plt.show()\n",
    "\n",
    "def visualize_features_2(features, title):\n",
    "    # Check the shape of the features\n",
    "    print(f\"Original feature shape: {features.shape}\")\n",
    "    \n",
    "    # Flatten the features to 2D (batch_size, -1)\n",
    "    flattened_features = features.reshape(features.shape[0], -1)  # Flatten each image\n",
    "    \n",
    "    # Reduce dimensionality using PCA\n",
    "    pca = PCA(n_components=2)\n",
    "    pca_result = pca.fit_transform(flattened_features)\n",
    "\n",
    "    # Reduce dimensionality using t-SNE\n",
    "    tsne = TSNE(n_components=2)\n",
    "    tsne_result = tsne.fit_transform(flattened_features)\n",
    "\n",
    "    # Visualization\n",
    "    plt.scatter(pca_result[:, 0], pca_result[:, 1], alpha=0.7)\n",
    "    plt.title(f'PCA: {title}')\n",
    "    plt.show()\n",
    "    \n",
    "    plt.scatter(tsne_result[:, 0], tsne_result[:, 1], alpha=0.7)\n",
    "    plt.title(f't-SNE: {title}')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Visualize average features\n",
    "#visualize_features(avg_feats.numpy(), \"Average Features\")\n",
    "# You can also visualize patch features or final embeddings if needed\n",
    "#visualize_features(patch_feats.numpy(), \"Patch Features\")\n",
    "visualize_features_2(final_embeddings.numpy(), \"Final Embeddings\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D2kBZTiCMmsE"
   },
   "source": [
    "### **Text Encoder**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Text Encoder - Encoding Dictionary, Encoding Medical History and Finding Cosine Similarity Between Them '''\n",
    "\n",
    "# reading medical reports collected and ensuring all are strings\n",
    "iu_xray_reports_preprocessed_df_path = os.path.join(download_path, 'iu_xray/iu_xray_reports_preprocessed_df.csv')\n",
    "medical_history = pd.read_csv(iu_xray_reports_preprocessed_df_path)\n",
    "medical_history = medical_history[\"findings\"].dropna().tolist()  \n",
    "\n",
    "medical_history = [str(report) for report in medical_history if isinstance(report, str) or pd.notna(report)]\n",
    "if not medical_history:\n",
    "    raise ValueError(\"No valid findings found in medical_history.\")\n",
    "\n",
    "\n",
    "# dictionary created with key as organ: chest\n",
    "radiology_dictionary = {\n",
    "    \"chest\": [\"small\", \"enlarged\", \"unchanged\", \"stable\", \"silhouette\", \"contours\", \"size\", \"focal\", \"mild\", \"acute\"]\n",
    "}\n",
    "\n",
    "\n",
    "# using pre-trained model to embed into numerical space\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "\n",
    "# function to embed text\n",
    "def embed_text(texts):\n",
    "    inputs = tokenizer(texts, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "    outputs = model(**inputs)\n",
    "    return outputs.last_hidden_state.mean(dim=1)\n",
    "\n",
    "\n",
    "# embedding process\n",
    "dictionary_embeddings = embed_text(radiology_dictionary[\"chest\"] )\n",
    "report_embeddings = embed_text(medical_history) \n",
    "\n",
    "\n",
    "# calculating cosine similarities\n",
    "similarities = torch.mm(report_embeddings, dictionary_embeddings.T)\n",
    "\n",
    "\n",
    "# finding top k relevant entries\n",
    "k = 5\n",
    "top_k_indices = similarities.topk(k=k, dim=1).indices\n",
    "relevant_entries = [[dictionary_entries[idx] for idx in row] for row in top_k_indices]\n",
    "\n",
    "\n",
    "# printing relevant entries of each report\n",
    "for i, entries in enumerate(relevant_entries):\n",
    "    print(f\"Report {i+1}: Relevant Entries: {entries}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary embeddings saved to /home/raginivi/CS550_ASMT_MRSMK/datasets/iu_xray/dictionary_embeddings.pt\n",
      "Dictionary Embeddings Shape: torch.Size([10, 768])\n"
     ]
    }
   ],
   "source": [
    "'''Text encoder -> encoding general knowledge, i.e. the dictionary,\n",
    "                -> encodings obtained are fine-grained aligned with the image embeddings\n",
    "\n",
    "                -> we get a final encoded info from the above, concatenate with the above'''\n",
    "import os                  # For file and directory operations\n",
    "import torch               # For tensor operations and saving/loading models\n",
    "from transformers import BertTokenizer, BertModel  # For loading the BERT tokenizer and model\n",
    "\n",
    "working_directory = os.getcwd()\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# dictionary created with key as organ: chest\n",
    "radiology_dictionary = {\n",
    "    \"chest\": [\"small\", \"enlarged\", \"unchanged\", \"stable\", \"silhouette\", \"contours\", \"size\", \"focal\", \"mild\", \"acute\"]\n",
    "}\n",
    "\n",
    "# using pre-trained model to embed into numerical space\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertModel.from_pretrained('bert-base-uncased').to(device)\n",
    "\n",
    "# function to embed text\n",
    "def embed_text(texts):\n",
    "    inputs = tokenizer(texts, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "    outputs = model(**inputs)\n",
    "    return outputs.last_hidden_state.mean(dim=1)\n",
    "\n",
    "# embedding process\n",
    "dictionary_embeddings = embed_text(radiology_dictionary[\"chest\"] )\n",
    "\n",
    "# Save the dictionary embeddings to a .pt file\n",
    "embeddings_dir = os.path.join(working_directory, 'datasets', 'iu_xray')\n",
    "embeddings_file_path = os.path.join(embeddings_dir, 'dictionary_embeddings.pt')\n",
    "torch.save(dictionary_embeddings.cpu(), embeddings_file_path)  # Move to CPU for saving if using GPU\n",
    "print(f\"Dictionary embeddings saved to {embeddings_file_path}\")\n",
    "\n",
    "# Print shape of the dictionary embeddings for verification\n",
    "print(f\"Dictionary Embeddings Shape: {dictionary_embeddings.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gIYzXGtsMqmQ"
   },
   "source": [
    "### **Multilevel Alignment**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Uz4pEhW2MuBF"
   },
   "source": [
    "### **Report Generator**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z_WeRBlDMwX2"
   },
   "source": [
    "### **Complete Model**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NZjhDeJxMzgg"
   },
   "source": [
    "## **Training**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7Uk_G0K8M2H9"
   },
   "source": [
    "### **Training**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OXhkaJVvM3Uu"
   },
   "source": [
    "## **Testing**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bzTZZUlQM401"
   },
   "source": [
    "### **Testing**"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "MlOl6I0rMa68",
    "PsELnlXpMjGX",
    "D2kBZTiCMmsE",
    "gIYzXGtsMqmQ",
    "Uz4pEhW2MuBF",
    "Z_WeRBlDMwX2",
    "7Uk_G0K8M2H9",
    "OXhkaJVvM3Uu"
   ],
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
