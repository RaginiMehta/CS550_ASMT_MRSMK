{"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":["MlOl6I0rMa68","PsELnlXpMjGX","D2kBZTiCMmsE","gIYzXGtsMqmQ","Uz4pEhW2MuBF","Z_WeRBlDMwX2","7Uk_G0K8M2H9","OXhkaJVvM3Uu"],"gpuType":"T4","provenance":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## **Medical Report Summarisation using Medical Knowledge**\n\n### **References**\n\n**Main Reference**\n- Radiology report generation with medical knowledge and multilevel image-report alignment: A new method and its verification\nhttps://www.sciencedirect.com/science/article/pii/S0933365723002282\n\n","metadata":{"id":"Bnpd7Zh0NPl2"}},{"cell_type":"markdown","source":"## **Data Collection**","metadata":{"id":"oKdcyk1sMEtD"}},{"cell_type":"markdown","source":"### **Collect Datasets**","metadata":{"id":"p81rfgckMR9a"}},{"cell_type":"code","source":"'''Libraries Installation and Import'''\n\n# installling necessary libraries\n!pip -q install --user requests numpy pandas matplotlib tqdm Pillow opencv-python nltk pyspellchecker torch torchvision torchaudio transformers scikit-learn sentence-transformers\n\n# importing required libraries\nimport os\nimport re\nimport requests\nimport tarfile\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport xml.etree.ElementTree as ET\nfrom tqdm import tqdm\nfrom PIL import Image\nimport cv2\n\nimport nltk\nfrom nltk.corpus import stopwords\nfrom spellchecker import SpellChecker\n\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\nimport torchvision.models as models\nfrom torchvision import datasets\nimport torchvision.transforms as transforms\nimport torch.nn.functional as F\n\nimport transformers\nfrom transformers import BertTokenizer, BertModel\nfrom sentence_transformers import SentenceTransformer\nfrom transformers import BlipProcessor, BlipForConditionalGeneration\n\nfrom sklearn.decomposition import PCA\nfrom sklearn.manifold import TSNE\nfrom sklearn.cluster import KMeans","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"AYR_B6u1ojJt","outputId":"faaac1ed-b54b-4cfd-b3e4-cca51372318e","execution":{"iopub.status.busy":"2024-10-19T18:57:50.064106Z","iopub.execute_input":"2024-10-19T18:57:50.064394Z","iopub.status.idle":"2024-10-19T18:58:24.874033Z","shell.execute_reply.started":"2024-10-19T18:57:50.064362Z","shell.execute_reply":"2024-10-19T18:58:24.872794Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"'''Setting Paths'''\n\n# project directory\n# from google.colab import drive\n# drive.mount('/content/drive')\n# project_directory = '/content/drive/Othercomputers/My Laptop/CS550_ASMT_MRSMK/datasets'\n# project_directory = '/content/drive/MyDrive/Academics/CS550 Machine Learning/CS550 ASMT MRSMK/datasets'\n\nproject_directory = \"./datasets\"\ndataset = 'iu_xray/'\niu_xray_dataset = os.path.join(project_directory, dataset)\n\n\n# input directory\ninput_directory = os.path.join(iu_xray_dataset, \"input\")\n\nimages_dir = os.path.join(input_directory, \"images\")\nreports_dir = os.path.join(input_directory, \"reports\")\niu_xray_images = images_dir\niu_xray_reports = os.path.join(reports_dir, 'ecgen-radiology')\n\n\n# output directory \noutput_directory = os.path.join(iu_xray_dataset, \"output\")","metadata":{"execution":{"iopub.status.busy":"2024-10-19T18:58:24.876208Z","iopub.execute_input":"2024-10-19T18:58:24.877003Z","iopub.status.idle":"2024-10-19T18:58:24.884658Z","shell.execute_reply.started":"2024-10-19T18:58:24.876929Z","shell.execute_reply":"2024-10-19T18:58:24.883799Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"'''Setup - Generalized'''\n\n# setup to download the IU X-Ray Dataset\nimages_url = \"https://openi.nlm.nih.gov/imgs/collections/NLMCXR_png.tgz\"\nreports_url = \"https://openi.nlm.nih.gov/imgs/collections/NLMCXR_reports.tgz\"\n\n\n# function to check the file size of a given URL\ndef get_file_size(url):\n    response = requests.head(url)\n    size_in_bytes = int(response.headers.get('Content-Length', 0))\n    size_in_mb = size_in_bytes / (1024 * 1024)\n    return size_in_mb\n\n\n# function to download and extract from a given url to a given directory\ndef download_and_extract(url, save_dir):\n    file_name = url.split('/')[-1]\n    file_path = os.path.join(save_dir, file_name)\n\n    response = requests.get(url, stream=True)\n    total_size = int(response.headers.get('Content-Length', 0))\n    downloaded_size = 0\n\n    with open(file_path, 'wb') as file:\n        for chunk in response.iter_content(chunk_size=1024):\n            if chunk:\n                file.write(chunk)\n                downloaded_size += len(chunk)\n                percent_complete = (downloaded_size / total_size) * 100\n                print(f\"Downloaded {downloaded_size / (1024*1024):.2f} MB out of {total_size / (1024*1024):.2f} MB: {percent_complete:.2f}% complete\")\n\n    print(\"\\nDownload complete!\")\n\n    with tarfile.open(file_path, 'r:gz') as tar:\n        members = tar.getmembers()\n        total_files = len(members)\n\n        for idx, member in enumerate(members, start=1):\n            tar.extract(member, path=save_dir)\n            print(f\"Extracting File {idx} out of {total_files}: {member.name}\")\n\n    os.remove(file_path)\n\n\n# downloading  IU X-Ray dataset\nif not os.path.exists(images_dir):\n    images_size = get_file_size(images_url)\n    print(f\"Downloading {images_url} to: {images_dir} ({images_size:.2f} MB)\")\n    os.makedirs(images_dir, exist_ok=True)\n    download_and_extract(images_url, images_dir)\n    print(f\"Downloaded {images_url} to: {images_dir}\")\nelse:\n    print(f\"{images_url} already exists at: {images_dir}\")\n\nif not os.path.exists(reports_dir):\n    reports_size = get_file_size(reports_url)\n    print(f\"Downloading {reports_url} to: {reports_dir} ({reports_size:.2f} MB)\")\n    os.makedirs(reports_dir, exist_ok=True)\n    download_and_extract(reports_url, reports_dir)\n    print(f\"Downloaded {reports_url} to: {reports_dir}\")\nelse:\n    print(f\"{reports_url} already exists at: {reports_dir}\")","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"49-0xiXplwH8","outputId":"ea63eda3-908a-4f5f-eeec-a4019130e57f","execution":{"iopub.status.busy":"2024-10-19T18:58:24.885947Z","iopub.execute_input":"2024-10-19T18:58:24.886422Z","iopub.status.idle":"2024-10-19T18:58:24.945116Z","shell.execute_reply.started":"2024-10-19T18:58:24.886378Z","shell.execute_reply":"2024-10-19T18:58:24.944038Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"https://openi.nlm.nih.gov/imgs/collections/NLMCXR_png.tgz already exists at: ./datasets/iu_xray/input/images\nhttps://openi.nlm.nih.gov/imgs/collections/NLMCXR_reports.tgz already exists at: ./datasets/iu_xray/input/reports\n","output_type":"stream"}]},{"cell_type":"code","source":"'''Exploring the IU X-Ray Dataset Contents'''\n\n# displaying directory and subdirectory contents\nprint(\"\\nPath: \", iu_xray_images)\nprint(f\"Directory Contents: {len(os.listdir(iu_xray_images))} Images\")\n\nprint(\"\\nPath: \", iu_xray_reports)\nprint(f\"Directory Contents: {len(os.listdir(iu_xray_reports))} Reports\")","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5ZSA8Jyowaoh","outputId":"dffa4899-00a7-4d49-a118-8dd85400ba06","execution":{"iopub.status.busy":"2024-10-19T18:58:24.947707Z","iopub.execute_input":"2024-10-19T18:58:24.948661Z","iopub.status.idle":"2024-10-19T18:58:24.961726Z","shell.execute_reply.started":"2024-10-19T18:58:24.948615Z","shell.execute_reply":"2024-10-19T18:58:24.960832Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"\nPath:  ./datasets/iu_xray/input/images\nDirectory Contents: 7471 Images\n\nPath:  ./datasets/iu_xray/input/reports/ecgen-radiology\nDirectory Contents: 3955 Reports\n","output_type":"stream"}]},{"cell_type":"code","source":"'''Processing Textual Data from each .xml Report File and Storing it in a .csv File'''\n\n# function to iterate through all .xml report files and storing them in a dataframe\ndef save_images_df():\n    data = []\n    cnt = 0\n    for file in os.listdir(iu_xray_reports):\n        if file.endswith(\".xml\"):\n            cnt += 1\n            print(f\"Processing .xml File {cnt} out of {len(os.listdir(iu_xray_reports))}: {file}\")\n\n            file_path = os.path.join(iu_xray_reports, file)\n            try:\n                tree = ET.parse(file_path)\n                root = tree.getroot()\n\n                pmc_id = root.find('.//pmcId').attrib.get('id')\n\n                comparison = indication = findings = impression = None\n\n                for abstract in root.findall('.//AbstractText'):\n                    if abstract.attrib.get('Label') == 'COMPARISON':\n                        comparison = abstract.text\n                    elif abstract.attrib.get('Label') == 'INDICATION':\n                        indication = abstract.text\n                    elif abstract.attrib.get('Label') == 'FINDINGS':\n                        findings = abstract.text\n                    elif abstract.attrib.get('Label') == 'IMPRESSION':\n                        impression = abstract.text\n\n                for parent_image in root.findall('parentImage'):\n                    image_file = parent_image.attrib['id'] + \".png\"\n                    image_path = os.path.join(iu_xray_images, image_file)\n                    image = cv2.imread(image_path)\n\n                    if image is not None:\n                        height, width, channels = image.shape\n                        caption = parent_image.find('caption').text if parent_image.find('caption') is not None else None\n                        data.append([pmc_id, image_file, caption, comparison, indication, findings, impression, height, width])\n                    else:\n                        print(f\"Warning: Unable to read image {image_path}\")\n\n            except Exception as e:\n                print(f\"Error processing file {file}: {e}\")\n\n    return data\n\n\n# creating a dataframe and saving it as .csv\niu_xray_images_df_path = os.path.join(output_directory, 'iu_xray_images_df.csv')\nif not os.path.exists(iu_xray_images_df_path):\n    data = save_images_df()\n    columns = ['pmc_id', 'image_filename', 'caption', 'comparison', 'indication', 'findings', 'impression', 'height', 'width']\n    iu_xray_images_df = pd.DataFrame(data, columns=columns)\n    iu_xray_images_df.to_csv(iu_xray_images_df_path, index=False)\n    print(f\"Dataframe saved to {iu_xray_images_df_path}\")\nelse:\n    print(f\"Dataframe already exists at {iu_xray_images_df_path}\")\n    iu_xray_images_df = pd.read_csv(iu_xray_images_df_path)\n\n\n# displaying the stored dataframe\nprint(\"\\n\\nDataframe Shape:\", iu_xray_images_df.shape)\n\nprint(\"\\n\\nDataframe Information:\\n\")\ndisplay(iu_xray_images_df.info())\n\nprint(\"\\n\\nDisplaying Dataframe:\\n\")\ndisplay(iu_xray_images_df.head())","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":822},"id":"qGczexPLUaN5","outputId":"8c7528a1-7064-4802-c276-edef004f0d5b","execution":{"iopub.status.busy":"2024-10-19T18:58:24.963106Z","iopub.execute_input":"2024-10-19T18:58:24.963724Z","iopub.status.idle":"2024-10-19T18:58:25.059827Z","shell.execute_reply.started":"2024-10-19T18:58:24.963685Z","shell.execute_reply":"2024-10-19T18:58:25.058789Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"Dataframe already exists at ./datasets/iu_xray/output/iu_xray_images_df.csv\n\n\nDataframe Shape: (7470, 9)\n\n\nDataframe Information:\n\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 7470 entries, 0 to 7469\nData columns (total 9 columns):\n #   Column          Non-Null Count  Dtype \n---  ------          --------------  ----- \n 0   pmc_id          7470 non-null   int64 \n 1   image_filename  7470 non-null   object\n 2   caption         7468 non-null   object\n 3   comparison      5210 non-null   object\n 4   indication      7311 non-null   object\n 5   findings        6473 non-null   object\n 6   impression      7418 non-null   object\n 7   height          7470 non-null   int64 \n 8   width           7470 non-null   int64 \ndtypes: int64(3), object(6)\nmemory usage: 525.4+ KB\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"None"},"metadata":{}},{"name":"stdout","text":"\n\nDisplaying Dataframe:\n\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"   pmc_id            image_filename  \\\n0    3967  CXR3967_IM-2028-1001.png   \n1    3967  CXR3967_IM-2028-2001.png   \n2    3332  CXR3332_IM-1596-1001.png   \n3    3332  CXR3332_IM-1596-2001.png   \n4    3332  CXR3332_IM-1596-3001.png   \n\n                                            caption  \\\n0  PA and lateral chest x-XXXX XXXX at XXXX hours.    \n1  PA and lateral chest x-XXXX XXXX at XXXX hours.    \n2      Radiograph Chest PA and Lateral XXXX, XXXX.    \n3      Radiograph Chest PA and Lateral XXXX, XXXX.    \n4      Radiograph Chest PA and Lateral XXXX, XXXX.    \n\n                                    comparison  indication  \\\n0                                          NaN  Chest pain   \n1                                          NaN  Chest pain   \n2  Radiograph Chest PA and Lateral XXXX, XXXX.   Weakness.   \n3  Radiograph Chest PA and Lateral XXXX, XXXX.   Weakness.   \n4  Radiograph Chest PA and Lateral XXXX, XXXX.   Weakness.   \n\n                                            findings  \\\n0  The cardiomediastinal silhouette is within nor...   \n1  The cardiomediastinal silhouette is within nor...   \n2  The heart is normal in size and contour. There...   \n3  The heart is normal in size and contour. There...   \n4  The heart is normal in size and contour. There...   \n\n                                          impression  height  width  \n0  1. No acute radiographic cardiopulmonary process.     420    512  \n1  1. No acute radiographic cardiopulmonary process.     624    512  \n2            No acute cardiopulmonary abnormalities.     420    512  \n3            No acute cardiopulmonary abnormalities.     624    512  \n4            No acute cardiopulmonary abnormalities.     624    512  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>pmc_id</th>\n      <th>image_filename</th>\n      <th>caption</th>\n      <th>comparison</th>\n      <th>indication</th>\n      <th>findings</th>\n      <th>impression</th>\n      <th>height</th>\n      <th>width</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>3967</td>\n      <td>CXR3967_IM-2028-1001.png</td>\n      <td>PA and lateral chest x-XXXX XXXX at XXXX hours.</td>\n      <td>NaN</td>\n      <td>Chest pain</td>\n      <td>The cardiomediastinal silhouette is within nor...</td>\n      <td>1. No acute radiographic cardiopulmonary process.</td>\n      <td>420</td>\n      <td>512</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>3967</td>\n      <td>CXR3967_IM-2028-2001.png</td>\n      <td>PA and lateral chest x-XXXX XXXX at XXXX hours.</td>\n      <td>NaN</td>\n      <td>Chest pain</td>\n      <td>The cardiomediastinal silhouette is within nor...</td>\n      <td>1. No acute radiographic cardiopulmonary process.</td>\n      <td>624</td>\n      <td>512</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3332</td>\n      <td>CXR3332_IM-1596-1001.png</td>\n      <td>Radiograph Chest PA and Lateral XXXX, XXXX.</td>\n      <td>Radiograph Chest PA and Lateral XXXX, XXXX.</td>\n      <td>Weakness.</td>\n      <td>The heart is normal in size and contour. There...</td>\n      <td>No acute cardiopulmonary abnormalities.</td>\n      <td>420</td>\n      <td>512</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3332</td>\n      <td>CXR3332_IM-1596-2001.png</td>\n      <td>Radiograph Chest PA and Lateral XXXX, XXXX.</td>\n      <td>Radiograph Chest PA and Lateral XXXX, XXXX.</td>\n      <td>Weakness.</td>\n      <td>The heart is normal in size and contour. There...</td>\n      <td>No acute cardiopulmonary abnormalities.</td>\n      <td>624</td>\n      <td>512</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>3332</td>\n      <td>CXR3332_IM-1596-3001.png</td>\n      <td>Radiograph Chest PA and Lateral XXXX, XXXX.</td>\n      <td>Radiograph Chest PA and Lateral XXXX, XXXX.</td>\n      <td>Weakness.</td>\n      <td>The heart is normal in size and contour. There...</td>\n      <td>No acute cardiopulmonary abnormalities.</td>\n      <td>624</td>\n      <td>512</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"'''Processing Textual Data from each .xml Report File and Storing it in a .csv File'''\n\n# function to iterate through all .xml report files and storing them in a dataframe\ndef save_reports_df():\n    data = []\n    cnt = 0\n    for file in os.listdir(iu_xray_reports):\n        if file.endswith(\".xml\"):\n            cnt += 1\n            print(f\"Processing .xml File {cnt} out of {len(os.listdir(iu_xray_reports))}: {file}\")\n\n            file_path = os.path.join(iu_xray_reports, file)\n            try:\n                tree = ET.parse(file_path)\n                root = tree.getroot()\n\n                pmc_id = root.find('.//pmcId').attrib.get('id')\n\n                comparison = indication = findings = impression = None\n\n                for abstract in root.findall('.//AbstractText'):\n                    if abstract.attrib.get('Label') == 'COMPARISON':\n                        comparison = abstract.text\n                    elif abstract.attrib.get('Label') == 'INDICATION':\n                        indication = abstract.text\n                    elif abstract.attrib.get('Label') == 'FINDINGS':\n                        findings = abstract.text\n                    elif abstract.attrib.get('Label') == 'IMPRESSION':\n                        impression = abstract.text\n\n                report_data = {\n                    'pmc_id': pmc_id,\n                    'findings': findings,\n                    'impression': impression,\n                    'comparison': comparison,\n                    'indication': indication,\n                }\n\n                parent_images = root.findall('parentImage')\n                report_data['image_count'] = len(parent_images)\n\n                for i, parent_image in enumerate(parent_images, start=1):\n                    image_file = parent_image.attrib['id'] + \".jpg\"\n                    caption = parent_image.find('caption').text if parent_image.find('caption') is not None else None\n                    report_data[f'image_{i}'] = f\"{image_file}: {caption}\" if caption else image_file\n\n                data.append(report_data)\n\n            except Exception as e:\n                print(f\"Error processing file {file}: {e}\")\n\n    return data\n\n\n# creating a dataframe and saving it as .csv\niu_xray_reports_df_path = os.path.join(output_directory, 'iu_xray_reports_df.csv')\nif not os.path.exists(iu_xray_reports_df_path):\n    data = save_reports_df()\n    iu_xray_reports_df = pd.DataFrame(data)\n    iu_xray_reports_df.to_csv(iu_xray_reports_df_path, index=False)\n    print(f\"Dataframe saved to {iu_xray_reports_df_path}\")\nelse:\n    print(f\"Dataframe already exists at {iu_xray_reports_df_path}\")\n    iu_xray_reports_df = pd.read_csv(iu_xray_reports_df_path)\n\n\n# displaying the stored dataframe\nprint(\"\\n\\nDataframe Shape:\", iu_xray_reports_df.shape)\n\nprint(\"\\n\\nDataframe Information:\\n\")\ndisplay(iu_xray_reports_df.info())\n\nprint(\"\\n\\nDisplaying Dataframe:\\n\")\ndisplay(iu_xray_reports_df.head())","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":893},"id":"vYnfzXXT0O6w","outputId":"d2692046-ebe0-45ac-8291-37641f6d5958","execution":{"iopub.status.busy":"2024-10-19T18:58:25.061414Z","iopub.execute_input":"2024-10-19T18:58:25.061834Z","iopub.status.idle":"2024-10-19T18:58:25.132065Z","shell.execute_reply.started":"2024-10-19T18:58:25.061787Z","shell.execute_reply":"2024-10-19T18:58:25.131025Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"Dataframe already exists at ./datasets/iu_xray/output/iu_xray_reports_df.csv\n\n\nDataframe Shape: (3955, 11)\n\n\nDataframe Information:\n\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 3955 entries, 0 to 3954\nData columns (total 11 columns):\n #   Column       Non-Null Count  Dtype \n---  ------       --------------  ----- \n 0   pmc_id       3955 non-null   int64 \n 1   findings     3425 non-null   object\n 2   impression   3921 non-null   object\n 3   comparison   2757 non-null   object\n 4   indication   3865 non-null   object\n 5   image_count  3955 non-null   int64 \n 6   image_1      3851 non-null   object\n 7   image_2      3405 non-null   object\n 8   image_3      197 non-null    object\n 9   image_4      16 non-null     object\n 10  image_5      1 non-null      object\ndtypes: int64(2), object(9)\nmemory usage: 340.0+ KB\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"None"},"metadata":{}},{"name":"stdout","text":"\n\nDisplaying Dataframe:\n\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"   pmc_id                                           findings  \\\n0    3967  The cardiomediastinal silhouette is within nor...   \n1    3332  The heart is normal in size and contour. There...   \n2      30  Lungs are clear without focal consolidation, e...   \n3    2593  Mild cardiomegaly is unchanged. Stable superio...   \n4    1165  Frontal and lateral views of the chest show no...   \n\n                                          impression  \\\n0  1. No acute radiographic cardiopulmonary process.   \n1            No acute cardiopulmonary abnormalities.   \n2        Negative acute cardiopulmonary abnormality.   \n3  Mild cardiomegaly with interstitial prominence...   \n4  No acute or active cardiac, pulmonary or pleur...   \n\n                                    comparison  \\\n0                                          NaN   \n1  Radiograph Chest PA and Lateral XXXX, XXXX.   \n2                                          NaN   \n3                                          NaN   \n4                                        None.   \n\n                                          indication  image_count  \\\n0                                         Chest pain            2   \n1                                          Weakness.            3   \n2                XXXX-year-old male with chest pain.            2   \n3                                          Back pain            2   \n4  Chest pain. Shortness of breath. The patient's...            1   \n\n                                             image_1  \\\n0  CXR3967_IM-2028-1001.jpg: PA and lateral chest...   \n1  CXR3332_IM-1596-1001.jpg: Radiograph Chest PA ...   \n2  CXR30_IM-1385-1001.jpg: Chest x-XXXX XXXX and ...   \n3  CXR2593_IM-1084-1001.jpg: Chest, 2 views, XXXX...   \n4  CXR1165_IM-0110-1001.jpg: Xray Chest PA and La...   \n\n                                             image_2  \\\n0  CXR3967_IM-2028-2001.jpg: PA and lateral chest...   \n1  CXR3332_IM-1596-2001.jpg: Radiograph Chest PA ...   \n2  CXR30_IM-1385-2001.jpg: Chest x-XXXX XXXX and ...   \n3  CXR2593_IM-1084-2001.jpg: Chest, 2 views, XXXX...   \n4                                                NaN   \n\n                                             image_3 image_4 image_5  \n0                                                NaN     NaN     NaN  \n1  CXR3332_IM-1596-3001.jpg: Radiograph Chest PA ...     NaN     NaN  \n2                                                NaN     NaN     NaN  \n3                                                NaN     NaN     NaN  \n4                                                NaN     NaN     NaN  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>pmc_id</th>\n      <th>findings</th>\n      <th>impression</th>\n      <th>comparison</th>\n      <th>indication</th>\n      <th>image_count</th>\n      <th>image_1</th>\n      <th>image_2</th>\n      <th>image_3</th>\n      <th>image_4</th>\n      <th>image_5</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>3967</td>\n      <td>The cardiomediastinal silhouette is within nor...</td>\n      <td>1. No acute radiographic cardiopulmonary process.</td>\n      <td>NaN</td>\n      <td>Chest pain</td>\n      <td>2</td>\n      <td>CXR3967_IM-2028-1001.jpg: PA and lateral chest...</td>\n      <td>CXR3967_IM-2028-2001.jpg: PA and lateral chest...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>3332</td>\n      <td>The heart is normal in size and contour. There...</td>\n      <td>No acute cardiopulmonary abnormalities.</td>\n      <td>Radiograph Chest PA and Lateral XXXX, XXXX.</td>\n      <td>Weakness.</td>\n      <td>3</td>\n      <td>CXR3332_IM-1596-1001.jpg: Radiograph Chest PA ...</td>\n      <td>CXR3332_IM-1596-2001.jpg: Radiograph Chest PA ...</td>\n      <td>CXR3332_IM-1596-3001.jpg: Radiograph Chest PA ...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>30</td>\n      <td>Lungs are clear without focal consolidation, e...</td>\n      <td>Negative acute cardiopulmonary abnormality.</td>\n      <td>NaN</td>\n      <td>XXXX-year-old male with chest pain.</td>\n      <td>2</td>\n      <td>CXR30_IM-1385-1001.jpg: Chest x-XXXX XXXX and ...</td>\n      <td>CXR30_IM-1385-2001.jpg: Chest x-XXXX XXXX and ...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>2593</td>\n      <td>Mild cardiomegaly is unchanged. Stable superio...</td>\n      <td>Mild cardiomegaly with interstitial prominence...</td>\n      <td>NaN</td>\n      <td>Back pain</td>\n      <td>2</td>\n      <td>CXR2593_IM-1084-1001.jpg: Chest, 2 views, XXXX...</td>\n      <td>CXR2593_IM-1084-2001.jpg: Chest, 2 views, XXXX...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1165</td>\n      <td>Frontal and lateral views of the chest show no...</td>\n      <td>No acute or active cardiac, pulmonary or pleur...</td>\n      <td>None.</td>\n      <td>Chest pain. Shortness of breath. The patient's...</td>\n      <td>1</td>\n      <td>CXR1165_IM-0110-1001.jpg: Xray Chest PA and La...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"'''Displaying the Number of Images per Report'''\n\n# displaying the distribution of number of images per report\nreports_count = iu_xray_reports_df['image_count'].value_counts().rename_axis('images_qty').reset_index(name='reports_count')\nprint(\"\\n\\nNumber of Images per Report:\\n\")\ndisplay(reports_count)","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":310},"id":"bw4Ylfa94M1o","outputId":"5ef503a0-265c-423b-8cae-1d36b136134d","execution":{"iopub.status.busy":"2024-10-19T18:58:25.133396Z","iopub.execute_input":"2024-10-19T18:58:25.133773Z","iopub.status.idle":"2024-10-19T18:58:25.148639Z","shell.execute_reply.started":"2024-10-19T18:58:25.133733Z","shell.execute_reply":"2024-10-19T18:58:25.147394Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"\n\nNumber of Images per Report:\n\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"   images_qty  reports_count\n0           2           3208\n1           1            446\n2           3            181\n3           0            104\n4           4             15\n5           5              1","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>images_qty</th>\n      <th>reports_count</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>2</td>\n      <td>3208</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>446</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3</td>\n      <td>181</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0</td>\n      <td>104</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>4</td>\n      <td>15</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>5</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"## **Data Preprocessing**","metadata":{"id":"6UouFxQwMNeo"}},{"cell_type":"markdown","source":"### **Preprocess Images**","metadata":{"id":"-cGf99_CMV47"}},{"cell_type":"code","source":"'''Preprocessing Images - Resizing, Tensor Conversion and Normalization'''\n\n# function to preprocess and save images\ndef preprocess_images(input_dir, output_dir):\n    preprocess = transforms.Compose([\n        transforms.Resize((224, 224)),\n        transforms.ToTensor(),\n        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n    ])\n\n    os.makedirs(output_dir, exist_ok=True)\n\n    cnt = 0\n    for filename in os.listdir(input_dir):\n        if filename.endswith('.png'):\n            cnt += 1\n            print(f\"Preprocessing File {cnt} out of {len(os.listdir(input_dir))}: {filename}\")\n\n            image_path = os.path.join(input_dir, filename)\n            image = Image.open(image_path).convert('RGB')\n            processed_image = preprocess(image)\n\n            processed_image_path = os.path.join(output_dir, filename)\n\n            processed_image_pil = transforms.ToPILImage()(processed_image)\n            processed_image_pil.save(processed_image_path)\n\n\n# preprocessing images\niu_xray_images_preprocessed = os.path.join(output_directory, 'images_preprocessed')\nif not os.path.exists(iu_xray_images_preprocessed):\n    print(f\"Preprocessing Images to: {iu_xray_images_preprocessed}\")\n    preprocess_images(iu_xray_images, iu_xray_images_preprocessed)\n    print(f\"Preprocessed Images saved to: {iu_xray_images_preprocessed}\")\nelse:\n    print(f\"Preprocessed Images already exist at: {iu_xray_images_preprocessed}\")","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"JTClOSxeSCOM","outputId":"811cbc93-70e7-4d60-f213-26238488c44d","execution":{"iopub.status.busy":"2024-10-19T18:58:25.150245Z","iopub.execute_input":"2024-10-19T18:58:25.150758Z","iopub.status.idle":"2024-10-19T18:58:25.163866Z","shell.execute_reply.started":"2024-10-19T18:58:25.150707Z","shell.execute_reply":"2024-10-19T18:58:25.162620Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"Preprocessed Images already exist at: ./datasets/iu_xray/output/images_preprocessed\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### **Preprocess Text**","metadata":{"id":"lvdRoNqXMZlN"}},{"cell_type":"code","source":"'''Preprocessing Text - Lowercasing, Decontracting, Punctuation Removal, Number Removal, Two-Letter Word Removal, Stop Word Removal, Spell Checking, Extra Space Removal'''\n\n# download nltk resources and initialize spell checker\nnltk.download('punkt')\nnltk.download('wordnet')\nnltk.download('stopwords')\nspell = SpellChecker()\n\n\n# function to convert text to lowercase\ndef lowercase(text):\n    return text.lower() if isinstance(text, str) else text\n\n\n# function to decontract words\ndef decontracted(text):\n    if not isinstance(text, str):\n        return text\n    contractions = {\n        \"won't\": \"will not\", \"can't\": \"can not\", \"couldn't\": \"could not\",\n        \"shouldn't\": \"should not\", \"wouldn't\": \"would not\", \"n't\": \" not\",\n        \"'re\": \" are\", \"'s\": \" is\", \"'d\": \" would\", \"'ll\": \" will\",\n        \"'t\": \" not\", \"'ve\": \" have\", \"'m\": \" am\"\n    }\n    for contraction, full_form in contractions.items():\n        text = text.replace(contraction, full_form)\n    return text\n\n\n# function to remove punctuations\ndef rem_punctuations(text):\n    return re.sub(r'[^\\w\\s]', ' ', text) if isinstance(text, str) else text\n\n\n# function to remove numbers\ndef rem_numbers(text):\n    return re.sub(r'\\d+', ' ', text) if isinstance(text, str) else text\n\n\n# function to remove two-letter words except \"no\" and \"ct\"\ndef rem_two_letter_words(text):\n    if not isinstance(text, str):\n        return text\n    return ' '.join(word for word in text.split() if len(word) > 2 or word in [\"no\", \"ct\"])\n\n\n# function to remove stop words\ndef rem_stop_words(text):\n    if not isinstance(text, str):\n        return text\n    stop_words = set(stopwords.words('english'))\n    return ' '.join(word for word in text.split() if word not in stop_words)\n\n\n# function to correct spelling\ndef correct_spelling(text):\n    if not isinstance(text, str):\n        return text\n    corrected = []\n    for word in text.split():\n        corrected_word = list(spell.candidates(word))[0] if spell.candidates(word) else word\n        corrected.append(corrected_word)\n    return ' '.join(corrected)\n\n\n# function to remove extra spaces\ndef rem_extra_spaces(text):\n    return ' '.join(text.split()) if isinstance(text, str) else text\n\n\n# function to preprocess text\ndef preprocess_text(data):\n    preprocessed = []\n    for sentence in tqdm(data.values):\n        sentence = str(sentence)\n        sentence = lowercase(sentence)\n        sentence = decontracted(sentence)\n        sentence = rem_punctuations(sentence)\n        sentence = rem_numbers(sentence)\n        sentence = rem_two_letter_words(sentence)\n        sentence = rem_stop_words(sentence)\n        sentence = correct_spelling(sentence)\n        sentence = rem_extra_spaces(sentence)\n        \n        preprocessed.append(sentence)\n\n    return preprocessed\n\n\n# path to the preprocessed dataframe\niu_xray_reports_preprocessed_df_path = os.path.join(output_directory, 'iu_xray_reports_preprocessed_df.csv')\niu_xray_reports_preprocessed_df = iu_xray_reports_df.copy()\n\n\n# preprocessing text columns in the dataframe\nif not os.path.exists(iu_xray_reports_preprocessed_df_path):\n    print(f\"Preprocessing Text of DataFrame {iu_xray_reports_df_path} to: {iu_xray_reports_preprocessed_df_path}\")\n    \n    preprocess_caption = True\n    preprocess_comparison = True\n    preprocess_indication = True\n    preprocess_findings = True\n    preprocess_impression = True\n    \n    if preprocess_caption and 'caption' in iu_xray_reports_preprocessed_df.columns:\n        print(\"Preprocessing Column: caption\")\n        iu_xray_reports_preprocessed_df['caption'] = iu_xray_reports_preprocessed_df['caption'].fillna('unknown').astype(str)\n        iu_xray_reports_preprocessed_df['caption'] = preprocess_text(iu_xray_reports_preprocessed_df['caption'])\n        iu_xray_reports_preprocessed_df.to_csv(iu_xray_reports_preprocessed_df_path, index=False)\n        print(f\"Saved preprocessed 'caption' column to: {iu_xray_reports_preprocessed_df_path}\")\n    \n    if preprocess_comparison and 'comparison' in iu_xray_reports_preprocessed_df.columns:\n        print(\"Preprocessing Column: comparison\")\n        iu_xray_reports_preprocessed_df['comparison'] = iu_xray_reports_preprocessed_df['comparison'].fillna('none').astype(str)\n        iu_xray_reports_preprocessed_df['comparison'] = preprocess_text(iu_xray_reports_preprocessed_df['comparison'])\n        iu_xray_reports_preprocessed_df.to_csv(iu_xray_reports_preprocessed_df_path, index=False)\n        print(f\"Saved preprocessed 'comparison' column to: {iu_xray_reports_preprocessed_df_path}\")\n    \n    if preprocess_indication and 'indication' in iu_xray_reports_preprocessed_df.columns:\n        print(\"Preprocessing Column: indication\")\n        iu_xray_reports_preprocessed_df['indication'] = iu_xray_reports_preprocessed_df['indication'].fillna('none').astype(str)\n        iu_xray_reports_preprocessed_df['indication'] = preprocess_text(iu_xray_reports_preprocessed_df['indication'])\n        iu_xray_reports_preprocessed_df.to_csv(iu_xray_reports_preprocessed_df_path, index=False)\n        print(f\"Saved preprocessed 'indication' column to: {iu_xray_reports_preprocessed_df_path}\")\n    \n    if preprocess_findings and 'findings' in iu_xray_reports_preprocessed_df.columns:\n        print(\"Preprocessing Column: findings\")\n        iu_xray_reports_preprocessed_df['findings'] = iu_xray_reports_preprocessed_df['findings'].fillna('none').astype(str)\n        iu_xray_reports_preprocessed_df['findings'] = preprocess_text(iu_xray_reports_preprocessed_df['findings'])\n        iu_xray_reports_preprocessed_df.to_csv(iu_xray_reports_preprocessed_df_path, index=False)\n        print(f\"Saved preprocessed 'findings' column to: {iu_xray_reports_preprocessed_df_path}\")\n    \n    if preprocess_impression and 'impression' in iu_xray_reports_preprocessed_df.columns:\n        print(\"Preprocessing Column: impression\")\n        iu_xray_reports_preprocessed_df['impression'] = iu_xray_reports_preprocessed_df['impression'].fillna('none').astype(str)\n        iu_xray_reports_preprocessed_df['impression'] = preprocess_text(iu_xray_reports_preprocessed_df['impression'])\n        iu_xray_reports_preprocessed_df.to_csv(iu_xray_reports_preprocessed_df_path, index=False)\n        print(f\"Saved preprocessed 'impression' column to: {iu_xray_reports_preprocessed_df_path}\")\nelse:\n    print(f\"Preprocessed Text of DataFrame {iu_xray_reports_df_path} already exists at: {iu_xray_reports_preprocessed_df_path}\")\n    \n\n# displaying the preprocessed dataframe\niu_xray_reports_preprocessed_df = pd.read_csv(iu_xray_reports_preprocessed_df_path)\ndisplay(iu_xray_reports_preprocessed_df.head())","metadata":{"id":"M8sW8uz8-plE","execution":{"iopub.status.busy":"2024-10-19T18:58:25.165357Z","iopub.execute_input":"2024-10-19T18:58:25.165652Z","iopub.status.idle":"2024-10-19T18:58:25.751858Z","shell.execute_reply.started":"2024-10-19T18:58:25.165620Z","shell.execute_reply":"2024-10-19T18:58:25.750990Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n[nltk_data] Downloading package wordnet to /usr/share/nltk_data...\n[nltk_data]   Package wordnet is already up-to-date!\n[nltk_data] Downloading package stopwords to /usr/share/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\nPreprocessed Text of DataFrame ./datasets/iu_xray/output/iu_xray_reports_df.csv already exists at: ./datasets/iu_xray/output/iu_xray_reports_preprocessed_df.csv\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"   pmc_id                                           findings  \\\n0    3967  cardiomediastinal silhouette within normal lim...   \n1    3332  heart normal size contour mediastinal widening...   \n2      30  lungs clear without focal consolidation effusi...   \n3    2593  mild cardiomegaly unchanged stable superior me...   \n4    1165  frontal lateral views chest show normal size c...   \n\n                                          impression  \\\n0         acute radiographic cardiopulmonary process   \n1                acute cardiopulmonary abnormalities   \n2         negative acute cardiopulmonary abnormality   \n3  mild cardiomegaly interstitial prominence coul...   \n4     acute active cardiac pulmonary pleural disease   \n\n                           comparison  \\\n0                                none   \n1  radiograph chest lateral xxxx xxxx   \n2                                none   \n3                                none   \n4                                none   \n\n                                          indication  image_count  \\\n0                                         chest pain            2   \n1                                           weakness            3   \n2                      xxxx year old male chest pain            2   \n3                                          back pain            2   \n4  chest pain shortness breath patient lower abdo...            1   \n\n                                             image_1  \\\n0  CXR3967_IM-2028-1001.jpg: PA and lateral chest...   \n1  CXR3332_IM-1596-1001.jpg: Radiograph Chest PA ...   \n2  CXR30_IM-1385-1001.jpg: Chest x-XXXX XXXX and ...   \n3  CXR2593_IM-1084-1001.jpg: Chest, 2 views, XXXX...   \n4  CXR1165_IM-0110-1001.jpg: Xray Chest PA and La...   \n\n                                             image_2  \\\n0  CXR3967_IM-2028-2001.jpg: PA and lateral chest...   \n1  CXR3332_IM-1596-2001.jpg: Radiograph Chest PA ...   \n2  CXR30_IM-1385-2001.jpg: Chest x-XXXX XXXX and ...   \n3  CXR2593_IM-1084-2001.jpg: Chest, 2 views, XXXX...   \n4                                                NaN   \n\n                                             image_3 image_4 image_5  \n0                                                NaN     NaN     NaN  \n1  CXR3332_IM-1596-3001.jpg: Radiograph Chest PA ...     NaN     NaN  \n2                                                NaN     NaN     NaN  \n3                                                NaN     NaN     NaN  \n4                                                NaN     NaN     NaN  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>pmc_id</th>\n      <th>findings</th>\n      <th>impression</th>\n      <th>comparison</th>\n      <th>indication</th>\n      <th>image_count</th>\n      <th>image_1</th>\n      <th>image_2</th>\n      <th>image_3</th>\n      <th>image_4</th>\n      <th>image_5</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>3967</td>\n      <td>cardiomediastinal silhouette within normal lim...</td>\n      <td>acute radiographic cardiopulmonary process</td>\n      <td>none</td>\n      <td>chest pain</td>\n      <td>2</td>\n      <td>CXR3967_IM-2028-1001.jpg: PA and lateral chest...</td>\n      <td>CXR3967_IM-2028-2001.jpg: PA and lateral chest...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>3332</td>\n      <td>heart normal size contour mediastinal widening...</td>\n      <td>acute cardiopulmonary abnormalities</td>\n      <td>radiograph chest lateral xxxx xxxx</td>\n      <td>weakness</td>\n      <td>3</td>\n      <td>CXR3332_IM-1596-1001.jpg: Radiograph Chest PA ...</td>\n      <td>CXR3332_IM-1596-2001.jpg: Radiograph Chest PA ...</td>\n      <td>CXR3332_IM-1596-3001.jpg: Radiograph Chest PA ...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>30</td>\n      <td>lungs clear without focal consolidation effusi...</td>\n      <td>negative acute cardiopulmonary abnormality</td>\n      <td>none</td>\n      <td>xxxx year old male chest pain</td>\n      <td>2</td>\n      <td>CXR30_IM-1385-1001.jpg: Chest x-XXXX XXXX and ...</td>\n      <td>CXR30_IM-1385-2001.jpg: Chest x-XXXX XXXX and ...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>2593</td>\n      <td>mild cardiomegaly unchanged stable superior me...</td>\n      <td>mild cardiomegaly interstitial prominence coul...</td>\n      <td>none</td>\n      <td>back pain</td>\n      <td>2</td>\n      <td>CXR2593_IM-1084-1001.jpg: Chest, 2 views, XXXX...</td>\n      <td>CXR2593_IM-1084-2001.jpg: Chest, 2 views, XXXX...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1165</td>\n      <td>frontal lateral views chest show normal size c...</td>\n      <td>acute active cardiac pulmonary pleural disease</td>\n      <td>none</td>\n      <td>chest pain shortness breath patient lower abdo...</td>\n      <td>1</td>\n      <td>CXR1165_IM-0110-1001.jpg: Xray Chest PA and La...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"### **Create Data Loaders**","metadata":{"id":"MlOl6I0rMa68"}},{"cell_type":"code","source":"'''Image Data Loaders to Supply Dataset to Model in Batches'''\n\n# classes in dataset\nclass CustomImageDataset(Dataset):\n    def __init__(self, image_dir, transform=None):\n        self.image_dir = image_dir\n        self.transform = transform\n        self.image_files = [f for f in os.listdir(image_dir) if f.endswith(('.png', '.jpg', '.jpeg'))]\n\n    def __len__(self):\n        return len(self.image_files)\n\n    def __getitem__(self, idx):\n        img_path = os.path.join(self.image_dir, self.image_files[idx])\n        image = Image.open(img_path).convert('RGB')\n\n        if self.transform:\n            image = self.transform(image)\n\n        return image\n\n\n# function to load image data with transformation and batching\ndef load_preprocessed_images(image_dir, batch_size=32):\n    transform = transforms.Compose([\n        transforms.Resize((224, 224)),  \n        transforms.ToTensor(),\n        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n    ])\n\n    dataset = CustomImageDataset(image_dir, transform=transform)\n    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n    return dataloader","metadata":{"execution":{"iopub.status.busy":"2024-10-19T18:58:25.755177Z","iopub.execute_input":"2024-10-19T18:58:25.755509Z","iopub.status.idle":"2024-10-19T18:58:25.764725Z","shell.execute_reply.started":"2024-10-19T18:58:25.755470Z","shell.execute_reply":"2024-10-19T18:58:25.763573Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"'''Text Data Loaders to Supply Dataset to Model in Batches'''\n\n# classes in dataset\nclass CustomTextDataset(Dataset):\n    def __init__(self, text_list, tokenizer, max_length=512):\n        self.text_list = text_list\n        self.tokenizer = tokenizer\n        self.max_length = max_length\n\n    def __len__(self):\n        return len(self.text_list)\n\n    def __getitem__(self, idx):\n        text = self.text_list[idx]\n        encoding = self.tokenizer(\n            text,\n            truncation=True,\n            padding='max_length',\n            max_length=self.max_length,\n            return_tensors='pt'\n        )\n        return {key: tensor.squeeze(0) for key, tensor in encoding.items()}\n\n\n# function to load text data with batching\ndef load_preprocessed_texts(text_list, tokenizer, batch_size=32, max_length=512):\n    dataset = CustomTextDataset(text_list, tokenizer, max_length)\n    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n    return dataloader","metadata":{"execution":{"iopub.status.busy":"2024-10-19T18:58:25.766162Z","iopub.execute_input":"2024-10-19T18:58:25.766517Z","iopub.status.idle":"2024-10-19T18:58:25.777607Z","shell.execute_reply.started":"2024-10-19T18:58:25.766477Z","shell.execute_reply":"2024-10-19T18:58:25.776644Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":"## **Model Implementation**","metadata":{"id":"wQ0H-X3HMgS1"}},{"cell_type":"markdown","source":"### **Visual Extractor**","metadata":{"id":"PsELnlXpMjGX"}},{"cell_type":"code","source":"'''Visual Extractor to Extract Data from Image and Encode it Accordingly'''\n\n# defining device for gpu support\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\n\n# defining the visual extractor model using ResNet101 \nclass VisualExtractor(nn.Module):\n    def __init__(self, args):\n        super(VisualExtractor, self).__init__()\n        self.visual_extractor = args.visual_extractor\n        weights = models.ResNet101_Weights.DEFAULT if args.visual_extractor_pretrained else None  \n        model = getattr(models, self.visual_extractor)(weights=weights)\n        modules = list(model.children())[:-2]  \n        self.model = nn.Sequential(*modules)\n        self.avg_pool = nn.AdaptiveAvgPool2d((1, 1))\n        self.fc_layer = nn.Linear(model.fc.in_features, 2048) \n        \n    def forward(self, images):\n        patch_feats = self.model(images)\n        avg_feats = self.avg_pool(patch_feats).squeeze() \n        avg_feats = self.fc_layer(avg_feats)\n\n        batch_size, feat_size, _, _ = patch_feats.shape\n        patch_feats = patch_feats.view(batch_size, feat_size, -1).permute(0, 2, 1)\n        \n        final_embedding = torch.cat((avg_feats.unsqueeze(1), patch_feats), dim=1) \n        \n        return patch_feats, avg_feats, final_embedding\n\n\n# arguments for the visual extractor\nclass Args:\n    visual_extractor = 'resnet101' \n    visual_extractor_pretrained = True\n\n\n# initializing the model\nargs = Args()\nvisual_extractor = VisualExtractor(args).to(device)\n\n\n# function to extract features from images\ndef extract_features(images_dataloader):\n    all_patch_feats = []\n    all_avg_feats = []\n    all_final_embeddings = []\n\n    visual_extractor.eval() \n    with torch.no_grad():\n        for images in tqdm(images_dataloader):\n            images = images.to(device)\n            patch_feats, avg_feats, final_embedding = visual_extractor(images)\n            all_patch_feats.append(patch_feats.cpu()) \n            all_avg_feats.append(avg_feats.cpu())\n            all_final_embeddings.append(final_embedding.cpu())\n\n    all_patch_feats = torch.cat(all_patch_feats, dim=0)\n    all_avg_feats = torch.cat(all_avg_feats, dim=0)\n    all_final_embeddings = torch.cat(all_final_embeddings, dim=0)\n\n    return all_patch_feats, all_avg_feats, all_final_embeddings\n\n\n# function to save extracted features\ndef save_features(file_path, features):\n    print(f\"Saving features to {file_path}\")\n    torch.save(features, file_path)\n\n\n# function to lead the extracted features\ndef load_features(file_path):\n    if os.path.exists(file_path):\n        print(f\"Loading features from {file_path}\")\n        return torch.load(file_path, weights_only=True)\n    return None\n\n\n# initializing paths\nfeature_dir = output_directory\npatch_feats_file = os.path.join(output_directory, 'patch_feats.pt')\navg_feats_file = os.path.join(output_directory, 'avg_feats.pt')\nimage_embeddings_file = os.path.join(output_directory, 'image_embeddings.pt')\nimages_dataloader = load_preprocessed_images(iu_xray_images_preprocessed)\n\n\n# extracting and saving the extracted features\nif os.path.exists(patch_feats_file) and os.path.exists(avg_feats_file) and os.path.exists(image_embeddings_file):\n    print(\"All features are already precomputed and will be loaded.\")\n    patch_feats = load_features(patch_feats_file)\n    avg_feats = load_features(avg_feats_file)\n    image_embeddings = load_features(image_embeddings_file)\nelse:\n    print(\"Extracting features since they are not precomputed...\")\n    patch_feats, avg_feats, image_embeddings = extract_features(images_dataloader) \n        \n    os.makedirs(feature_dir, exist_ok=True)\n    save_features(patch_feats_file, patch_feats)\n    save_features(avg_feats_file, avg_feats)\n    save_features(image_embeddings_file, image_embeddings)\n\n\n# displaying sizes of the feature dataframes\nprint(\"Patch Features Shape:\", patch_feats.shape)\nprint(\"Average Features Shape:\", avg_feats.shape)\nprint(\"Final Embedding Shape:\", image_embeddings.shape)","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2024-10-19T18:58:25.779194Z","iopub.execute_input":"2024-10-19T18:58:25.779484Z","iopub.status.idle":"2024-10-19T18:58:32.604387Z","shell.execute_reply.started":"2024-10-19T18:58:25.779453Z","shell.execute_reply":"2024-10-19T18:58:32.603412Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stderr","text":"Downloading: \"https://download.pytorch.org/models/resnet101-cd907fc2.pth\" to /root/.cache/torch/hub/checkpoints/resnet101-cd907fc2.pth\n100%|██████████| 171M/171M [00:01<00:00, 143MB/s]  \n","output_type":"stream"},{"name":"stdout","text":"All features are already precomputed and will be loaded.\nLoading features from ./datasets/iu_xray/output/patch_feats.pt\nLoading features from ./datasets/iu_xray/output/avg_feats.pt\nLoading features from ./datasets/iu_xray/output/image_embeddings.pt\nPatch Features Shape: torch.Size([7470, 49, 2048])\nAverage Features Shape: torch.Size([7470, 2048])\nFinal Embedding Shape: torch.Size([7470, 50, 2048])\n","output_type":"stream"}]},{"cell_type":"code","source":"'''Visualizing Extracted Features using Plots'''\n\n# function to visualize features using PCA and t-SNE, including K-Means clustering\ndef visualize_features(features, title):\n    print(f\"Original feature shape: {features.shape}\")\n    \n    features = features.reshape(features.shape[0], -1) \n    \n    pca = PCA(n_components=2)\n    pca_result = pca.fit_transform(features)\n\n    tsne = TSNE(n_components=2)\n    tsne_result = tsne.fit_transform(features)\n\n    plt.figure(figsize=(12, 6))\n    plt.subplot(1, 2, 1)\n    plt.scatter(pca_result[:, 0], pca_result[:, 1], alpha=0.5)\n    plt.title(f\"PCA - {title}\")\n    plt.xlabel(\"PCA Component 1\")\n    plt.ylabel(\"PCA Component 2\")\n\n    plt.subplot(1, 2, 2)\n    plt.scatter(tsne_result[:, 0], tsne_result[:, 1], alpha=0.5)\n    plt.title(f\"t-SNE - {title}\")\n    plt.xlabel(\"t-SNE Component 1\")\n    plt.ylabel(\"t-SNE Component 2\")\n\n    plt.show()\n\n    num_clusters = 3\n    kmeans = KMeans(n_clusters=num_clusters)\n    labels = kmeans.fit_predict(features)\n    \n    plt.figure(figsize=(8, 8))\n    plt.scatter(tsne_result[:, 0], tsne_result[:, 1], c=labels, cmap='viridis', alpha=0.5)\n    plt.title('t-SNE Result with K-Means Clusters')\n    plt.colorbar()\n    plt.show()\n    plt.figure(figsize=(8, 8))\n    plt.scatter(pca_result[:, 0], pca_result[:, 1], c=labels, cmap='viridis', alpha=0.5)\n    plt.title('PCA Result with K-Means Clusters')\n    plt.colorbar()\n    plt.show()\n\n\n# function to visualize features using PCA and t-SNE, without clustering\ndef visualize_features_2(features, title):\n    print(f\"Original feature shape: {features.shape}\")\n    \n    flattened_features = features.reshape(features.shape[0], -1) \n    \n    pca = PCA(n_components=2)\n    pca_result = pca.fit_transform(flattened_features)\n\n    tsne = TSNE(n_components=2)\n    tsne_result = tsne.fit_transform(flattened_features)\n\n    plt.scatter(pca_result[:, 0], pca_result[:, 1], alpha=0.7)\n    plt.title(f'PCA: {title}')\n    plt.show()\n    \n    plt.scatter(tsne_result[:, 0], tsne_result[:, 1], alpha=0.7)\n    plt.title(f't-SNE: {title}')\n    plt.show()\n\n\n# visualizing average features\n# visualize_features(avg_feats.numpy(), \"Average Features\")\n# visualize_features(patch_feats.numpy(), \"Patch Features\")\n# visualize_features(image_embeddings.numpy(), \"Image Embeddings\")\n\n# visualize_features_2(avg_feats.numpy(), \"Average Features\")\n# visualize_features_2(patch_feats.numpy(), \"Patch Features\")\n# visualize_features_2(image_embeddings.numpy(), \"Image Embeddings\")","metadata":{"execution":{"iopub.status.busy":"2024-10-19T18:58:32.605566Z","iopub.execute_input":"2024-10-19T18:58:32.605842Z","iopub.status.idle":"2024-10-19T18:58:32.621101Z","shell.execute_reply.started":"2024-10-19T18:58:32.605812Z","shell.execute_reply":"2024-10-19T18:58:32.620165Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"markdown","source":"### **Text Encoder**","metadata":{"id":"D2kBZTiCMmsE"}},{"cell_type":"code","source":"'''Text Encoder'''\n\n# function to embed text\ndef embed_text(text_dataloader, model):\n    all_embeddings = []\n    \n    try:\n        for batch in text_dataloader:\n            if isinstance(batch, str):\n                inputs = tokenizer([batch], return_tensors=\"pt\", padding=True, truncation=True).to(device)\n            elif isinstance(batch, list):\n                inputs = tokenizer(batch, return_tensors=\"pt\", padding=True, truncation=True).to(device)\n            else:\n                raise ValueError(\"Batch must be of type str or List[str]\")\n\n            outputs = model(**inputs)\n            embeddings = outputs.last_hidden_state.mean(dim=1)\n            all_embeddings.append(embeddings)\n\n        if all_embeddings: \n            return torch.cat(all_embeddings, dim=0)\n        else:\n            if isinstance(model, SentenceTransformer):\n                return torch.empty(0, model.get_sentence_embedding_dimension()).to(device)\n            else:\n                return torch.empty(0, model.config.hidden_size).to(device)\n\n    except Exception as e:\n        print(f\"Error in embedding: {e}\")\n        if isinstance(model, SentenceTransformer):\n            return torch.empty(0, model.get_sentence_embedding_dimension()).to(device)\n        else:\n            return torch.empty(0, model.config.hidden_size).to(device)\n\n\n# function to compute cosine similarity\ndef compute_cosine_similarity(embeddings1, embeddings2):\n    embeddings1 = F.normalize(embeddings1, p=2, dim=1)\n    embeddings2 = F.normalize(embeddings2, p=2, dim=1)\n    cosine = torch.mm(embeddings1, embeddings2.t())\n    return cosine\n\n\n# defining device\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\n\n# loading tokenizer, bert model and sentence-bert model\ntokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\nbert_model = BertModel.from_pretrained('bert-base-uncased').to(device)\nsentence_model = SentenceTransformer('all-MiniLM-L6-v2').to(device)","metadata":{"execution":{"iopub.status.busy":"2024-10-19T19:04:40.201095Z","iopub.execute_input":"2024-10-19T19:04:40.201902Z","iopub.status.idle":"2024-10-19T19:04:56.599472Z","shell.execute_reply.started":"2024-10-19T19:04:40.201860Z","shell.execute_reply":"2024-10-19T19:04:56.598541Z"},"trusted":true},"execution_count":14,"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e315f397896b430698e71fa693bf4f5d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"726be6fedae04a05a4a849ff753dd72a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"00774455ff34481f8f715bf7d25b51e0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b8fdee5b55c84efe81f02df805741217"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f5665c1410e74b44a9fdbf188fb7b926"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5d982f419e5a4ce894464f911b142f07"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d84307f67c544761830def581648b9a0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/10.7k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bbadd1b3c0c8471aaca725730976899f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7542b821cb4243669d7d132c6989164b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/612 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ba17dfd6b748418687d5dad38e62df4d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/90.9M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3554698069854fcfbd5e34d58f6abbea"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/350 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8f2ef3bc51ab4d329db6b0655c6a1c5a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9adb59b12b6e4b8895d66602aed69953"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"14c7585f90474d8d8ef569044ea06590"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a5e2a503b33349c4a3e50f09e4a045b6"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"1_Pooling/config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c9b9ba7723fc4c349bef8744c7ba8c74"}},"metadata":{}}]},{"cell_type":"code","source":"'''Text Encoder - Text Encoder using Medical Knowledge'''\n\n# defining radiology dictionary\nradiology_dictionary = {\n    \"pleural\": ['hemithorax', 'effusion', 'pneumothorax', 'parenchymal'],\n    \"lung\": ['lungs', 'pulmonary', 'hilar', 'lobe', 'consolidation', 'atelectasis', 'edema', 'opacity', 'pneumonia'],\n    \"mediastinal\": ['mediastinum', 'diaphragm', 'hemidiaphragm'],\n    \"cardiac\": ['heart', 'cardiomegaly', 'cardiomediastinal', 'atrium', 'ventricle', 'retrocardiac'],\n    \"vascular\": ['aorta', 'venous', 'jugular', 'aortic', 'vasculature', 'cabg'],\n    \"osseous\": ['rib', 'sternal', 'subclavian', 'thoracic'],\n    \"trachea\": ['endotrachea'],\n    \"stomach\": [],\n    \"abdomen\": [],\n    \"tube\": ['clips'],\n    \"spine\": ['vertebral', 'degenerative'],\n    \"nodule\": ['mass'],\n    \"chest\": ['small', 'enlarged', 'unchanged', 'stable', 'silhouette', 'contours', 'size', 'focal', 'mild', 'acute']\n}\n\n\n# embedding dictionary and reports using bert, and historical medical reports using sentence transformer\ndictionary_dataloader = load_preprocessed_texts(radiology_dictionary, tokenizer)\ndictionary_embeddings = embed_text(dictionary_dataloader, bert_model)\n\n\n# saving embeddings\nembeddings_file_path = os.path.join(output_directory, 'dictionary_embeddings.pt')\nif not os.path.exists(embeddings_file_path):\n    torch.save(dictionary_embeddings.cpu(), embeddings_file_path)\n    print(f\"Dictionary embeddings saved to {embeddings_file_path}\")\n\n\n# displaying shape of embeddings\nprint(f\"Dictionary Embeddings Shape: {dictionary_embeddings.shape}\")","metadata":{"execution":{"iopub.status.busy":"2024-10-19T19:07:25.239151Z","iopub.execute_input":"2024-10-19T19:07:25.240286Z","iopub.status.idle":"2024-10-19T19:07:25.251448Z","shell.execute_reply.started":"2024-10-19T19:07:25.240230Z","shell.execute_reply":"2024-10-19T19:07:25.250454Z"},"trusted":true},"execution_count":22,"outputs":[{"name":"stdout","text":"Error in embedding: 0\nDictionary Embeddings Shape: torch.Size([0, 768])\n","output_type":"stream"}]},{"cell_type":"code","source":"'''Text Encoder -  Sentence Encoder using Medical History'''\n\n# reading and preprocessing medical history reports\niu_xray_reports_preprocessed_df_path = os.path.join(output_directory, 'iu_xray_reports_preprocessed_df.csv')\nmedical_history = pd.read_csv(iu_xray_reports_preprocessed_df_path)[\"findings\"].dropna().tolist()\nmedical_history = [str(report) for report in medical_history if isinstance(report, str) or pd.notna(report)]\n\n\n# encoding medical history using Sentence-BERT\nhistorical_dataloader = load_preprocessed_texts(medical_history, tokenizer)\nhistorical_embeddings = embed_text(historical_dataloader, sentence_model)\n\n\n# saving embeddings\nembeddings_file_path = os.path.join(output_directory, 'historical_embeddings.pt')\nif not os.path.exists(embeddings_file_path):\n    torch.save(dictionary_embeddings.cpu(), embeddings_file_path)\n    print(f\"Historical embeddings saved to {embeddings_file_path}\")\n\n\n# displaying shape of embeddings\nprint(f\"Historical Embeddings Shape: {historical_embeddings.shape}\")","metadata":{"execution":{"iopub.status.busy":"2024-10-19T19:10:34.784020Z","iopub.execute_input":"2024-10-19T19:10:34.784811Z","iopub.status.idle":"2024-10-19T19:10:34.885023Z","shell.execute_reply.started":"2024-10-19T19:10:34.784771Z","shell.execute_reply":"2024-10-19T19:10:34.884027Z"},"trusted":true},"execution_count":23,"outputs":[{"name":"stdout","text":"Error in embedding: Batch must be of type str or List[str]\nHistorical embeddings saved to ./datasets/iu_xray/output/historical_embeddings.pt\nHistorical Embeddings Shape: torch.Size([0, 384])\n","output_type":"stream"}]},{"cell_type":"code","source":"'''Text Encoder - Finding Reports Similar to Current Report'''\n\n# current report embedding\ncurrent_report = 'Heart size pulmonary vascularity appear within normal limits mild tortuosity descending thoracic aorta lungs free focal airspace disease pleural effusion pneumothorax seen discrete nodules adenopathy noted degenerative changes present spine'\ncurrent_embeddings = embed_text(current_report, sentence_model)\n\n\n# computing cosine similarity\nhistorical_embeddings_normalized = historical_embeddings / historical_embeddings.norm(dim=1, keepdim=True)\ncurrent_embeddings_normalized = current_embeddings / current_embeddings.norm(dim=1, keepdim=True)\n\nsimilarity_matrix = compute_cosine_similarity(dictionary_embeddings, historical_embeddings)\nprint(f\"Similarity Matrix Shape: {similarity_matrix.shape}\")\n\n\n# finding top-k relevant entries\nk = 5\ntop_k_indices = similarity_matrix.topk(k=k, dim=1).indices\n\n\n# preparing relevant entries based on indices\nrelevant_entries = []\nfor row in top_k_indices:\n    relevant_entries.append(medical_history[row.item()])\n\n\n# printing relevant entries for each report\nprint(f\"Relevant Entries for the Current Report: {relevant_entries}\")\n\n\n# update the historical embeddingx\nhistorical_embeddings.append(current_embeddings)","metadata":{"execution":{"iopub.status.busy":"2024-10-19T19:11:16.520038Z","iopub.execute_input":"2024-10-19T19:11:16.520892Z","iopub.status.idle":"2024-10-19T19:11:16.658663Z","shell.execute_reply.started":"2024-10-19T19:11:16.520840Z","shell.execute_reply":"2024-10-19T19:11:16.657206Z"},"trusted":true},"execution_count":25,"outputs":[{"name":"stdout","text":"Error in embedding: SentenceTransformer.forward() missing 1 required positional argument: 'input'\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","Cell \u001b[0;32mIn[25], line 12\u001b[0m\n\u001b[1;32m      9\u001b[0m historical_embeddings_normalized \u001b[38;5;241m=\u001b[39m historical_embeddings \u001b[38;5;241m/\u001b[39m historical_embeddings\u001b[38;5;241m.\u001b[39mnorm(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, keepdim\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     10\u001b[0m current_embeddings_normalized \u001b[38;5;241m=\u001b[39m current_embeddings \u001b[38;5;241m/\u001b[39m current_embeddings\u001b[38;5;241m.\u001b[39mnorm(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, keepdim\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m---> 12\u001b[0m similarity_matrix \u001b[38;5;241m=\u001b[39m \u001b[43mcompute_cosine_similarity\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdictionary_embeddings\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhistorical_embeddings\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSimilarity Matrix Shape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msimilarity_matrix\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# finding top-k relevant entries\u001b[39;00m\n","Cell \u001b[0;32mIn[14], line 40\u001b[0m, in \u001b[0;36mcompute_cosine_similarity\u001b[0;34m(embeddings1, embeddings2)\u001b[0m\n\u001b[1;32m     38\u001b[0m embeddings1 \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mnormalize(embeddings1, p\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     39\u001b[0m embeddings2 \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mnormalize(embeddings2, p\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m---> 40\u001b[0m cosine \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmm\u001b[49m\u001b[43m(\u001b[49m\u001b[43membeddings1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membeddings2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mt\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m cosine\n","\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (0x768 and 384x0)"],"ename":"RuntimeError","evalue":"mat1 and mat2 shapes cannot be multiplied (0x768 and 384x0)","output_type":"error"}]},{"cell_type":"markdown","source":"### **Multilevel Alignment**","metadata":{"id":"gIYzXGtsMqmQ"}},{"cell_type":"code","source":"'''Multilevel Alignment based on BLIP Architecture'''\n\n\n'''\nUse the reports and their corresponding image embeddings for that particular report, \nto find alignment for each report present and some sort of relation based off the BLIP architecture.\n\nimage_features = extract_image_features(df[['image_1', 'image_2', 'image_3']].values.flatten())\nsimilarity_matrix = cosine_similarity(findings_embeddings[0].detach().numpy(), image_features)\n\nCoarse Alignment (Cosine Similarity)\nFine-Grained Alignment (Attention)\n\n\n# Step 1: Encode Findings Text (Text Embeddings)\ndef get_findings_embeddings(findings_list):\n    encoded_findings = tokenizer(findings_list, return_tensors='pt', padding=True, truncation=True)\n    with torch.no_grad():\n        findings_embeddings = bert_model(**encoded_findings).last_hidden_state.mean(dim=1)  # Average pooling\n    return findings_embeddings\n\n# Step 2: Extract Image Features\ndef get_image_features(df):\n    images = df[['image_1', 'image_2', 'image_3', 'image_4', 'image_5']].values.flatten()\n    image_features = extract_image_features(images)\n    return image_features\n\n# Step 3: Coarse Alignment (Cosine Similarity)\ndef coarse_alignment(findings_embeddings, image_features):\n    # Compute cosine similarity between findings and image features\n    findings_embeddings_np = findings_embeddings.detach().numpy()\n    similarity_matrix = cosine_similarity(findings_embeddings_np, image_features)\n    return similarity_matrix\n\n# Step 4: Fine-Grained Alignment (Attention Mechanism)\nclass Attention(torch.nn.Module):\n    def __init__(self, embedding_dim):\n        super(Attention, self).__init__()\n        self.attention = torch.nn.Linear(embedding_dim, 1)\n\n    def forward(self, embeddings):\n        attn_weights = torch.nn.functional.softmax(self.attention(embeddings), dim=1)\n        weighted_embeddings = embeddings * attn_weights\n        return weighted_embeddings.sum(dim=1)\n\ndef fine_grained_alignment(findings_embeddings, image_features):\n    attention_layer = Attention(findings_embeddings.size(1))\n    weighted_findings = attention_layer(findings_embeddings)  # Attention on findings embeddings\n    weighted_image_features = attention_layer(torch.tensor(image_features))  # Attention on image features\n    return weighted_findings, weighted_image_features\n\n# Final Code Integration\ndef multilevel_alignment(df):\n    # Step 1: Findings Embeddings\n    findings_embeddings = get_findings_embeddings(df['findings'].tolist())\n\n    # Step 2: Image Features\n    image_features = get_image_features(df)\n\n    # Step 3: Coarse Alignment\n    similarity_matrix = coarse_alignment(findings_embeddings, image_features)\n    print(\"Coarse Alignment (Cosine Similarity):\", similarity_matrix)\n\n    # Step 4: Fine-Grained Alignment\n    weighted_findings, weighted_images = fine_grained_alignment(findings_embeddings, image_features)\n    print(\"Fine-Grained Alignment (Findings):\", weighted_findings)\n    print(\"Fine-Grained Alignment (Images):\", weighted_images)\n\n# Example usage\n# df is your DataFrame with the columns: findings, image_1, image_2, etc.\n# multilevel_alignment(df)\n\n\n\n\n\n'''\n\n\n\n# Load the BLIP model and processor\nprocessor = BlipProcessor.from_pretrained(\"Salesforce/blip\")\nmodel = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip\")\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel.to(device)\n\n\n'''\n# Define ITC and ITM losses\ndef itc_loss(image_embeddings, text_embeddings, temperature=0.1):\n    # Normalize embeddings\n    image_embeddings = nn.functional.normalize(image_embeddings, dim=1)\n    text_embeddings = nn.functional.normalize(text_embeddings, dim=1)\n    \n    # Calculate similarity\n    similarity_matrix = torch.matmul(image_embeddings, text_embeddings.t())\n    labels = torch.arange(similarity_matrix.size(0)).to(similarity_matrix.device)\n    \n    # ITC loss\n    loss = nn.CrossEntropyLoss()(similarity_matrix / temperature, labels)\n    return loss\n\n\ndef itm_loss(image_embeddings, text_embeddings, match_labels):\n    logits = torch.matmul(image_embeddings, text_embeddings.t())\n    return nn.BCEWithLogitsLoss()(logits, match_labels.float())\n\n\n# Define the alignment module\ndef align_with_dictionary(image_embedding, dictionary_knowledge):\n    # Simple alignment based on keywords\n    relevant_knowledge = {}\n    for key, value in dictionary_knowledge.items():\n        if key in image_embedding:  # Replace with actual keyword checking\n            relevant_knowledge[key] = value\n    return relevant_knowledge\n    \n\n# Extract and process embeddings\ndef get_embeddings(image, text):\n    inputs = processor(images=image, text=text, return_tensors=\"pt\").to(device)\n    with torch.no_grad():\n        outputs = model.get_image_features(**inputs)\n        image_embeddings = outputs.last_hidden_state.mean(dim=1)\n        text_embeddings = model.get_text_features(**inputs).last_hidden_state.mean(dim=1)\n    return image_embeddings, text_embeddings\n\n\n# Training step\ndef train_step(image, text, dictionary_knowledge, optimizer):\n    model.train()\n    image_embeddings, text_embeddings = get_embeddings(image, text)\n    \n    # ITC and ITM tasks\n    itc_loss_value = itc_loss(image_embeddings, text_embeddings)\n    match_labels = (text_embeddings > 0).float()  # Example match labels\n    itm_loss_value = itm_loss(image_embeddings, text_embeddings, match_labels)\n    \n    total_loss = itc_loss_value + itm_loss_value\n    \n    optimizer.zero_grad()\n    total_loss.backward()\n    optimizer.step()\n    \n    return total_loss.item()\n\n\n# Example dictionary knowledge\ndictionary_knowledge = radiology_dictionary\n\n\n# Training loop\nnum_epochs = 10  # Set your number of epochs\noptimizer = torch.optim.Adam(model.parameters(), lr=1e-5)\n\n\n# Assuming you have a data loader defined that provides (image, text) pairs\nfor epoch in range(num_epochs):\n    for image, text in data_loader:  # Replace with your actual data loader\n        image = image.to(device)  # Move image to the appropriate device\n        loss = train_step(image, text, dictionary_knowledge, optimizer)\n        print(f\"Epoch {epoch}, Loss: {loss}\")\n\n'''","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"'''Multilevel Alignment based on BLIP Architecture - Coarse Grained Alignment (Image Text Contrastive)'''\n\n'''\nImage Feature Extraction (ResNet101):\n\nExtracts features from images using a pre-trained ResNet.\nFunction: extract_image_features()\nText Embedding Extraction (BERT):\n\nEncodes textual findings into embeddings using BERT.\nFunction: get_findings_embeddings()\nProjection and Normalization:\n\nProjects both image features and text embeddings to a common 512-dimensional space and normalizes them.\nCode part:\npython\nCopy code\nimage_proj = F.normalize(self.image_projection(image_features), p=2, dim=-1)\ntext_proj = F.normalize(self.text_projection(text_embeddings), p=2, dim=-1)\nCosine Similarity Calculation:\n\nCalculates the cosine similarity between projected text and image embeddings.\nCode part:\npython\nCopy code\ncosine_sim = torch.matmul(text_proj, image_proj.T)\nCoarse Alignment Function (ITC):\n\nFunction: coarse_alignment()\nOutputs the cosine similarity between text and image pairs.\n'''","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"'''Multilevel Alignment based on BLIP Architecture - Fine Grained Alignment (Image Text Matching)'''\n\n\n'''\nTask List for ITM (Fine-Grained Alignment):\nBinary Classification Head (ITM):\n\nDetermines whether an image and text pair match (1) or not (0).\nCode part:\npython\nCopy code\nitm_logits = self.itm_head(text_proj * image_proj)\nFine-Grained Alignment Function (ITM):\n\nFunction: fine_grained_alignment()\nOutputs logits indicating the match/mismatch of image-text pairs.\n'''","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### **Report Generator**","metadata":{"id":"Uz4pEhW2MuBF"}},{"cell_type":"markdown","source":"### **Complete Model**","metadata":{"id":"Z_WeRBlDMwX2"}},{"cell_type":"markdown","source":"## **Training**","metadata":{"id":"NZjhDeJxMzgg"}},{"cell_type":"markdown","source":"### **Training**","metadata":{"id":"7Uk_G0K8M2H9"}},{"cell_type":"markdown","source":"## **Testing**","metadata":{"id":"OXhkaJVvM3Uu"}},{"cell_type":"markdown","source":"### **Testing**","metadata":{"id":"bzTZZUlQM401"}}]}