{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Bnpd7Zh0NPl2"
   },
   "source": [
    "## **Medical Report Summarisation using Medical Knowledge**\n",
    "\n",
    "### **References**\n",
    "\n",
    "**Main Reference**\n",
    "- Radiology report generation with medical knowledge and multilevel image-report alignment: A new method and its verification\n",
    "https://www.sciencedirect.com/science/article/pii/S0933365723002282\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oKdcyk1sMEtD"
   },
   "source": [
    "## **Data Collection**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p81rfgckMR9a"
   },
   "source": [
    "### **Collect Datasets**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AYR_B6u1ojJt",
    "outputId": "faaac1ed-b54b-4cfd-b3e4-cca51372318e"
   },
   "outputs": [],
   "source": [
    "'''Libraries Installation and Import'''\n",
    "\n",
    "# installling necessary libraries\n",
    "!pip -q install --user requests numpy pandas matplotlib tqdm Pillow opencv-python nltk pyspellchecker torch torchvision torchaudio transformers scikit-learn sentence-transformers\n",
    "\n",
    "# importing required libraries\n",
    "import os\n",
    "import re\n",
    "import csv\n",
    "import requests\n",
    "import tarfile\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import xml.etree.ElementTree as ET\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "import cv2\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from spellchecker import SpellChecker\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.models as models\n",
    "from torchvision import datasets\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import transformers\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from transformers import BlipProcessor, BlipForConditionalGeneration\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Setting Paths'''\n",
    "\n",
    "# project directory\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')\n",
    "# project_directory = '/content/drive/Othercomputers/My Laptop/CS550_ASMT_MRSMK/datasets'\n",
    "# project_directory = '/content/drive/MyDrive/Academics/CS550 Machine Learning/CS550 ASMT MRSMK/datasets'\n",
    "\n",
    "project_directory = \"./datasets\"\n",
    "dataset = 'iu_xray/'\n",
    "iu_xray_dataset = os.path.join(project_directory, dataset)\n",
    "\n",
    "\n",
    "# input directory\n",
    "input_directory = os.path.join(iu_xray_dataset, \"input\")\n",
    "\n",
    "images_dir = os.path.join(input_directory, \"images\")\n",
    "reports_dir = os.path.join(input_directory, \"reports\")\n",
    "iu_xray_images = images_dir\n",
    "iu_xray_reports = os.path.join(reports_dir, 'ecgen-radiology')\n",
    "\n",
    "\n",
    "# output directory \n",
    "output_directory = os.path.join(iu_xray_dataset, \"output\")\n",
    "os.makedirs(output_directory, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "49-0xiXplwH8",
    "outputId": "ea63eda3-908a-4f5f-eeec-a4019130e57f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://openi.nlm.nih.gov/imgs/collections/NLMCXR_png.tgz already exists at: ./datasets/iu_xray/input/images\n",
      "https://openi.nlm.nih.gov/imgs/collections/NLMCXR_reports.tgz already exists at: ./datasets/iu_xray/input/reports\n"
     ]
    }
   ],
   "source": [
    "'''Setup - Generalized'''\n",
    "\n",
    "# setup to download the IU X-Ray Dataset\n",
    "images_url = \"https://openi.nlm.nih.gov/imgs/collections/NLMCXR_png.tgz\"\n",
    "reports_url = \"https://openi.nlm.nih.gov/imgs/collections/NLMCXR_reports.tgz\"\n",
    "\n",
    "\n",
    "# function to check the file size of a given URL\n",
    "def get_file_size(url):\n",
    "    response = requests.head(url)\n",
    "    size_in_bytes = int(response.headers.get('Content-Length', 0))\n",
    "    size_in_mb = size_in_bytes / (1024 * 1024)\n",
    "    return size_in_mb\n",
    "\n",
    "\n",
    "# function to download and extract from a given url to a given directory\n",
    "def download_and_extract(url, save_dir):\n",
    "    file_name = url.split('/')[-1]\n",
    "    file_path = os.path.join(save_dir, file_name)\n",
    "\n",
    "    response = requests.get(url, stream=True)\n",
    "    total_size = int(response.headers.get('Content-Length', 0))\n",
    "    downloaded_size = 0\n",
    "\n",
    "    with open(file_path, 'wb') as file:\n",
    "        for chunk in response.iter_content(chunk_size=1024):\n",
    "            if chunk:\n",
    "                file.write(chunk)\n",
    "                downloaded_size += len(chunk)\n",
    "                percent_complete = (downloaded_size / total_size) * 100\n",
    "                print(f\"Downloaded {downloaded_size / (1024*1024):.2f} MB out of {total_size / (1024*1024):.2f} MB: {percent_complete:.2f}% complete\")\n",
    "\n",
    "    print(\"\\nDownload complete!\")\n",
    "\n",
    "    with tarfile.open(file_path, 'r:gz') as tar:\n",
    "        members = tar.getmembers()\n",
    "        total_files = len(members)\n",
    "\n",
    "        for idx, member in enumerate(members, start=1):\n",
    "            tar.extract(member, path=save_dir)\n",
    "            print(f\"Extracting File {idx} out of {total_files}: {member.name}\")\n",
    "\n",
    "    os.remove(file_path)\n",
    "\n",
    "\n",
    "# downloading  IU X-Ray dataset\n",
    "if not os.path.exists(images_dir):\n",
    "    images_size = get_file_size(images_url)\n",
    "    print(f\"Downloading {images_url} to: {images_dir} ({images_size:.2f} MB)\")\n",
    "    os.makedirs(images_dir, exist_ok=True)\n",
    "    download_and_extract(images_url, images_dir)\n",
    "    print(f\"Downloaded {images_url} to: {images_dir}\")\n",
    "else:\n",
    "    print(f\"{images_url} already exists at: {images_dir}\")\n",
    "\n",
    "if not os.path.exists(reports_dir):\n",
    "    reports_size = get_file_size(reports_url)\n",
    "    print(f\"Downloading {reports_url} to: {reports_dir} ({reports_size:.2f} MB)\")\n",
    "    os.makedirs(reports_dir, exist_ok=True)\n",
    "    download_and_extract(reports_url, reports_dir)\n",
    "    print(f\"Downloaded {reports_url} to: {reports_dir}\")\n",
    "else:\n",
    "    print(f\"{reports_url} already exists at: {reports_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5ZSA8Jyowaoh",
    "outputId": "dffa4899-00a7-4d49-a118-8dd85400ba06"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Path:  ./datasets/iu_xray/input/images\n",
      "Directory Contents: 7471 Images\n",
      "\n",
      "Path:  ./datasets/iu_xray/input/reports/ecgen-radiology\n",
      "Directory Contents: 3955 Reports\n"
     ]
    }
   ],
   "source": [
    "'''Exploring the IU X-Ray Dataset Contents'''\n",
    "\n",
    "# displaying directory and subdirectory contents\n",
    "print(\"\\nPath: \", iu_xray_images)\n",
    "print(f\"Directory Contents: {len(os.listdir(iu_xray_images))} Images\")\n",
    "\n",
    "print(\"\\nPath: \", iu_xray_reports)\n",
    "print(f\"Directory Contents: {len(os.listdir(iu_xray_reports))} Reports\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 822
    },
    "id": "qGczexPLUaN5",
    "outputId": "8c7528a1-7064-4802-c276-edef004f0d5b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataframe already exists at ./datasets/iu_xray/output/iu_xray_images_df.csv\n",
      "\n",
      "\n",
      "Dataframe Shape: (7470, 9)\n",
      "\n",
      "\n",
      "Dataframe Information:\n",
      "\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 7470 entries, 0 to 7469\n",
      "Data columns (total 9 columns):\n",
      " #   Column          Non-Null Count  Dtype \n",
      "---  ------          --------------  ----- \n",
      " 0   pmc_id          7470 non-null   int64 \n",
      " 1   image_filename  7470 non-null   object\n",
      " 2   caption         7468 non-null   object\n",
      " 3   comparison      5210 non-null   object\n",
      " 4   indication      7311 non-null   object\n",
      " 5   findings        6473 non-null   object\n",
      " 6   impression      7418 non-null   object\n",
      " 7   height          7470 non-null   int64 \n",
      " 8   width           7470 non-null   int64 \n",
      "dtypes: int64(3), object(6)\n",
      "memory usage: 525.4+ KB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Displaying Dataframe:\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pmc_id</th>\n",
       "      <th>image_filename</th>\n",
       "      <th>caption</th>\n",
       "      <th>comparison</th>\n",
       "      <th>indication</th>\n",
       "      <th>findings</th>\n",
       "      <th>impression</th>\n",
       "      <th>height</th>\n",
       "      <th>width</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>CXR1_1_IM-0001-3001.png</td>\n",
       "      <td>Xray Chest PA and Lateral</td>\n",
       "      <td>None.</td>\n",
       "      <td>Positive TB test</td>\n",
       "      <td>The cardiac silhouette and mediastinum size ar...</td>\n",
       "      <td>Normal chest x-XXXX.</td>\n",
       "      <td>624</td>\n",
       "      <td>512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>CXR1_1_IM-0001-4001.png</td>\n",
       "      <td>Xray Chest PA and Lateral</td>\n",
       "      <td>None.</td>\n",
       "      <td>Positive TB test</td>\n",
       "      <td>The cardiac silhouette and mediastinum size ar...</td>\n",
       "      <td>Normal chest x-XXXX.</td>\n",
       "      <td>420</td>\n",
       "      <td>512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10</td>\n",
       "      <td>CXR10_IM-0002-1001.png</td>\n",
       "      <td>PA and lateral chest x-XXXX XXXX.</td>\n",
       "      <td>Chest radiographs XXXX.</td>\n",
       "      <td>XXXX-year-old male, chest pain.</td>\n",
       "      <td>The cardiomediastinal silhouette is within nor...</td>\n",
       "      <td>No acute cardiopulmonary process.</td>\n",
       "      <td>624</td>\n",
       "      <td>512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10</td>\n",
       "      <td>CXR10_IM-0002-2001.png</td>\n",
       "      <td>PA and lateral chest x-XXXX XXXX.</td>\n",
       "      <td>Chest radiographs XXXX.</td>\n",
       "      <td>XXXX-year-old male, chest pain.</td>\n",
       "      <td>The cardiomediastinal silhouette is within nor...</td>\n",
       "      <td>No acute cardiopulmonary process.</td>\n",
       "      <td>420</td>\n",
       "      <td>512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>100</td>\n",
       "      <td>CXR100_IM-0002-1001.png</td>\n",
       "      <td>CHEST 2V FRONTAL/LATERAL XXXX, XXXX XXXX PM</td>\n",
       "      <td>None.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Both lungs are clear and expanded. Heart and m...</td>\n",
       "      <td>No active disease.</td>\n",
       "      <td>420</td>\n",
       "      <td>512</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   pmc_id           image_filename   \n",
       "0       1  CXR1_1_IM-0001-3001.png  \\\n",
       "1       1  CXR1_1_IM-0001-4001.png   \n",
       "2      10   CXR10_IM-0002-1001.png   \n",
       "3      10   CXR10_IM-0002-2001.png   \n",
       "4     100  CXR100_IM-0002-1001.png   \n",
       "\n",
       "                                         caption               comparison   \n",
       "0                      Xray Chest PA and Lateral                    None.  \\\n",
       "1                      Xray Chest PA and Lateral                    None.   \n",
       "2             PA and lateral chest x-XXXX XXXX.   Chest radiographs XXXX.   \n",
       "3             PA and lateral chest x-XXXX XXXX.   Chest radiographs XXXX.   \n",
       "4   CHEST 2V FRONTAL/LATERAL XXXX, XXXX XXXX PM                     None.   \n",
       "\n",
       "                        indication   \n",
       "0                 Positive TB test  \\\n",
       "1                 Positive TB test   \n",
       "2  XXXX-year-old male, chest pain.   \n",
       "3  XXXX-year-old male, chest pain.   \n",
       "4                              NaN   \n",
       "\n",
       "                                            findings   \n",
       "0  The cardiac silhouette and mediastinum size ar...  \\\n",
       "1  The cardiac silhouette and mediastinum size ar...   \n",
       "2  The cardiomediastinal silhouette is within nor...   \n",
       "3  The cardiomediastinal silhouette is within nor...   \n",
       "4  Both lungs are clear and expanded. Heart and m...   \n",
       "\n",
       "                          impression  height  width  \n",
       "0               Normal chest x-XXXX.     624    512  \n",
       "1               Normal chest x-XXXX.     420    512  \n",
       "2  No acute cardiopulmonary process.     624    512  \n",
       "3  No acute cardiopulmonary process.     420    512  \n",
       "4                 No active disease.     420    512  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "'''Processing Textual Data from each .xml Report File and Storing it in a .csv File'''\n",
    "\n",
    "# function to iterate through all .xml report files and storing them in a dataframe\n",
    "def save_images_df():\n",
    "    data = []\n",
    "    cnt = 0\n",
    "    for file in os.listdir(iu_xray_reports):\n",
    "        if file.endswith(\".xml\"):\n",
    "            cnt += 1\n",
    "            print(f\"Processing .xml File {cnt} out of {len(os.listdir(iu_xray_reports))}: {file}\")\n",
    "\n",
    "            file_path = os.path.join(iu_xray_reports, file)\n",
    "            try:\n",
    "                tree = ET.parse(file_path)\n",
    "                root = tree.getroot()\n",
    "\n",
    "                pmc_id = root.find('.//pmcId').attrib.get('id')\n",
    "\n",
    "                comparison = indication = findings = impression = None\n",
    "\n",
    "                for abstract in root.findall('.//AbstractText'):\n",
    "                    if abstract.attrib.get('Label') == 'COMPARISON':\n",
    "                        comparison = abstract.text\n",
    "                    elif abstract.attrib.get('Label') == 'INDICATION':\n",
    "                        indication = abstract.text\n",
    "                    elif abstract.attrib.get('Label') == 'FINDINGS':\n",
    "                        findings = abstract.text\n",
    "                    elif abstract.attrib.get('Label') == 'IMPRESSION':\n",
    "                        impression = abstract.text\n",
    "\n",
    "                for parent_image in root.findall('parentImage'):\n",
    "                    image_file = parent_image.attrib['id'] + \".png\"\n",
    "                    image_path = os.path.join(iu_xray_images, image_file)\n",
    "                    image = cv2.imread(image_path)\n",
    "\n",
    "                    if image is not None:\n",
    "                        height, width, channels = image.shape\n",
    "                        caption = parent_image.find('caption').text if parent_image.find('caption') is not None else None\n",
    "                        data.append([pmc_id, image_file, caption, comparison, indication, findings, impression, height, width])\n",
    "                    else:\n",
    "                        print(f\"Warning: Unable to read image {image_path}\")\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing file {file}: {e}\")\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "# creating a dataframe and saving it as .csv\n",
    "iu_xray_images_df_path = os.path.join(output_directory, 'iu_xray_images_df.csv')\n",
    "if not os.path.exists(iu_xray_images_df_path):\n",
    "    data = save_images_df()\n",
    "    columns = ['pmc_id', 'image_filename', 'caption', 'comparison', 'indication', 'findings', 'impression', 'height', 'width']\n",
    "    iu_xray_images_df = pd.DataFrame(data, columns=columns)\n",
    "    iu_xray_images_df.to_csv(iu_xray_images_df_path, index=False)\n",
    "    print(f\"Dataframe saved to {iu_xray_images_df_path}\")\n",
    "else:\n",
    "    print(f\"Dataframe already exists at {iu_xray_images_df_path}\")\n",
    "    iu_xray_images_df = pd.read_csv(iu_xray_images_df_path)\n",
    "\n",
    "\n",
    "# displaying the stored dataframe\n",
    "print(\"\\n\\nDataframe Shape:\", iu_xray_images_df.shape)\n",
    "\n",
    "print(\"\\n\\nDataframe Information:\\n\")\n",
    "display(iu_xray_images_df.info())\n",
    "\n",
    "print(\"\\n\\nDisplaying Dataframe:\\n\")\n",
    "display(iu_xray_images_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 893
    },
    "id": "vYnfzXXT0O6w",
    "outputId": "d2692046-ebe0-45ac-8291-37641f6d5958"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataframe already exists at ./datasets/iu_xray/output/iu_xray_reports_df.csv\n",
      "\n",
      "\n",
      "Dataframe Shape: (3955, 11)\n",
      "\n",
      "\n",
      "Dataframe Information:\n",
      "\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 3955 entries, 0 to 3954\n",
      "Data columns (total 11 columns):\n",
      " #   Column       Non-Null Count  Dtype \n",
      "---  ------       --------------  ----- \n",
      " 0   pmc_id       3955 non-null   int64 \n",
      " 1   findings     3425 non-null   object\n",
      " 2   impression   3921 non-null   object\n",
      " 3   comparison   2757 non-null   object\n",
      " 4   indication   3865 non-null   object\n",
      " 5   image_count  3955 non-null   int64 \n",
      " 6   image_1      3851 non-null   object\n",
      " 7   image_2      3405 non-null   object\n",
      " 8   image_3      197 non-null    object\n",
      " 9   image_4      16 non-null     object\n",
      " 10  image_5      1 non-null      object\n",
      "dtypes: int64(2), object(9)\n",
      "memory usage: 340.0+ KB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Displaying Dataframe:\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pmc_id</th>\n",
       "      <th>findings</th>\n",
       "      <th>impression</th>\n",
       "      <th>comparison</th>\n",
       "      <th>indication</th>\n",
       "      <th>image_count</th>\n",
       "      <th>image_1</th>\n",
       "      <th>image_2</th>\n",
       "      <th>image_3</th>\n",
       "      <th>image_4</th>\n",
       "      <th>image_5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>The cardiac silhouette and mediastinum size ar...</td>\n",
       "      <td>Normal chest x-XXXX.</td>\n",
       "      <td>None.</td>\n",
       "      <td>Positive TB test</td>\n",
       "      <td>2</td>\n",
       "      <td>CXR1_1_IM-0001-3001.jpg: Xray Chest PA and Lat...</td>\n",
       "      <td>CXR1_1_IM-0001-4001.jpg: Xray Chest PA and Lat...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10</td>\n",
       "      <td>The cardiomediastinal silhouette is within nor...</td>\n",
       "      <td>No acute cardiopulmonary process.</td>\n",
       "      <td>Chest radiographs XXXX.</td>\n",
       "      <td>XXXX-year-old male, chest pain.</td>\n",
       "      <td>2</td>\n",
       "      <td>CXR10_IM-0002-1001.jpg: PA and lateral chest x...</td>\n",
       "      <td>CXR10_IM-0002-2001.jpg: PA and lateral chest x...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>100</td>\n",
       "      <td>Both lungs are clear and expanded. Heart and m...</td>\n",
       "      <td>No active disease.</td>\n",
       "      <td>None.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>CXR100_IM-0002-1001.jpg:  CHEST 2V FRONTAL/LAT...</td>\n",
       "      <td>CXR100_IM-0002-2001.jpg:  CHEST 2V FRONTAL/LAT...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1000</td>\n",
       "      <td>There is XXXX increased opacity within the rig...</td>\n",
       "      <td>1. Increased opacity in the right upper lobe w...</td>\n",
       "      <td>XXXX PA and lateral chest radiographs</td>\n",
       "      <td>XXXX-year-old male, XXXX.</td>\n",
       "      <td>3</td>\n",
       "      <td>CXR1000_IM-0003-1001.jpg: PA and lateral chest...</td>\n",
       "      <td>CXR1000_IM-0003-2001.jpg: PA and lateral chest...</td>\n",
       "      <td>CXR1000_IM-0003-3001.jpg: PA and lateral chest...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1001</td>\n",
       "      <td>Interstitial markings are diffusely prominent ...</td>\n",
       "      <td>Diffuse fibrosis. No visible focal acute disease.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>dyspnea, subjective fevers, arthritis, immigra...</td>\n",
       "      <td>2</td>\n",
       "      <td>CXR1001_IM-0004-1001.jpg: CHEST 2V FRONTAL/LAT...</td>\n",
       "      <td>CXR1001_IM-0004-1002.jpg: CHEST 2V FRONTAL/LAT...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   pmc_id                                           findings   \n",
       "0       1  The cardiac silhouette and mediastinum size ar...  \\\n",
       "1      10  The cardiomediastinal silhouette is within nor...   \n",
       "2     100  Both lungs are clear and expanded. Heart and m...   \n",
       "3    1000  There is XXXX increased opacity within the rig...   \n",
       "4    1001  Interstitial markings are diffusely prominent ...   \n",
       "\n",
       "                                          impression   \n",
       "0                               Normal chest x-XXXX.  \\\n",
       "1                  No acute cardiopulmonary process.   \n",
       "2                                 No active disease.   \n",
       "3  1. Increased opacity in the right upper lobe w...   \n",
       "4  Diffuse fibrosis. No visible focal acute disease.   \n",
       "\n",
       "                              comparison   \n",
       "0                                  None.  \\\n",
       "1                Chest radiographs XXXX.   \n",
       "2                                  None.   \n",
       "3  XXXX PA and lateral chest radiographs   \n",
       "4                                    NaN   \n",
       "\n",
       "                                          indication  image_count   \n",
       "0                                   Positive TB test            2  \\\n",
       "1                    XXXX-year-old male, chest pain.            2   \n",
       "2                                                NaN            2   \n",
       "3                          XXXX-year-old male, XXXX.            3   \n",
       "4  dyspnea, subjective fevers, arthritis, immigra...            2   \n",
       "\n",
       "                                             image_1   \n",
       "0  CXR1_1_IM-0001-3001.jpg: Xray Chest PA and Lat...  \\\n",
       "1  CXR10_IM-0002-1001.jpg: PA and lateral chest x...   \n",
       "2  CXR100_IM-0002-1001.jpg:  CHEST 2V FRONTAL/LAT...   \n",
       "3  CXR1000_IM-0003-1001.jpg: PA and lateral chest...   \n",
       "4  CXR1001_IM-0004-1001.jpg: CHEST 2V FRONTAL/LAT...   \n",
       "\n",
       "                                             image_2   \n",
       "0  CXR1_1_IM-0001-4001.jpg: Xray Chest PA and Lat...  \\\n",
       "1  CXR10_IM-0002-2001.jpg: PA and lateral chest x...   \n",
       "2  CXR100_IM-0002-2001.jpg:  CHEST 2V FRONTAL/LAT...   \n",
       "3  CXR1000_IM-0003-2001.jpg: PA and lateral chest...   \n",
       "4  CXR1001_IM-0004-1002.jpg: CHEST 2V FRONTAL/LAT...   \n",
       "\n",
       "                                             image_3 image_4 image_5  \n",
       "0                                                NaN     NaN     NaN  \n",
       "1                                                NaN     NaN     NaN  \n",
       "2                                                NaN     NaN     NaN  \n",
       "3  CXR1000_IM-0003-3001.jpg: PA and lateral chest...     NaN     NaN  \n",
       "4                                                NaN     NaN     NaN  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "'''Processing Textual Data from each .xml Report File and Storing it in a .csv File'''\n",
    "\n",
    "# function to iterate through all .xml report files and storing them in a dataframe\n",
    "def save_reports_df():\n",
    "    data = []\n",
    "    cnt = 0\n",
    "    for file in os.listdir(iu_xray_reports):\n",
    "        if file.endswith(\".xml\"):\n",
    "            cnt += 1\n",
    "            print(f\"Processing .xml File {cnt} out of {len(os.listdir(iu_xray_reports))}: {file}\")\n",
    "\n",
    "            file_path = os.path.join(iu_xray_reports, file)\n",
    "            try:\n",
    "                tree = ET.parse(file_path)\n",
    "                root = tree.getroot()\n",
    "\n",
    "                pmc_id = root.find('.//pmcId').attrib.get('id')\n",
    "\n",
    "                comparison = indication = findings = impression = None\n",
    "\n",
    "                for abstract in root.findall('.//AbstractText'):\n",
    "                    if abstract.attrib.get('Label') == 'COMPARISON':\n",
    "                        comparison = abstract.text\n",
    "                    elif abstract.attrib.get('Label') == 'INDICATION':\n",
    "                        indication = abstract.text\n",
    "                    elif abstract.attrib.get('Label') == 'FINDINGS':\n",
    "                        findings = abstract.text\n",
    "                    elif abstract.attrib.get('Label') == 'IMPRESSION':\n",
    "                        impression = abstract.text\n",
    "\n",
    "                report_data = {\n",
    "                    'pmc_id': pmc_id,\n",
    "                    'findings': findings,\n",
    "                    'impression': impression,\n",
    "                    'comparison': comparison,\n",
    "                    'indication': indication,\n",
    "                }\n",
    "\n",
    "                parent_images = root.findall('parentImage')\n",
    "                report_data['image_count'] = len(parent_images)\n",
    "\n",
    "                for i, parent_image in enumerate(parent_images, start=1):\n",
    "                    image_file = parent_image.attrib['id'] + \".jpg\"\n",
    "                    caption = parent_image.find('caption').text if parent_image.find('caption') is not None else None\n",
    "                    report_data[f'image_{i}'] = f\"{image_file}: {caption}\" if caption else image_file\n",
    "\n",
    "                data.append(report_data)\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing file {file}: {e}\")\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "# creating a dataframe and saving it as .csv\n",
    "iu_xray_reports_df_path = os.path.join(output_directory, 'iu_xray_reports_df.csv')\n",
    "if not os.path.exists(iu_xray_reports_df_path):\n",
    "    data = save_reports_df()\n",
    "    iu_xray_reports_df = pd.DataFrame(data)\n",
    "    iu_xray_reports_df.to_csv(iu_xray_reports_df_path, index=False)\n",
    "    print(f\"Dataframe saved to {iu_xray_reports_df_path}\")\n",
    "else:\n",
    "    print(f\"Dataframe already exists at {iu_xray_reports_df_path}\")\n",
    "    iu_xray_reports_df = pd.read_csv(iu_xray_reports_df_path)\n",
    "\n",
    "\n",
    "# displaying the stored dataframe\n",
    "print(\"\\n\\nDataframe Shape:\", iu_xray_reports_df.shape)\n",
    "\n",
    "print(\"\\n\\nDataframe Information:\\n\")\n",
    "display(iu_xray_reports_df.info())\n",
    "\n",
    "print(\"\\n\\nDisplaying Dataframe:\\n\")\n",
    "display(iu_xray_reports_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 310
    },
    "id": "bw4Ylfa94M1o",
    "outputId": "5ef503a0-265c-423b-8cae-1d36b136134d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Number of Images per Report:\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>images_qty</th>\n",
       "      <th>reports_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>3208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>446</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   images_qty  reports_count\n",
       "0           2           3208\n",
       "1           1            446\n",
       "2           3            181\n",
       "3           0            104\n",
       "4           4             15\n",
       "5           5              1"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "'''Displaying the Number of Images per Report'''\n",
    "\n",
    "# displaying the distribution of number of images per report\n",
    "reports_count = iu_xray_reports_df['image_count'].value_counts().rename_axis('images_qty').reset_index(name='reports_count')\n",
    "print(\"\\n\\nNumber of Images per Report:\\n\")\n",
    "display(reports_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6UouFxQwMNeo"
   },
   "source": [
    "## **Data Preprocessing**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-cGf99_CMV47"
   },
   "source": [
    "### **Preprocess Images**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JTClOSxeSCOM",
    "outputId": "811cbc93-70e7-4d60-f213-26238488c44d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessed Images already exist at: ./datasets/iu_xray/output/images_preprocessed\n"
     ]
    }
   ],
   "source": [
    "'''Preprocessing Images - Resizing, Tensor Conversion and Normalization'''\n",
    "\n",
    "# function to preprocess and save images\n",
    "def preprocess_images(input_dir, output_dir):\n",
    "    preprocess = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ])\n",
    "\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    cnt = 0\n",
    "    for filename in os.listdir(input_dir):\n",
    "        if filename.endswith('.png'):\n",
    "            cnt += 1\n",
    "            print(f\"Preprocessing File {cnt} out of {len(os.listdir(input_dir))}: {filename}\")\n",
    "\n",
    "            image_path = os.path.join(input_dir, filename)\n",
    "            image = Image.open(image_path).convert('RGB')\n",
    "            processed_image = preprocess(image)\n",
    "\n",
    "            processed_image_path = os.path.join(output_dir, filename)\n",
    "\n",
    "            processed_image_pil = transforms.ToPILImage()(processed_image)\n",
    "            processed_image_pil.save(processed_image_path)\n",
    "\n",
    "\n",
    "# preprocessing images\n",
    "iu_xray_images_preprocessed = os.path.join(output_directory, 'images_preprocessed')\n",
    "if not os.path.exists(iu_xray_images_preprocessed):\n",
    "    print(f\"Preprocessing Images to: {iu_xray_images_preprocessed}\")\n",
    "    preprocess_images(iu_xray_images, iu_xray_images_preprocessed)\n",
    "    print(f\"Preprocessed Images saved to: {iu_xray_images_preprocessed}\")\n",
    "else:\n",
    "    print(f\"Preprocessed Images already exist at: {iu_xray_images_preprocessed}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lvdRoNqXMZlN"
   },
   "source": [
    "### **Preprocess Text**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "M8sW8uz8-plE"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/shivangi/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/shivangi/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/shivangi/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing Text of DataFrame ./datasets/iu_xray/output/iu_xray_reports_df.csv to: ./datasets/iu_xray/output/iu_xray_reports_preprocessed_sorted_df.csv\n",
      "Preprocessing Column: comparison\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|██▊                                     | 283/3955 [00:23<05:00, 12.23it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 116\u001b[0m\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPreprocessing Column: comparison\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    115\u001b[0m iu_xray_reports_preprocessed_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcomparison\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m iu_xray_reports_preprocessed_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcomparison\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mfillna(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnone\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mstr\u001b[39m)\n\u001b[0;32m--> 116\u001b[0m iu_xray_reports_preprocessed_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcomparison\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mpreprocess_text\u001b[49m\u001b[43m(\u001b[49m\u001b[43miu_xray_reports_preprocessed_df\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcomparison\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    117\u001b[0m iu_xray_reports_preprocessed_df\u001b[38;5;241m.\u001b[39mto_csv(iu_xray_reports_preprocessed_df_path, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSaved preprocessed \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcomparison\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m column to: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00miu_xray_reports_preprocessed_df_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[13], line 82\u001b[0m, in \u001b[0;36mpreprocess_text\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m     80\u001b[0m sentence \u001b[38;5;241m=\u001b[39m rem_two_letter_words(sentence)\n\u001b[1;32m     81\u001b[0m sentence \u001b[38;5;241m=\u001b[39m rem_stop_words(sentence)\n\u001b[0;32m---> 82\u001b[0m sentence \u001b[38;5;241m=\u001b[39m \u001b[43mcorrect_spelling\u001b[49m\u001b[43m(\u001b[49m\u001b[43msentence\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     83\u001b[0m sentence \u001b[38;5;241m=\u001b[39m rem_extra_spaces(sentence)\n\u001b[1;32m     85\u001b[0m preprocessed\u001b[38;5;241m.\u001b[39mappend(sentence)\n",
      "Cell \u001b[0;32mIn[13], line 61\u001b[0m, in \u001b[0;36mcorrect_spelling\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m     59\u001b[0m corrected \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     60\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m text\u001b[38;5;241m.\u001b[39msplit():\n\u001b[0;32m---> 61\u001b[0m     corrected_word \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(spell\u001b[38;5;241m.\u001b[39mcandidates(word))[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mspell\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcandidates\u001b[49m\u001b[43m(\u001b[49m\u001b[43mword\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01melse\u001b[39;00m word\n\u001b[1;32m     62\u001b[0m     corrected\u001b[38;5;241m.\u001b[39mappend(corrected_word)\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(corrected)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/spellchecker/spellchecker.py:185\u001b[0m, in \u001b[0;36mSpellChecker.candidates\u001b[0;34m(self, word)\u001b[0m\n\u001b[1;32m    183\u001b[0m \u001b[38;5;66;03m# if still not found, use the edit distance 1 to calc edit distance 2\u001b[39;00m\n\u001b[1;32m    184\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_distance \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[0;32m--> 185\u001b[0m     tmp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mknown(\u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__edit_distance_alt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mres\u001b[49m\u001b[43m)\u001b[49m))\n\u001b[1;32m    186\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m tmp:\n\u001b[1;32m    187\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m tmp\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/spellchecker/spellchecker.py:252\u001b[0m, in \u001b[0;36mSpellChecker.__edit_distance_alt\u001b[0;34m(self, words)\u001b[0m\n\u001b[1;32m    250\u001b[0m tmp_words \u001b[38;5;241m=\u001b[39m [ensure_unicode(w) \u001b[38;5;28;01mfor\u001b[39;00m w \u001b[38;5;129;01min\u001b[39;00m words]\n\u001b[1;32m    251\u001b[0m tmp \u001b[38;5;241m=\u001b[39m [w \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_case_sensitive \u001b[38;5;28;01melse\u001b[39;00m w\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;28;01mfor\u001b[39;00m w \u001b[38;5;129;01min\u001b[39;00m tmp_words \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_if_should_check(w)]\n\u001b[0;32m--> 252\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m [e2 \u001b[38;5;28;01mfor\u001b[39;00m e1 \u001b[38;5;129;01min\u001b[39;00m tmp \u001b[38;5;28;01mfor\u001b[39;00m e2 \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mknown(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39medit_distance_1(e1))]\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/spellchecker/spellchecker.py:252\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    250\u001b[0m tmp_words \u001b[38;5;241m=\u001b[39m [ensure_unicode(w) \u001b[38;5;28;01mfor\u001b[39;00m w \u001b[38;5;129;01min\u001b[39;00m words]\n\u001b[1;32m    251\u001b[0m tmp \u001b[38;5;241m=\u001b[39m [w \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_case_sensitive \u001b[38;5;28;01melse\u001b[39;00m w\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;28;01mfor\u001b[39;00m w \u001b[38;5;129;01min\u001b[39;00m tmp_words \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_if_should_check(w)]\n\u001b[0;32m--> 252\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m [e2 \u001b[38;5;28;01mfor\u001b[39;00m e1 \u001b[38;5;129;01min\u001b[39;00m tmp \u001b[38;5;28;01mfor\u001b[39;00m e2 \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mknown\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43medit_distance_1\u001b[49m\u001b[43m(\u001b[49m\u001b[43me1\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m]\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/spellchecker/spellchecker.py:197\u001b[0m, in \u001b[0;36mSpellChecker.known\u001b[0;34m(self, words)\u001b[0m\n\u001b[1;32m    190\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mknown\u001b[39m(\u001b[38;5;28mself\u001b[39m, words: typing\u001b[38;5;241m.\u001b[39mIterable[KeyT]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m typing\u001b[38;5;241m.\u001b[39mSet[\u001b[38;5;28mstr\u001b[39m]:\n\u001b[1;32m    191\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"The subset of `words` that appear in the dictionary of words\u001b[39;00m\n\u001b[1;32m    192\u001b[0m \n\u001b[1;32m    193\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m    194\u001b[0m \u001b[38;5;124;03m        words (list): List of words to determine which are in the corpus\u001b[39;00m\n\u001b[1;32m    195\u001b[0m \u001b[38;5;124;03m    Returns:\u001b[39;00m\n\u001b[1;32m    196\u001b[0m \u001b[38;5;124;03m        set: The set of those words from the input that are in the corpus\"\"\"\u001b[39;00m\n\u001b[0;32m--> 197\u001b[0m     tmp_words \u001b[38;5;241m=\u001b[39m [ensure_unicode(w) \u001b[38;5;28;01mfor\u001b[39;00m w \u001b[38;5;129;01min\u001b[39;00m words]\n\u001b[1;32m    198\u001b[0m     tmp \u001b[38;5;241m=\u001b[39m [w \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_case_sensitive \u001b[38;5;28;01melse\u001b[39;00m w\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;28;01mfor\u001b[39;00m w \u001b[38;5;129;01min\u001b[39;00m tmp_words]\n\u001b[1;32m    199\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m {w \u001b[38;5;28;01mfor\u001b[39;00m w \u001b[38;5;129;01min\u001b[39;00m tmp \u001b[38;5;28;01mif\u001b[39;00m w \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_word_frequency\u001b[38;5;241m.\u001b[39mdictionary \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_if_should_check(w)}\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/spellchecker/spellchecker.py:197\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    190\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mknown\u001b[39m(\u001b[38;5;28mself\u001b[39m, words: typing\u001b[38;5;241m.\u001b[39mIterable[KeyT]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m typing\u001b[38;5;241m.\u001b[39mSet[\u001b[38;5;28mstr\u001b[39m]:\n\u001b[1;32m    191\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"The subset of `words` that appear in the dictionary of words\u001b[39;00m\n\u001b[1;32m    192\u001b[0m \n\u001b[1;32m    193\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m    194\u001b[0m \u001b[38;5;124;03m        words (list): List of words to determine which are in the corpus\u001b[39;00m\n\u001b[1;32m    195\u001b[0m \u001b[38;5;124;03m    Returns:\u001b[39;00m\n\u001b[1;32m    196\u001b[0m \u001b[38;5;124;03m        set: The set of those words from the input that are in the corpus\"\"\"\u001b[39;00m\n\u001b[0;32m--> 197\u001b[0m     tmp_words \u001b[38;5;241m=\u001b[39m [\u001b[43mensure_unicode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mw\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m w \u001b[38;5;129;01min\u001b[39;00m words]\n\u001b[1;32m    198\u001b[0m     tmp \u001b[38;5;241m=\u001b[39m [w \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_case_sensitive \u001b[38;5;28;01melse\u001b[39;00m w\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;28;01mfor\u001b[39;00m w \u001b[38;5;129;01min\u001b[39;00m tmp_words]\n\u001b[1;32m    199\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m {w \u001b[38;5;28;01mfor\u001b[39;00m w \u001b[38;5;129;01min\u001b[39;00m tmp \u001b[38;5;28;01mif\u001b[39;00m w \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_word_frequency\u001b[38;5;241m.\u001b[39mdictionary \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_if_should_check(w)}\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "'''Preprocessing Text - Lowercasing, Decontracting, Punctuation Removal, Number Removal, Two-Letter Word Removal, Stop Word Removal, Spell Checking, Extra Space Removal'''\n",
    "\n",
    "# download nltk resources and initialize spell checker\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "spell = SpellChecker()\n",
    "\n",
    "\n",
    "# function to convert text to lowercase\n",
    "def lowercase(text):\n",
    "    return text.lower() if isinstance(text, str) else text\n",
    "\n",
    "\n",
    "# function to decontract words\n",
    "def decontracted(text):\n",
    "    if not isinstance(text, str):\n",
    "        return text\n",
    "    contractions = {\n",
    "        \"won't\": \"will not\", \"can't\": \"can not\", \"couldn't\": \"could not\",\n",
    "        \"shouldn't\": \"should not\", \"wouldn't\": \"would not\", \"n't\": \" not\",\n",
    "        \"'re\": \" are\", \"'s\": \" is\", \"'d\": \" would\", \"'ll\": \" will\",\n",
    "        \"'t\": \" not\", \"'ve\": \" have\", \"'m\": \" am\"\n",
    "    }\n",
    "    for contraction, full_form in contractions.items():\n",
    "        text = text.replace(contraction, full_form)\n",
    "    return text\n",
    "\n",
    "\n",
    "# function to remove punctuations\n",
    "def rem_punctuations(text):\n",
    "    return re.sub(r'[^\\w\\s]', ' ', text) if isinstance(text, str) else text\n",
    "\n",
    "\n",
    "# function to remove numbers\n",
    "def rem_numbers(text):\n",
    "    return re.sub(r'\\d+', ' ', text) if isinstance(text, str) else text\n",
    "\n",
    "\n",
    "# function to remove two-letter words except \"no\" and \"ct\"\n",
    "def rem_two_letter_words(text):\n",
    "    if not isinstance(text, str):\n",
    "        return text\n",
    "    return ' '.join(word for word in text.split() if len(word) > 2 or word in [\"no\", \"ct\"])\n",
    "\n",
    "\n",
    "# function to remove stop words\n",
    "def rem_stop_words(text):\n",
    "    if not isinstance(text, str):\n",
    "        return text\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    return ' '.join(word for word in text.split() if word not in stop_words)\n",
    "\n",
    "\n",
    "# function to correct spelling\n",
    "def correct_spelling(text):\n",
    "    if not isinstance(text, str):\n",
    "        return text\n",
    "    corrected = []\n",
    "    for word in text.split():\n",
    "        corrected_word = list(spell.candidates(word))[0] if spell.candidates(word) else word\n",
    "        corrected.append(corrected_word)\n",
    "    return ' '.join(corrected)\n",
    "\n",
    "\n",
    "# function to remove extra spaces\n",
    "def rem_extra_spaces(text):\n",
    "    return ' '.join(text.split()) if isinstance(text, str) else text\n",
    "\n",
    "\n",
    "# function to preprocess text\n",
    "def preprocess_text(data):\n",
    "    preprocessed = []\n",
    "    for sentence in tqdm(data.values):\n",
    "        sentence = str(sentence)\n",
    "        sentence = lowercase(sentence)\n",
    "        sentence = decontracted(sentence)\n",
    "        sentence = rem_punctuations(sentence)\n",
    "        sentence = rem_numbers(sentence)\n",
    "        sentence = rem_two_letter_words(sentence)\n",
    "        sentence = rem_stop_words(sentence)\n",
    "        sentence = correct_spelling(sentence)\n",
    "        sentence = rem_extra_spaces(sentence)\n",
    "        \n",
    "        preprocessed.append(sentence)\n",
    "\n",
    "    return preprocessed\n",
    "\n",
    "\n",
    "# path to the preprocessed dataframe\n",
    "iu_xray_reports_preprocessed_df_path = os.path.join(output_directory, 'iu_xray_reports_preprocessed_sorted_df.csv')\n",
    "# report_data_path = os.path.join(output_directory,'iu_xray_reports_preprocessed_sorted_df.csv')\n",
    "iu_xray_reports_preprocessed_df = iu_xray_reports_df.copy()\n",
    "\n",
    "\n",
    "# preprocessing text columns in the dataframe\n",
    "if os.path.exists(iu_xray_reports_preprocessed_df_path):\n",
    "    print(f\"Preprocessing Text of DataFrame {iu_xray_reports_df_path} to: {iu_xray_reports_preprocessed_df_path}\")\n",
    "    \n",
    "    preprocess_caption = True\n",
    "    preprocess_comparison = True\n",
    "    preprocess_indication = True\n",
    "    preprocess_findings = True\n",
    "    preprocess_impression = True\n",
    "    \n",
    "    if preprocess_caption and 'caption' in iu_xray_reports_preprocessed_df.columns:\n",
    "        print(\"Preprocessing Column: caption\")\n",
    "        iu_xray_reports_preprocessed_df['caption'] = iu_xray_reports_preprocessed_df['caption'].fillna('unknown').astype(str)\n",
    "        iu_xray_reports_preprocessed_df['caption'] = preprocess_text(iu_xray_reports_preprocessed_df['caption'])\n",
    "        iu_xray_reports_preprocessed_df.to_csv(iu_xray_reports_preprocessed_df_path, index=False)\n",
    "        print(f\"Saved preprocessed 'caption' column to: {iu_xray_reports_preprocessed_df_path}\")\n",
    "    \n",
    "    if preprocess_comparison and 'comparison' in iu_xray_reports_preprocessed_df.columns:\n",
    "        print(\"Preprocessing Column: comparison\")\n",
    "        iu_xray_reports_preprocessed_df['comparison'] = iu_xray_reports_preprocessed_df['comparison'].fillna('none').astype(str)\n",
    "        iu_xray_reports_preprocessed_df['comparison'] = preprocess_text(iu_xray_reports_preprocessed_df['comparison'])\n",
    "        iu_xray_reports_preprocessed_df.to_csv(iu_xray_reports_preprocessed_df_path, index=False)\n",
    "        print(f\"Saved preprocessed 'comparison' column to: {iu_xray_reports_preprocessed_df_path}\")\n",
    "    \n",
    "    if preprocess_indication and 'indication' in iu_xray_reports_preprocessed_df.columns:\n",
    "        print(\"Preprocessing Column: indication\")\n",
    "        iu_xray_reports_preprocessed_df['indication'] = iu_xray_reports_preprocessed_df['indication'].fillna('none').astype(str)\n",
    "        iu_xray_reports_preprocessed_df['indication'] = preprocess_text(iu_xray_reports_preprocessed_df['indication'])\n",
    "        iu_xray_reports_preprocessed_df.to_csv(iu_xray_reports_preprocessed_df_path, index=False)\n",
    "        print(f\"Saved preprocessed 'indication' column to: {iu_xray_reports_preprocessed_df_path}\")\n",
    "    \n",
    "    if preprocess_findings and 'findings' in iu_xray_reports_preprocessed_df.columns:\n",
    "        print(\"Preprocessing Column: findings\")\n",
    "        iu_xray_reports_preprocessed_df['findings'] = iu_xray_reports_preprocessed_df['findings'].fillna('none').astype(str)\n",
    "        iu_xray_reports_preprocessed_df['findings'] = preprocess_text(iu_xray_reports_preprocessed_df['findings'])\n",
    "        iu_xray_reports_preprocessed_df.to_csv(iu_xray_reports_preprocessed_df_path, index=False)\n",
    "        print(f\"Saved preprocessed 'findings' column to: {iu_xray_reports_preprocessed_df_path}\")\n",
    "    \n",
    "    if preprocess_impression and 'impression' in iu_xray_reports_preprocessed_df.columns:\n",
    "        print(\"Preprocessing Column: impression\")\n",
    "        iu_xray_reports_preprocessed_df['impression'] = iu_xray_reports_preprocessed_df['impression'].fillna('none').astype(str)\n",
    "        iu_xray_reports_preprocessed_df['impression'] = preprocess_text(iu_xray_reports_preprocessed_df['impression'])\n",
    "        iu_xray_reports_preprocessed_df.to_csv(iu_xray_reports_preprocessed_df_path, index=False)\n",
    "        print(f\"Saved preprocessed 'impression' column to: {iu_xray_reports_preprocessed_df_path}\")\n",
    "else:\n",
    "    print(f\"Preprocessed Text of DataFrame {iu_xray_reports_df_path} already exists at: {iu_xray_reports_preprocessed_df_path}\")\n",
    "    \n",
    "\n",
    "# displaying the preprocessed dataframe\n",
    "iu_xray_reports_preprocessed_df = pd.read_csv(iu_xray_reports_preprocessed_df_path)\n",
    "display(iu_xray_reports_preprocessed_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/shivangi/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/shivangi/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/shivangi/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   pmc_id                                           findings   \n",
      "0       1  cardiac silhouette mediastinum size within nor...  \\\n",
      "1       2  borderline cardiomegaly midline stereotomy xxx...   \n",
      "2       3                                               none   \n",
      "3       4  diffuse bilateral interstitial alveolar capaci...   \n",
      "4       5  cardiomediastinal silhouette pulmonary muscula...   \n",
      "\n",
      "                                             image_1   \n",
      "0  CXR1_1_IM-0001-3001.jpg: Xray Chest PA and Lat...  \\\n",
      "1  CXR2_IM-0652-1001.jpg: Chest, 2 views, frontal...   \n",
      "2   CXR3_IM-1384-1001.jpg: Xray Chest PA and Lateral   \n",
      "3  CXR4_IM-2050-1001.jpg: PA and lateral views of...   \n",
      "4  CXR5_IM-2117-1003002.jpg: Xray Chest PA and La...   \n",
      "\n",
      "                                             image_2  \n",
      "0  CXR1_1_IM-0001-4001.jpg: Xray Chest PA and Lat...  \n",
      "1  CXR2_IM-0652-2001.jpg: Chest, 2 views, frontal...  \n",
      "2   CXR3_IM-1384-2001.jpg: Xray Chest PA and Lateral  \n",
      "3  CXR4_IM-2050-2001.jpg: PA and lateral views of...  \n",
      "4  CXR5_IM-2117-1004003.jpg: Xray Chest PA and La...  \n",
      "Preprocessing Column: findings\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████| 3955/3955 [1:27:33<00:00,  1.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved preprocessed 'findings' column to: ./datasets/iu_xray/output/preprocessed_findings.csv\n",
      "Train data saved to: ./datasets/iu_xray/output/train_data.csv\n",
      "Validation data saved to: ./datasets/iu_xray/output/val_data.csv\n",
      "Test data saved to: ./datasets/iu_xray/output/test_data.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import nltk\n",
    "from spellchecker import SpellChecker\n",
    "from sklearn.model_selection import train_test_split\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Download necessary NLTK resources\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Initialize spell checker\n",
    "spell = SpellChecker()\n",
    "\n",
    "# Combined preprocessing functions\n",
    "def lowercase(text):\n",
    "    \"\"\"Convert text to lowercase.\"\"\"\n",
    "    return text.lower() if isinstance(text, str) else text\n",
    "\n",
    "def decontracted(text):\n",
    "    \"\"\"Decontract phrases in the text.\"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return text\n",
    "    contractions = {\n",
    "        \"won't\": \"will not\", \"can't\": \"can not\", \"couldn't\": \"could not\",\n",
    "        \"shouldn't\": \"should not\", \"wouldn't\": \"would not\", \"n't\": \" not\",\n",
    "        \"'re\": \" are\", \"'s\": \" is\", \"'d\": \" would\", \"'ll\": \" will\",\n",
    "        \"'t\": \" not\", \"'ve\": \" have\", \"'m\": \" am\"\n",
    "    }\n",
    "    for contraction, full_form in contractions.items():\n",
    "        text = text.replace(contraction, full_form)\n",
    "    return text\n",
    "\n",
    "def rem_punctuations(text):\n",
    "    \"\"\"Remove punctuations except for full stops.\"\"\"\n",
    "    return re.sub(r'[^\\w\\s.]', '', text) if isinstance(text, str) else text\n",
    "\n",
    "def rem_numbers(text):\n",
    "    \"\"\"Remove numbers and irrelevant text like 'XXXX'.\"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return text\n",
    "    text = re.sub(r'[xX]{2,}', '', text)  # Removes sequences like 'XXXX'\n",
    "    return re.sub(r'\\d+', '', text)\n",
    "\n",
    "def rem_two_letter_words(text):\n",
    "    \"\"\"Remove words with fewer than 2 characters except 'no' and 'ct'.\"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return text\n",
    "    return ' '.join(word for word in text.split() if len(word) > 2 or word in [\"no\", \"ct\"])\n",
    "\n",
    "def rem_stop_words(text):\n",
    "    \"\"\"Remove stop words.\"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return text\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    return ' '.join(word for word in text.split() if word not in stop_words)\n",
    "\n",
    "def correct_spelling(text):\n",
    "    \"\"\"Correct spelling using a spell checker.\"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return text\n",
    "    corrected = []\n",
    "    for word in text.split():\n",
    "        corrected_word = list(spell.candidates(word))[0] if spell.candidates(word) else word\n",
    "        corrected.append(corrected_word)\n",
    "    return ' '.join(corrected)\n",
    "\n",
    "def rem_extra_spaces(text):\n",
    "    \"\"\"Remove extra spaces.\"\"\"\n",
    "    return ' '.join(text.split()) if isinstance(text, str) else text\n",
    "\n",
    "def handle_fullstops(text):\n",
    "    \"\"\"Handle full stops, spacing around them, and remove multiple consecutive stops.\"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return text\n",
    "    text = re.sub(r'\\.\\.+', '.', text)  # Convert multiple full stops to single\n",
    "    return re.sub(r'\\.', ' . ', text)  # Add space around full stops\n",
    "\n",
    "def rem_apostrophes(text):\n",
    "    \"\"\"Remove apostrophes.\"\"\"\n",
    "    return re.sub(\"'\", '', text) if isinstance(text, str) else text\n",
    "\n",
    "# Combined text preprocessing function\n",
    "def preprocess_text(data):\n",
    "    \"\"\"Apply combined preprocessing steps.\"\"\"\n",
    "    preprocessed = []\n",
    "    for sentence in tqdm(data.values):\n",
    "        sentence = str(sentence)\n",
    "        sentence = lowercase(sentence)\n",
    "        sentence = decontracted(sentence)\n",
    "        sentence = rem_punctuations(sentence)\n",
    "        sentence = rem_numbers(sentence)\n",
    "        sentence = rem_two_letter_words(sentence)\n",
    "        sentence = rem_stop_words(sentence)\n",
    "        sentence = correct_spelling(sentence)\n",
    "        sentence = rem_apostrophes(sentence)\n",
    "        sentence = handle_fullstops(sentence)\n",
    "        sentence = rem_extra_spaces(sentence)\n",
    "        \n",
    "        preprocessed.append(sentence)\n",
    "\n",
    "    return preprocessed\n",
    "\n",
    "# Load your DataFrame (replace with actual path)\n",
    "input_path = os.path.join(output_directory, 'iu_xray_reports_sorted_df.csv')\n",
    "# output_directory = os.path.join(output_directory, 'iu_xray_reports_sorted_preprocessed_df.csv')\n",
    "df = pd.read_csv(input_path)\n",
    "\n",
    "# Select only the columns 'pmc_id', 'findings', 'image_1', and 'image_2'\n",
    "\n",
    "# Apply preprocessing on specific columns if they exist\n",
    "preprocess_columns = ['findings']\n",
    "for column in preprocess_columns:\n",
    "    if column in df.columns:\n",
    "        print(f\"Preprocessing Column: {column}\")\n",
    "        df[column] = df[column].fillna('none').astype(str)\n",
    "        df[column] = preprocess_text(df[column])\n",
    "        output_path = os.path.join(output_directory, f'preprocessed_{column}.csv')\n",
    "        df.to_csv(output_path, index=False)\n",
    "        print(f\"Saved preprocessed '{column}' column to: {output_path}\")\n",
    "\n",
    "# Split into Train/Validation/Test (70%/10%/20%)\n",
    "train_df, temp_df = train_test_split(df, test_size=0.3, random_state=42)\n",
    "val_df, test_df = train_test_split(temp_df, test_size=(2/3), random_state=42)\n",
    "\n",
    "# Save the splits\n",
    "train_path = os.path.join(output_directory, 'train_data.csv')\n",
    "val_path = os.path.join(output_directory, 'val_data.csv')\n",
    "test_path = os.path.join(output_directory, 'test_data.csv')\n",
    "\n",
    "train_df.to_csv(train_path, index=False)\n",
    "val_df.to_csv(val_path, index=False)\n",
    "test_df.to_csv(test_path, index=False)\n",
    "\n",
    "print(f\"Train data saved to: {train_path}\")\n",
    "print(f\"Validation data saved to: {val_path}\")\n",
    "print(f\"Test data saved to: {test_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the DataFrame: (792, 5)\n",
      "   pmc_id                                           findings   \n",
      "0     136  heart size normal lungs clear normal pneumonia...  \\\n",
      "1    3457                                               none   \n",
      "2     933  change lungs clear expanded heart mediastinum ...   \n",
      "3    1041  lucency crosses left posterior rib visualized ...   \n",
      "4    2538  normal heart size mediastinal contours abnorma...   \n",
      "\n",
      "                                             image_1   \n",
      "0   CXR136_IM-0233-1001.jpg:  PA and lateral chest.   \\\n",
      "1  CXR3457_IM-1678-1001.jpg: History preop XXXX f...   \n",
      "2  CXR933_IM-2431-1001.jpg: CHEST 2V FRONTAL/LATE...   \n",
      "3  CXR1041_IM-0033-1001.jpg: PA and lateral views...   \n",
      "4  CXR2538_IM-1050-1001.jpg: PA and lateral chest...   \n",
      "\n",
      "                                             image_2  index  \n",
      "0   CXR136_IM-0233-1002.jpg:  PA and lateral chest.       1  \n",
      "1  CXR3457_IM-1678-2001.jpg: History preop XXXX f...      2  \n",
      "2  CXR933_IM-2431-1002.jpg: CHEST 2V FRONTAL/LATE...      3  \n",
      "3  CXR1041_IM-0033-2001.jpg: PA and lateral views...      4  \n",
      "4  CXR2538_IM-1050-1002.jpg: PA and lateral chest...      5  \n",
      "Train data saved to: ./datasets/iu_xray/output/test_data.csv\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"./datasets/iu_xray/output/test_data.csv\")     \n",
    "df = df[['pmc_id', 'findings', 'image_1', 'image_2']]\n",
    "\n",
    "# Display the modified DataFrame to check the output\n",
    "\n",
    "df['index'] = range(1, len(df) + 1)\n",
    "print(\"Shape of the DataFrame:\", df.shape)\n",
    "print(df.head())\n",
    "df.to_csv(test_path, index=False)\n",
    "\n",
    "print(f\"Train data saved to: {test_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the DataFrame: (395, 5)\n",
      "Shape of the DataFrame: (383, 5)\n",
      "Train data saved to: ./datasets/iu_xray/output/filtered_val_data.csv\n"
     ]
    }
   ],
   "source": [
    "filtered_train_path = os.path.join(output_directory, 'filtered_train_data.csv')\n",
    "filtered_val_path = os.path.join(output_directory, 'filtered_val_data.csv')\n",
    "filtered_test_path = os.path.join(output_directory, 'filtered_test_data.csv')\n",
    "\n",
    "\n",
    "\n",
    "df = pd.read_csv(\"./datasets/iu_xray/output/val_data.csv\")  \n",
    "print(\"Shape of the DataFrame:\", df.shape)\n",
    "filtered_df = df.dropna(subset=['image_1', 'image_2'], how='all')\n",
    "print(\"Shape of the DataFrame:\", filtered_df.shape)\n",
    "filtered_df.to_csv(filtered_val_path, index=False)\n",
    "\n",
    "print(f\"Train data saved to: {filtered_val_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MlOl6I0rMa68"
   },
   "source": [
    "### **Create Data Loaders**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Image Data Loaders to Supply Dataset to Model in Batches'''\n",
    "\n",
    "# classes in dataset\n",
    "class CustomImageDataset(Dataset):\n",
    "    def __init__(self, image_dir, transform=None):\n",
    "        self.image_dir = image_dir\n",
    "        self.transform = transform\n",
    "        self.image_files = [f for f in os.listdir(image_dir) if f.endswith(('.png', '.jpg', '.jpeg'))]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.image_dir, self.image_files[idx])\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image\n",
    "\n",
    "\n",
    "# function to load image data with transformation and batching\n",
    "def load_preprocessed_images(image_dir, batch_size=32):\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),  \n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ])\n",
    "\n",
    "    dataset = CustomImageDataset(image_dir, transform=transform)\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Text Data Loaders to Supply Dataset to Model in Batches'''\n",
    "\n",
    "# classes in dataset\n",
    "class CustomTextDataset(Dataset):\n",
    "    def __init__(self, text_list, tokenizer, max_length=512):\n",
    "        self.text_list = text_list\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.text_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.text_list[idx]\n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            max_length=self.max_length,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        return {key: tensor.squeeze(0) for key, tensor in encoding.items()}\n",
    "\n",
    "\n",
    "# function to load text data with batching\n",
    "def load_preprocessed_texts(text_list, tokenizer, batch_size=32, max_length=512):\n",
    "    dataset = CustomTextDataset(text_list, tokenizer, max_length)\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wQ0H-X3HMgS1"
   },
   "source": [
    "## **Model Implementation**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PsELnlXpMjGX"
   },
   "source": [
    "### **Visual Extractor**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shivangi/.local/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/shivangi/.local/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet101_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet101_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pmc_id</th>\n",
       "      <th>findings</th>\n",
       "      <th>image_1</th>\n",
       "      <th>image_2</th>\n",
       "      <th>index</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3224</td>\n",
       "      <td>cardiomediastinal silhouette normal size conto...</td>\n",
       "      <td>CXR3224_IM-1524-1001.jpg: Radiographs of the c...</td>\n",
       "      <td>CXR3224_IM-1524-2001.jpg: Radiographs of the c...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3226</td>\n",
       "      <td>cardiac contours normal lungs clear thoracic s...</td>\n",
       "      <td>CXR3226_IM-1525-1001.jpg: Xray Chest PA and La...</td>\n",
       "      <td>CXR3226_IM-1525-2001.jpg: Xray Chest PA and La...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>113</td>\n",
       "      <td>heart mediastinum unremarkable two subcentimet...</td>\n",
       "      <td>CXR113_IM-0086-1001.jpg: PA and lateral views ...</td>\n",
       "      <td>CXR113_IM-0086-2001.jpg: PA and lateral views ...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2347</td>\n",
       "      <td>cardiomediastinal silhouette normal size conto...</td>\n",
       "      <td>CXR2347_IM-0912-1001.jpg:  PA and lateral views.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1101</td>\n",
       "      <td>heart pulmonary mediastinum within normal limi...</td>\n",
       "      <td>CXR1101_IM-0068-1001.jpg: Xray Chest PA and La...</td>\n",
       "      <td>CXR1101_IM-0068-2001.jpg: Xray Chest PA and La...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>3305</td>\n",
       "      <td>right pics tip overlying right brachycephalic ...</td>\n",
       "      <td>CXR3305_IM-1581-1001.jpg: Xray Chest PA and La...</td>\n",
       "      <td>CXR3305_IM-1581-4001.jpg: Xray Chest PA and La...</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1994</td>\n",
       "      <td>lungs clear pneumothorax pleural effusion norm...</td>\n",
       "      <td>CXR1994_IM-0651-1001.jpg: Two-view chest XXXX,...</td>\n",
       "      <td>CXR1994_IM-0651-2001.jpg: Two-view chest XXXX,...</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>3645</td>\n",
       "      <td>none</td>\n",
       "      <td>CXR3645_IM-1807-2001.jpg: Xray Chest PA and La...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>3523</td>\n",
       "      <td>heart lungs interval lungs clear expanded hear...</td>\n",
       "      <td>CXR3523_IM-1721-1001.jpg:  CHEST 2V FRONTAL/LA...</td>\n",
       "      <td>CXR3523_IM-1721-1002.jpg:  CHEST 2V FRONTAL/LA...</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>480</td>\n",
       "      <td>none</td>\n",
       "      <td>CXR480_IM-2104-1001.jpg: 2 view CHEST: XXXX, X...</td>\n",
       "      <td>CXR480_IM-2104-2001.jpg: 2 view CHEST: XXXX, X...</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   pmc_id                                           findings   \n",
       "0    3224  cardiomediastinal silhouette normal size conto...  \\\n",
       "1    3226  cardiac contours normal lungs clear thoracic s...   \n",
       "2     113  heart mediastinum unremarkable two subcentimet...   \n",
       "3    2347  cardiomediastinal silhouette normal size conto...   \n",
       "4    1101  heart pulmonary mediastinum within normal limi...   \n",
       "5    3305  right pics tip overlying right brachycephalic ...   \n",
       "6    1994  lungs clear pneumothorax pleural effusion norm...   \n",
       "7    3645                                               none   \n",
       "8    3523  heart lungs interval lungs clear expanded hear...   \n",
       "9     480                                               none   \n",
       "\n",
       "                                             image_1   \n",
       "0  CXR3224_IM-1524-1001.jpg: Radiographs of the c...  \\\n",
       "1  CXR3226_IM-1525-1001.jpg: Xray Chest PA and La...   \n",
       "2  CXR113_IM-0086-1001.jpg: PA and lateral views ...   \n",
       "3  CXR2347_IM-0912-1001.jpg:  PA and lateral views.    \n",
       "4  CXR1101_IM-0068-1001.jpg: Xray Chest PA and La...   \n",
       "5  CXR3305_IM-1581-1001.jpg: Xray Chest PA and La...   \n",
       "6  CXR1994_IM-0651-1001.jpg: Two-view chest XXXX,...   \n",
       "7  CXR3645_IM-1807-2001.jpg: Xray Chest PA and La...   \n",
       "8  CXR3523_IM-1721-1001.jpg:  CHEST 2V FRONTAL/LA...   \n",
       "9  CXR480_IM-2104-1001.jpg: 2 view CHEST: XXXX, X...   \n",
       "\n",
       "                                             image_2  index  \n",
       "0  CXR3224_IM-1524-2001.jpg: Radiographs of the c...      1  \n",
       "1  CXR3226_IM-1525-2001.jpg: Xray Chest PA and La...      2  \n",
       "2  CXR113_IM-0086-2001.jpg: PA and lateral views ...      3  \n",
       "3                                                NaN      4  \n",
       "4  CXR1101_IM-0068-2001.jpg: Xray Chest PA and La...      5  \n",
       "5  CXR3305_IM-1581-4001.jpg: Xray Chest PA and La...      6  \n",
       "6  CXR1994_IM-0651-2001.jpg: Two-view chest XXXX,...      7  \n",
       "7                                                NaN      8  \n",
       "8  CXR3523_IM-1721-1002.jpg:  CHEST 2V FRONTAL/LA...      9  \n",
       "9  CXR480_IM-2104-2001.jpg: 2 view CHEST: XXXX, X...     10  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting features since they are not precomputed...\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'image_count'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/core/indexes/base.py:3652\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3651\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 3652\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3653\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/_libs/index.pyx:147\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/_libs/index.pyx:176\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7080\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7088\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'image_count'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[33], line 136\u001b[0m\n\u001b[1;32m    131\u001b[0m \u001b[38;5;66;03m# os.makedirs(feature_dir, exist_ok=True)\u001b[39;00m\n\u001b[1;32m    132\u001b[0m \n\u001b[1;32m    133\u001b[0m \u001b[38;5;66;03m# Here you might want to set the model to training mode\u001b[39;00m\n\u001b[1;32m    134\u001b[0m visual_extractor\u001b[38;5;241m.\u001b[39mtrain()  \n\u001b[0;32m--> 136\u001b[0m patch_feats, avg_feats, final_embeddings \u001b[38;5;241m=\u001b[39m \u001b[43mextract_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43miu_xray_reports_df\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    137\u001b[0m patch_feats \u001b[38;5;241m=\u001b[39m patch_feats\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# Shape: (3851, 4096)\u001b[39;00m\n\u001b[1;32m    138\u001b[0m avg_feats \u001b[38;5;241m=\u001b[39m avg_feats\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m1\u001b[39m)      \u001b[38;5;66;03m# Shape: (3851, 2048)\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[33], line 82\u001b[0m, in \u001b[0;36mextract_features\u001b[0;34m(data_loader)\u001b[0m\n\u001b[1;32m     79\u001b[0m patch_feats, avg_feats, final_embeddings \u001b[38;5;241m=\u001b[39m [], [], []\n\u001b[1;32m     81\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m idx, row \u001b[38;5;129;01min\u001b[39;00m data_loader\u001b[38;5;241m.\u001b[39miterrows():\n\u001b[0;32m---> 82\u001b[0m     images \u001b[38;5;241m=\u001b[39m \u001b[43mload_images\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrow\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43miu_xray_images_preprocessed\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mto(device)  \u001b[38;5;66;03m# Load up to 2 images\u001b[39;00m\n\u001b[1;32m     84\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m images\u001b[38;5;241m.\u001b[39mnumel() \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:  \u001b[38;5;66;03m# Skip if no images were loaded\u001b[39;00m\n\u001b[1;32m     85\u001b[0m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[33], line 46\u001b[0m, in \u001b[0;36mload_images\u001b[0;34m(report_row, img_folder)\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_images\u001b[39m(report_row, img_folder):\n\u001b[1;32m     45\u001b[0m     images \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m---> 46\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mmin\u001b[39m(\u001b[43mreport_row\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mimage_count\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m, \u001b[38;5;241m2\u001b[39m)):  \u001b[38;5;66;03m# Limit to 2 images max\u001b[39;00m\n\u001b[1;32m     47\u001b[0m         img_filename \u001b[38;5;241m=\u001b[39m report_row[\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimage_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.png\u001b[39m\u001b[38;5;124m'\u001b[39m  \u001b[38;5;66;03m# Replace with .png\u001b[39;00m\n\u001b[1;32m     48\u001b[0m         img_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(img_folder, img_filename)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/core/series.py:1012\u001b[0m, in \u001b[0;36mSeries.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   1009\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_values[key]\n\u001b[1;32m   1011\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m key_is_scalar:\n\u001b[0;32m-> 1012\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_value\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1014\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_hashable(key):\n\u001b[1;32m   1015\u001b[0m     \u001b[38;5;66;03m# Otherwise index.get_value will raise InvalidIndexError\u001b[39;00m\n\u001b[1;32m   1016\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1017\u001b[0m         \u001b[38;5;66;03m# For labels that don't resolve as scalars like tuples and frozensets\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/core/series.py:1121\u001b[0m, in \u001b[0;36mSeries._get_value\u001b[0;34m(self, label, takeable)\u001b[0m\n\u001b[1;32m   1118\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_values[label]\n\u001b[1;32m   1120\u001b[0m \u001b[38;5;66;03m# Similar to Index.get_value, but we do not fall back to positional\u001b[39;00m\n\u001b[0;32m-> 1121\u001b[0m loc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1123\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(loc):\n\u001b[1;32m   1124\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_values[loc]\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/core/indexes/base.py:3654\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3652\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine\u001b[38;5;241m.\u001b[39mget_loc(casted_key)\n\u001b[1;32m   3653\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[0;32m-> 3654\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[1;32m   3655\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m   3656\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[1;32m   3657\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[1;32m   3658\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[1;32m   3659\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'image_count'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import models, transforms\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "import torch.optim as optim\n",
    "\n",
    "# Define paths for saving features\n",
    "patch_feats_file = os.path.join(output_directory, 'patch_feats.pt')\n",
    "avg_feats_file = os.path.join(output_directory, 'avg_feats.pt')\n",
    "final_embeddings_file = os.path.join(output_directory, 'final_embeddings.pt')\n",
    "\n",
    "# Define the transform for image preprocessing\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),  # Adjust size based on model input requirement\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # Normalization for ResNet\n",
    "])\n",
    "\n",
    "# Define VisualExtractor class\n",
    "class VisualExtractor(nn.Module):\n",
    "    def __init__(self, visual_extractor='resnet101', pretrained=True):\n",
    "        super(VisualExtractor, self).__init__()\n",
    "\n",
    "        model = getattr(models, visual_extractor)(pretrained=pretrained)\n",
    "        \n",
    "        # Remove the last fully connected layer\n",
    "        modules = list(model.children())[:-2]  \n",
    "        self.model = nn.Sequential(*modules)\n",
    "        \n",
    "        # Average pooling and a fully connected layer to transform the features\n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.fc_layer = nn.Linear(model.fc.in_features, 2048)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.model(x)\n",
    "        avg_feats = self.avg_pool(x).view(x.size(0), -1)\n",
    "        patch_feats = self.fc_layer(avg_feats)\n",
    "        return patch_feats, avg_feats\n",
    "\n",
    "# Load images function that handles the new DataFrame structure\n",
    "def load_images(report_row, img_folder):\n",
    "    images = []\n",
    "    for i in range(1, 3):  # image_1 and image_2\n",
    "        img_filename = report_row[f'image_{i}'].split('.')[0] + '.png'  # Replace with .png\n",
    "        img_path = os.path.join(img_folder, img_filename)\n",
    "        \n",
    "        # Check if the image file exists before attempting to open it\n",
    "        if os.path.exists(img_path):\n",
    "            img = Image.open(img_path).convert(\"RGB\")\n",
    "            images.append(transform(img))\n",
    "        else:\n",
    "            print(f\"Warning: Image file not found: {img_path}\")  # Warning if file not found\n",
    "\n",
    "    # If only one image was loaded, duplicate it\n",
    "    if len(images) == 1:\n",
    "        images.append(images[0].clone())  # Duplicate the single available image\n",
    "    \n",
    "    return torch.stack(images) if images else torch.tensor([])  # Return empty tensor if no images loaded\n",
    "\n",
    "# Initialize visual extractor\n",
    "visual_extractor = VisualExtractor()\n",
    "\n",
    "# Move the model to GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "visual_extractor.to(device)\n",
    "\n",
    "# Set learning rates and other parameters\n",
    "learning_rate = 5e-5  # 5 × 10^−5\n",
    "other_parameter = 1e-4  # Example of another parameter (like weight decay)\n",
    "\n",
    "# Create an optimizer\n",
    "optimizer = optim.Adam(visual_extractor.parameters(), lr=learning_rate, weight_decay=other_parameter)\n",
    "\n",
    "def extract_features(data_loader):\n",
    "    patch_feats, avg_feats, final_embeddings = [], [], []\n",
    "    \n",
    "    for idx, row in data_loader.iterrows():\n",
    "        images = load_images(row, iu_xray_images_preprocessed).to(device)  # Load up to 2 images\n",
    "        \n",
    "        if images.numel() == 0:  # Skip if no images were loaded\n",
    "            continue\n",
    "        \n",
    "        report_patch_feats, report_avg_feats = [], []\n",
    "        \n",
    "        # Iterate directly over the images tensor\n",
    "        for image in images:  # Process each image in the loaded images tensor\n",
    "            image = image.unsqueeze(0).to(device)  # Move single image to device\n",
    "            pf, af = visual_extractor(image)  # Extract patch_feats, avg_feats\n",
    "            print(f\"{row['index']}, {row['pmc_id']} Patch Features Shape: {pf.shape}, Avg Features Shape: {af.shape}\")\n",
    "            \n",
    "            # Store the output temporarily, releasing earlier references\n",
    "            report_patch_feats.append(pf.detach())\n",
    "            report_avg_feats.append(af.detach())\n",
    "\n",
    "        # Concatenate patch features and average the avg_feats\n",
    "        concatenated_patch_feats = torch.cat(report_patch_feats, dim=1)  # Concatenate along feature dimension\n",
    "        averaged_avg_feats = torch.mean(torch.stack(report_avg_feats), dim=0)  # Average the avg_feats\n",
    "\n",
    "        # Combine avg_feats with patch_feats to create final embedding\n",
    "        final_embedding = torch.cat((averaged_avg_feats, concatenated_patch_feats), dim=1)\n",
    "        patch_feats.append(concatenated_patch_feats)\n",
    "        avg_feats.append(averaged_avg_feats)\n",
    "        final_embeddings.append(final_embedding)\n",
    "        \n",
    "    return torch.stack(patch_feats), torch.stack(avg_feats), torch.stack(final_embeddings)\n",
    "\n",
    "# Functions to load and save features\n",
    "def load_features(file_path):\n",
    "    return torch.load(file_path)\n",
    "\n",
    "def save_features(file_path, features):\n",
    "    torch.save(features, file_path)\n",
    "\n",
    "# Load data\n",
    "report_data_path = os.path.join(output_directory, 'filtered_train_data.csv')\n",
    "iu_xray_reports_df = pd.read_csv(report_data_path)\n",
    "display(iu_xray_reports_df.head(10))\n",
    "\n",
    "# Check if features are already saved; if not, extract and save them\n",
    "if not os.path.exists(patch_feats_file) and os.path.exists(avg_feats_file) and os.path.exists(final_embeddings_file):\n",
    "    print(\"All features are already precomputed and will be loaded.\")\n",
    "    patch_feats = load_features(patch_feats_file)\n",
    "    avg_feats = load_features(avg_feats_file)\n",
    "    final_embeddings = load_features(final_embeddings_file)\n",
    "else:\n",
    "    print(\"Extracting features since they are not precomputed...\")\n",
    "    \n",
    "    # Set the model to training mode\n",
    "    visual_extractor.train()  \n",
    "    \n",
    "    patch_feats, avg_feats, final_embeddings = extract_features(iu_xray_reports_df)\n",
    "    patch_feats = patch_feats.squeeze(1)  # Shape: (num_reports, 4096)\n",
    "    avg_feats = avg_feats.squeeze(1)      # Shape: (num_reports, 2048)\n",
    "    final_embeddings = final_embeddings.squeeze(1)  # Shape: (num_reports, 6144)\n",
    "\n",
    "    save_features(patch_feats_file, patch_feats)\n",
    "    save_features(avg_feats_file, avg_feats)\n",
    "    save_features(final_embeddings_file, final_embeddings)\n",
    "\n",
    "# Displaying the shapes of the feature dataframes\n",
    "print(\"Patch Features Shape:\", patch_feats.shape)\n",
    "print(\"Average Features Shape:\", avg_feats.shape)\n",
    "print(\"Final Embedding Shape:\", final_embeddings.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patch_feats = patch_feats.squeeze(1)  # Shape: (3851, 4096)\n",
    "avg_feats = avg_feats.squeeze(1)      # Shape: (3851, 2048)\n",
    "final_embeddings = final_embeddings.squeeze(1)  # Shape: (3851, 6144)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Patch Features Shape: torch.Size([2705, 4096])\n",
      "Average Features Shape: torch.Size([2705, 2048])\n",
      "Final Embedding Shape: torch.Size([2705, 6144])\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "\n",
    "# Define paths for the saved features\n",
    "patch_feats_file = os.path.join(output_directory, 'patch_feats.pt')\n",
    "avg_feats_file = os.path.join(output_directory, 'avg_feats.pt')\n",
    "final_embeddings_file = os.path.join(output_directory, 'final_embeddings.pt')\n",
    "\n",
    "# Function to load features and print their shapes\n",
    "def print_tensor_shapes():\n",
    "    # Load patch features\n",
    "    if os.path.exists(patch_feats_file):\n",
    "        patch_feats = torch.load(patch_feats_file, map_location=torch.device('cpu'))\n",
    "        print(\"Patch Features Shape:\", patch_feats.shape)\n",
    "    else:\n",
    "        print(f\"Patch features file not found: {patch_feats_file}\")\n",
    "\n",
    "    # Load average features\n",
    "    if os.path.exists(avg_feats_file):\n",
    "        avg_feats = torch.load(avg_feats_file, map_location=torch.device('cpu'))\n",
    "        print(\"Average Features Shape:\", avg_feats.shape)\n",
    "    else:\n",
    "        print(f\"Average features file not found: {avg_feats_file}\")\n",
    "\n",
    "    # Load final embeddings\n",
    "    if os.path.exists(final_embeddings_file):\n",
    "        final_embeddings = torch.load(final_embeddings_file, map_location=torch.device('cpu'))\n",
    "        print(\"Final Embedding Shape:\", final_embeddings.shape)\n",
    "    else:\n",
    "        print(f\"Final embeddings file not found: {final_embeddings_file}\")\n",
    "\n",
    "# Call the function to print shapes\n",
    "print_tensor_shapes()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting features since they are not precomputed...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|█▋                                         | 9/234 [00:21<09:09,  2.44s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 95\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     94\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExtracting features since they are not precomputed...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 95\u001b[0m     patch_feats, avg_feats, image_embeddings \u001b[38;5;241m=\u001b[39m \u001b[43mextract_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages_dataloader\u001b[49m\u001b[43m)\u001b[49m \n\u001b[1;32m     97\u001b[0m     os\u001b[38;5;241m.\u001b[39mmakedirs(feature_dir, exist_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     98\u001b[0m     save_features(patch_feats_file, patch_feats)\n",
      "Cell \u001b[0;32mIn[18], line 53\u001b[0m, in \u001b[0;36mextract_features\u001b[0;34m(images_dataloader)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m images \u001b[38;5;129;01min\u001b[39;00m tqdm(images_dataloader):\n\u001b[1;32m     52\u001b[0m     images \u001b[38;5;241m=\u001b[39m images\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m---> 53\u001b[0m     patch_feats, avg_feats, final_embedding \u001b[38;5;241m=\u001b[39m \u001b[43mvisual_extractor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     54\u001b[0m     all_patch_feats\u001b[38;5;241m.\u001b[39mappend(patch_feats\u001b[38;5;241m.\u001b[39mcpu()) \n\u001b[1;32m     55\u001b[0m     all_avg_feats\u001b[38;5;241m.\u001b[39mappend(avg_feats\u001b[38;5;241m.\u001b[39mcpu())\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[18], line 20\u001b[0m, in \u001b[0;36mVisualExtractor.forward\u001b[0;34m(self, images)\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, images):\n\u001b[0;32m---> 20\u001b[0m     patch_feats \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     21\u001b[0m     avg_feats \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mavg_pool(patch_feats)\u001b[38;5;241m.\u001b[39msqueeze() \n\u001b[1;32m     22\u001b[0m     avg_feats \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc_layer(avg_feats)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/container.py:204\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    202\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    203\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 204\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    205\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/container.py:204\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    202\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    203\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 204\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    205\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torchvision/models/resnet.py:150\u001b[0m, in \u001b[0;36mBottleneck.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    147\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbn1(out)\n\u001b[1;32m    148\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrelu(out)\n\u001b[0;32m--> 150\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2\u001b[49m\u001b[43m(\u001b[49m\u001b[43mout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    151\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbn2(out)\n\u001b[1;32m    152\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrelu(out)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/conv.py:463\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    462\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 463\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/conv.py:459\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    455\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    456\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(F\u001b[38;5;241m.\u001b[39mpad(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode),\n\u001b[1;32m    457\u001b[0m                     weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride,\n\u001b[1;32m    458\u001b[0m                     _pair(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups)\n\u001b[0;32m--> 459\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    460\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "'''Visual Extractor to Extract Data from Image and Encode it Accordingly'''\n",
    "\n",
    "# defining device for gpu support\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "\n",
    "# defining the visual extractor model using ResNet101 \n",
    "class VisualExtractor(nn.Module):\n",
    "    def __init__(self, args):\n",
    "        super(VisualExtractor, self).__init__()\n",
    "        self.visual_extractor = args.visual_extractor\n",
    "        weights = models.ResNet101_Weights.DEFAULT if args.visual_extractor_pretrained else None  \n",
    "        model = getattr(models, self.visual_extractor)(weights=weights)\n",
    "        modules = list(model.children())[:-2]  \n",
    "        self.model = nn.Sequential(*modules)\n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.fc_layer = nn.Linear(model.fc.in_features, 2048) \n",
    "        \n",
    "    def forward(self, images):\n",
    "        patch_feats = self.model(images)\n",
    "        avg_feats = self.avg_pool(patch_feats).squeeze() \n",
    "        avg_feats = self.fc_layer(avg_feats)\n",
    "\n",
    "        batch_size, feat_size, _, _ = patch_feats.shape\n",
    "        patch_feats = patch_feats.view(batch_size, feat_size, -1).permute(0, 2, 1)\n",
    "        \n",
    "        final_embedding = torch.cat((avg_feats.unsqueeze(1), patch_feats), dim=1) \n",
    "        \n",
    "        return patch_feats, avg_feats, final_embedding\n",
    "\n",
    "\n",
    "# arguments for the visual extractor\n",
    "class Args:\n",
    "    visual_extractor = 'resnet101' \n",
    "    visual_extractor_pretrained = True\n",
    "\n",
    "\n",
    "# initializing the model\n",
    "args = Args()\n",
    "visual_extractor = VisualExtractor(args).to(device)\n",
    "\n",
    "\n",
    "# function to extract features from images\n",
    "def extract_features(images_dataloader):\n",
    "    all_patch_feats = []\n",
    "    all_avg_feats = []\n",
    "    all_final_embeddings = []\n",
    "\n",
    "    visual_extractor.eval() \n",
    "    with torch.no_grad():\n",
    "        for images in tqdm(images_dataloader):\n",
    "            images = images.to(device)\n",
    "            patch_feats, avg_feats, final_embedding = visual_extractor(images)\n",
    "            all_patch_feats.append(patch_feats.cpu()) \n",
    "            all_avg_feats.append(avg_feats.cpu())\n",
    "            all_final_embeddings.append(final_embedding.cpu())\n",
    "\n",
    "    all_patch_feats = torch.cat(all_patch_feats, dim=0)\n",
    "    all_avg_feats = torch.cat(all_avg_feats, dim=0)\n",
    "    all_final_embeddings = torch.cat(all_final_embeddings, dim=0)\n",
    "\n",
    "    return all_patch_feats, all_avg_feats, all_final_embeddings\n",
    "\n",
    "\n",
    "# function to save extracted features\n",
    "def save_features(file_path, features):\n",
    "    print(f\"Saving features to {file_path}\")\n",
    "    torch.save(features, file_path)\n",
    "\n",
    "\n",
    "# function to lead the extracted features\n",
    "def load_features(file_path):\n",
    "    if os.path.exists(file_path):\n",
    "        print(f\"Loading features from {file_path}\")\n",
    "        return torch.load(file_path, weights_only=True)\n",
    "    return None\n",
    "\n",
    "\n",
    "# initializing paths\n",
    "feature_dir = output_directory\n",
    "patch_feats_file = os.path.join(output_directory, 'patch_feats.pt')\n",
    "avg_feats_file = os.path.join(output_directory, 'avg_feats.pt')\n",
    "image_embeddings_file = os.path.join(output_directory, 'image_embeddings.pt')\n",
    "images_dataloader = load_preprocessed_images(iu_xray_images_preprocessed)\n",
    "\n",
    "\n",
    "# extracting and saving the extracted features\n",
    "if os.path.exists(patch_feats_file) and os.path.exists(avg_feats_file) and os.path.exists(image_embeddings_file):\n",
    "    print(\"All features are already precomputed and will be loaded.\")\n",
    "    patch_feats = load_features(patch_feats_file)\n",
    "    avg_feats = load_features(avg_feats_file)\n",
    "    image_embeddings = load_features(image_embeddings_file)\n",
    "else:\n",
    "    print(\"Extracting features since they are not precomputed...\")\n",
    "    patch_feats, avg_feats, image_embeddings = extract_features(images_dataloader) \n",
    "        \n",
    "    os.makedirs(feature_dir, exist_ok=True)\n",
    "    save_features(patch_feats_file, patch_feats)\n",
    "    save_features(avg_feats_file, avg_feats)\n",
    "    save_features(image_embeddings_file, image_embeddings)\n",
    "\n",
    "\n",
    "# displaying sizes of the feature dataframes\n",
    "print(\"Patch Features Shape:\", patch_feats.shape)\n",
    "print(\"Average Features Shape:\", avg_feats.shape)\n",
    "print(\"Final Embedding Shape:\", image_embeddings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-20T18:08:22.009202Z",
     "iopub.status.busy": "2024-10-20T18:08:22.008359Z",
     "iopub.status.idle": "2024-10-20T18:08:22.023380Z",
     "shell.execute_reply": "2024-10-20T18:08:22.022287Z",
     "shell.execute_reply.started": "2024-10-20T18:08:22.009163Z"
    }
   },
   "outputs": [],
   "source": [
    "'''Visualizing Extracted Features using Plots'''\n",
    "\n",
    "# function to visualize features using PCA and t-SNE, including K-Means clustering\n",
    "def visualize_features(features, title):\n",
    "    print(f\"Original feature shape: {features.shape}\")\n",
    "    \n",
    "    features = features.reshape(features.shape[0], -1) \n",
    "    \n",
    "    pca = PCA(n_components=2)\n",
    "    pca_result = pca.fit_transform(features)\n",
    "\n",
    "    tsne = TSNE(n_components=2)\n",
    "    tsne_result = tsne.fit_transform(features)\n",
    "\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.scatter(pca_result[:, 0], pca_result[:, 1], alpha=0.5)\n",
    "    plt.title(f\"PCA - {title}\")\n",
    "    plt.xlabel(\"PCA Component 1\")\n",
    "    plt.ylabel(\"PCA Component 2\")\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.scatter(tsne_result[:, 0], tsne_result[:, 1], alpha=0.5)\n",
    "    plt.title(f\"t-SNE - {title}\")\n",
    "    plt.xlabel(\"t-SNE Component 1\")\n",
    "    plt.ylabel(\"t-SNE Component 2\")\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "    num_clusters = 3\n",
    "    kmeans = KMeans(n_clusters=num_clusters)\n",
    "    labels = kmeans.fit_predict(features)\n",
    "    \n",
    "    plt.figure(figsize=(8, 8))\n",
    "    plt.scatter(tsne_result[:, 0], tsne_result[:, 1], c=labels, cmap='viridis', alpha=0.5)\n",
    "    plt.title('t-SNE Result with K-Means Clusters')\n",
    "    plt.colorbar()\n",
    "    plt.show()\n",
    "    plt.figure(figsize=(8, 8))\n",
    "    plt.scatter(pca_result[:, 0], pca_result[:, 1], c=labels, cmap='viridis', alpha=0.5)\n",
    "    plt.title('PCA Result with K-Means Clusters')\n",
    "    plt.colorbar()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# function to visualize features using PCA and t-SNE, without clustering\n",
    "def visualize_features_2(features, title):\n",
    "    print(f\"Original feature shape: {features.shape}\")\n",
    "    \n",
    "    flattened_features = features.reshape(features.shape[0], -1) \n",
    "    \n",
    "    pca = PCA(n_components=2)\n",
    "    pca_result = pca.fit_transform(flattened_features)\n",
    "\n",
    "    tsne = TSNE(n_components=2)\n",
    "    tsne_result = tsne.fit_transform(flattened_features)\n",
    "\n",
    "    plt.scatter(pca_result[:, 0], pca_result[:, 1], alpha=0.7)\n",
    "    plt.title(f'PCA: {title}')\n",
    "    plt.show()\n",
    "    \n",
    "    plt.scatter(tsne_result[:, 0], tsne_result[:, 1], alpha=0.7)\n",
    "    plt.title(f't-SNE: {title}')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# visualizing average features\n",
    "# visualize_features(avg_feats.numpy(), \"Average Features\")\n",
    "# visualize_features(patch_feats.numpy(), \"Patch Features\")\n",
    "# visualize_features(image_embeddings.numpy(), \"Image Embeddings\")\n",
    "\n",
    "# visualize_features_2(avg_feats.numpy(), \"Average Features\")\n",
    "# visualize_features_2(patch_feats.numpy(), \"Patch Features\")\n",
    "# visualize_features_2(image_embeddings.numpy(), \"Image Embeddings\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D2kBZTiCMmsE"
   },
   "source": [
    "### **Text Encoder**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-20T18:08:28.481410Z",
     "iopub.status.busy": "2024-10-20T18:08:28.481040Z",
     "iopub.status.idle": "2024-10-20T18:08:39.363790Z",
     "shell.execute_reply": "2024-10-20T18:08:39.363050Z",
     "shell.execute_reply.started": "2024-10-20T18:08:28.481374Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b87028cc066649089330d14a648f950d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f9ea4a053f254fd082fc3495d4a375ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f5f75b01cf149ffb87f1a8f9d282f3d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "32dda27e83184256a039bcfa484bd50a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f779c69d3de44969430f783544d5f56",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f21840528bef4de0aad74ae5de8dfc66",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "25d5982f417b4a039dfde980021342a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e5139e9721e14f63911b0f1d0402df26",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/10.7k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "407f9e8d48a240bd950ce3b0c9eb8af8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b70139e3124c47d08f21d19e54c195f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/612 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "46950fe9b3894016b24eba5460416eec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/90.9M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5cd6835e6c394805b4229c5a95eff15f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/350 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2353daac091943199f6fd6a532e3bc46",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f97bff5b78934350bcf25d221f36bd39",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "471cc0f693ae47d3a6a3542649469c13",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f85c00fc9d334efc905cc118fdb491d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "1_Pooling/config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "'''Text Encoder'''\n",
    "\n",
    "# function to embed text\n",
    "def embed_text(text_dataloader, model):\n",
    "    all_embeddings = []\n",
    "    \n",
    "    try:\n",
    "        for batch in text_dataloader:\n",
    "            if isinstance(batch, str):\n",
    "                inputs = tokenizer([batch], return_tensors=\"pt\", padding=True, truncation=True).to(device)\n",
    "            elif isinstance(batch, list):\n",
    "                inputs = tokenizer(batch, return_tensors=\"pt\", padding=True, truncation=True).to(device)\n",
    "            else:\n",
    "                raise ValueError(\"Batch must be of type str or List[str]\")\n",
    "\n",
    "            outputs = model(**inputs)\n",
    "            embeddings = outputs.last_hidden_state.mean(dim=1)\n",
    "            all_embeddings.append(embeddings)\n",
    "\n",
    "        if all_embeddings: \n",
    "            return torch.cat(all_embeddings, dim=0)\n",
    "        else:\n",
    "            if isinstance(model, SentenceTransformer):\n",
    "                return torch.empty(0, model.get_sentence_embedding_dimension()).to(device)\n",
    "            else:\n",
    "                return torch.empty(0, model.config.hidden_size).to(device)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error in embedding: {e}\")\n",
    "        if isinstance(model, SentenceTransformer):\n",
    "            return torch.empty(0, model.get_sentence_embedding_dimension()).to(device)\n",
    "        else:\n",
    "            return torch.empty(0, model.config.hidden_size).to(device)\n",
    "\n",
    "\n",
    "# function to compute cosine similarity\n",
    "def compute_cosine_similarity(embeddings1, embeddings2):\n",
    "    embeddings1 = F.normalize(embeddings1, p=2, dim=1)\n",
    "    embeddings2 = F.normalize(embeddings2, p=2, dim=1)\n",
    "    cosine = torch.mm(embeddings1, embeddings2.t())\n",
    "    return cosine\n",
    "\n",
    "\n",
    "# defining device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "\n",
    "# loading tokenizer, bert model and sentence-bert model\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "bert_model = BertModel.from_pretrained('bert-base-uncased').to(device)\n",
    "sentence_model = SentenceTransformer('all-MiniLM-L6-v2').to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-20T18:11:50.432032Z",
     "iopub.status.busy": "2024-10-20T18:11:50.431647Z",
     "iopub.status.idle": "2024-10-20T18:11:50.519562Z",
     "shell.execute_reply": "2024-10-20T18:11:50.518317Z",
     "shell.execute_reply.started": "2024-10-20T18:11:50.431994Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File already exists at ./datasets/iu_xray/output/radiology_terms.csv. No changes made.\n"
     ]
    },
    {
     "ename": "StopIteration",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mStopIteration\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 48\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(dictionary_csv, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m file:\n\u001b[1;32m     47\u001b[0m     reader \u001b[38;5;241m=\u001b[39m csv\u001b[38;5;241m.\u001b[39mreader(file)\n\u001b[0;32m---> 48\u001b[0m     \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mreader\u001b[49m\u001b[43m)\u001b[49m \n\u001b[1;32m     49\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m row \u001b[38;5;129;01min\u001b[39;00m reader:\n\u001b[1;32m     50\u001b[0m         text_list\u001b[38;5;241m.\u001b[39mappend(row[\u001b[38;5;241m1\u001b[39m]) \n",
      "\u001b[0;31mStopIteration\u001b[0m: "
     ]
    }
   ],
   "source": [
    "'''Text Encoder - Medical Knowledge encoder using TextEncoder -> note shape'''\n",
    "\n",
    "# defining radiology dictionary\n",
    "radiology_dictionary = {\n",
    "    \"pleural\": ['hemithorax', 'effusion', 'pneumothorax', 'parenchymal'],\n",
    "    \"lung\": ['lungs', 'pulmonary', 'hilar', 'lobe', 'consolidation', 'atelectasis', 'edema', 'opacity', 'pneumonia'],\n",
    "    \"mediastinal\": ['mediastinum', 'diaphragm', 'hemidiaphragm'],\n",
    "    \"cardiac\": ['heart', 'cardiomegaly', 'cardiomediastinal', 'atrium', 'ventricle', 'retrocardiac'],\n",
    "    \"vascular\": ['aorta', 'venous', 'jugular', 'aortic', 'vasculature', 'cabg'],\n",
    "    \"osseous\": ['rib', 'sternal', 'subclavian', 'thoracic'],\n",
    "    \"trachea\": ['endotrachea'],\n",
    "    \"stomach\": [],\n",
    "    \"abdomen\": [],\n",
    "    \"tube\": ['clips'],\n",
    "    \"spine\": ['vertebral', 'degenerative'],\n",
    "    \"nodule\": ['mass'],\n",
    "    \"chest\": ['small', 'enlarged', 'unchanged', 'stable', 'silhouette', 'contours', 'size', 'focal', 'mild', 'acute']\n",
    "}\n",
    "\n",
    "\n",
    "# Ensure the output directory exists\n",
    "os.makedirs(output_directory, exist_ok=True)\n",
    "\n",
    "\n",
    "# define the filename and full path\n",
    "filename = 'radiology_terms.csv'\n",
    "dictionary_csv = os.path.join(output_directory, filename) \n",
    "\n",
    "\n",
    "# Open the CSV file in write mode using the full path\n",
    "if not os.path.exists(dictionary_csv):    \n",
    "    with open(dictionary_csv, 'w', newline='') as file:  \n",
    "        writer = csv.writer(file)  \n",
    "        writer.writerow(['Category', 'Term']) \n",
    "    \n",
    "        for category, terms in radiology_dictionary.items():\n",
    "            for term in terms:\n",
    "                if term: \n",
    "                    writer.writerow([category, term]) \n",
    "                    \n",
    "    print(f\"Dictionary saved to {dictionary_csv}\")  \n",
    "else:     \n",
    "    print(f\"File already exists at {dictionary_csv}. No changes made.\")\n",
    "\n",
    "text_list = []\n",
    "with open(dictionary_csv, 'r') as file:\n",
    "    reader = csv.reader(file)\n",
    "    next(reader) \n",
    "    for row in reader:\n",
    "        text_list.append(row[1]) \n",
    "\n",
    "        \n",
    "# load the SentenceTransformer model\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# Generate embeddings\n",
    "embeddings = model.encode(text_list)\n",
    "\n",
    "# Display the shape of the embeddings\n",
    "print(f\"Embeddings Shape: {embeddings.shape}\")\n",
    "\n",
    "dictionary_pt = os.path.join(output_directory, 'dictionary_embeddings.pt')\n",
    "if not os.path.exists(dictionary_pt):    \n",
    "    torch.save(embeddings, dictionary_pt)\n",
    "    print(f\"Saved at : {dictionary_pt}\")\n",
    "else : print(f\"Already saved at path : {dictionary_pt}\")\n",
    "\n",
    "    \n",
    "# # embedding dictionary and reports using bert, and historical medical reports using sentence transformer\n",
    "# dictionary_dataloader = load_preprocessed_texts(radiology_dictionary, tokenizer)\n",
    "# dictionary_embeddings = embed_text(dictionary_dataloader, bert_model)\n",
    "\n",
    "\n",
    "# # saving embeddings\n",
    "# embeddings_file_path = os.path.join(output_directory, 'dictionary_embeddings.pt')\n",
    "# if not os.path.exists(embeddings_file_path):\n",
    "#     torch.save(dictionary_embeddings.cpu(), embeddings_file_path)\n",
    "#     print(f\"Dictionary embeddings saved to {embeddings_file_path}\")\n",
    "\n",
    "\n",
    "# # displaying shape of embeddings\n",
    "# print(f\"Dictionary Embeddings Shape: {dictionary_embeddings.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "all-MiniLM-L6-v2 is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'\nIf this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/huggingface_hub/utils/_http.py:406\u001b[0m, in \u001b[0;36mhf_raise_for_status\u001b[0;34m(response, endpoint_name)\u001b[0m\n\u001b[1;32m    405\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 406\u001b[0m     response\u001b[38;5;241m.\u001b[39mraise_for_status()\n\u001b[1;32m    407\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m HTTPError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/requests/models.py:1024\u001b[0m, in \u001b[0;36mResponse.raise_for_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1023\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m http_error_msg:\n\u001b[0;32m-> 1024\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HTTPError(http_error_msg, response\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m)\n",
      "\u001b[0;31mHTTPError\u001b[0m: 401 Client Error: Unauthorized for url: https://huggingface.co/all-MiniLM-L6-v2/resolve/main/tokenizer_config.json",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mRepositoryNotFoundError\u001b[0m                   Traceback (most recent call last)",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/transformers/utils/hub.py:403\u001b[0m, in \u001b[0;36mcached_file\u001b[0;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[1;32m    401\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    402\u001b[0m     \u001b[38;5;66;03m# Load from URL or cache if already cached\u001b[39;00m\n\u001b[0;32m--> 403\u001b[0m     resolved_file \u001b[38;5;241m=\u001b[39m hf_hub_download(\n\u001b[1;32m    404\u001b[0m         path_or_repo_id,\n\u001b[1;32m    405\u001b[0m         filename,\n\u001b[1;32m    406\u001b[0m         subfolder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(subfolder) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m subfolder,\n\u001b[1;32m    407\u001b[0m         repo_type\u001b[38;5;241m=\u001b[39mrepo_type,\n\u001b[1;32m    408\u001b[0m         revision\u001b[38;5;241m=\u001b[39mrevision,\n\u001b[1;32m    409\u001b[0m         cache_dir\u001b[38;5;241m=\u001b[39mcache_dir,\n\u001b[1;32m    410\u001b[0m         user_agent\u001b[38;5;241m=\u001b[39muser_agent,\n\u001b[1;32m    411\u001b[0m         force_download\u001b[38;5;241m=\u001b[39mforce_download,\n\u001b[1;32m    412\u001b[0m         proxies\u001b[38;5;241m=\u001b[39mproxies,\n\u001b[1;32m    413\u001b[0m         resume_download\u001b[38;5;241m=\u001b[39mresume_download,\n\u001b[1;32m    414\u001b[0m         token\u001b[38;5;241m=\u001b[39mtoken,\n\u001b[1;32m    415\u001b[0m         local_files_only\u001b[38;5;241m=\u001b[39mlocal_files_only,\n\u001b[1;32m    416\u001b[0m     )\n\u001b[1;32m    417\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m GatedRepoError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/huggingface_hub/utils/_deprecation.py:101\u001b[0m, in \u001b[0;36m_deprecate_arguments.<locals>._inner_deprecate_positional_args.<locals>.inner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    100\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(message, \u001b[38;5;167;01mFutureWarning\u001b[39;00m)\n\u001b[0;32m--> 101\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m f(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/huggingface_hub/utils/_validators.py:114\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[38;5;241m=\u001b[39mfn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, has_token\u001b[38;5;241m=\u001b[39mhas_token, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[0;32m--> 114\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/huggingface_hub/file_download.py:1232\u001b[0m, in \u001b[0;36mhf_hub_download\u001b[0;34m(repo_id, filename, subfolder, repo_type, revision, library_name, library_version, cache_dir, local_dir, user_agent, force_download, proxies, etag_timeout, token, local_files_only, headers, endpoint, legacy_cache_layout, resume_download, force_filename, local_dir_use_symlinks)\u001b[0m\n\u001b[1;32m   1231\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1232\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _hf_hub_download_to_cache_dir(\n\u001b[1;32m   1233\u001b[0m         \u001b[38;5;66;03m# Destination\u001b[39;00m\n\u001b[1;32m   1234\u001b[0m         cache_dir\u001b[38;5;241m=\u001b[39mcache_dir,\n\u001b[1;32m   1235\u001b[0m         \u001b[38;5;66;03m# File info\u001b[39;00m\n\u001b[1;32m   1236\u001b[0m         repo_id\u001b[38;5;241m=\u001b[39mrepo_id,\n\u001b[1;32m   1237\u001b[0m         filename\u001b[38;5;241m=\u001b[39mfilename,\n\u001b[1;32m   1238\u001b[0m         repo_type\u001b[38;5;241m=\u001b[39mrepo_type,\n\u001b[1;32m   1239\u001b[0m         revision\u001b[38;5;241m=\u001b[39mrevision,\n\u001b[1;32m   1240\u001b[0m         \u001b[38;5;66;03m# HTTP info\u001b[39;00m\n\u001b[1;32m   1241\u001b[0m         endpoint\u001b[38;5;241m=\u001b[39mendpoint,\n\u001b[1;32m   1242\u001b[0m         etag_timeout\u001b[38;5;241m=\u001b[39metag_timeout,\n\u001b[1;32m   1243\u001b[0m         headers\u001b[38;5;241m=\u001b[39mheaders,\n\u001b[1;32m   1244\u001b[0m         proxies\u001b[38;5;241m=\u001b[39mproxies,\n\u001b[1;32m   1245\u001b[0m         token\u001b[38;5;241m=\u001b[39mtoken,\n\u001b[1;32m   1246\u001b[0m         \u001b[38;5;66;03m# Additional options\u001b[39;00m\n\u001b[1;32m   1247\u001b[0m         local_files_only\u001b[38;5;241m=\u001b[39mlocal_files_only,\n\u001b[1;32m   1248\u001b[0m         force_download\u001b[38;5;241m=\u001b[39mforce_download,\n\u001b[1;32m   1249\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/huggingface_hub/file_download.py:1339\u001b[0m, in \u001b[0;36m_hf_hub_download_to_cache_dir\u001b[0;34m(cache_dir, repo_id, filename, repo_type, revision, endpoint, etag_timeout, headers, proxies, token, local_files_only, force_download)\u001b[0m\n\u001b[1;32m   1338\u001b[0m     \u001b[38;5;66;03m# Otherwise, raise appropriate error\u001b[39;00m\n\u001b[0;32m-> 1339\u001b[0m     _raise_on_head_call_error(head_call_error, force_download, local_files_only)\n\u001b[1;32m   1341\u001b[0m \u001b[38;5;66;03m# From now on, etag, commit_hash, url and size are not None.\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/huggingface_hub/file_download.py:1854\u001b[0m, in \u001b[0;36m_raise_on_head_call_error\u001b[0;34m(head_call_error, force_download, local_files_only)\u001b[0m\n\u001b[1;32m   1852\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(head_call_error, RepositoryNotFoundError) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(head_call_error, GatedRepoError):\n\u001b[1;32m   1853\u001b[0m     \u001b[38;5;66;03m# Repo not found or gated => let's raise the actual error\u001b[39;00m\n\u001b[0;32m-> 1854\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m head_call_error\n\u001b[1;32m   1855\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1856\u001b[0m     \u001b[38;5;66;03m# Otherwise: most likely a connection issue or Hub downtime => let's warn the user\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/huggingface_hub/file_download.py:1746\u001b[0m, in \u001b[0;36m_get_metadata_or_catch_error\u001b[0;34m(repo_id, filename, repo_type, revision, endpoint, proxies, etag_timeout, headers, token, local_files_only, relative_filename, storage_folder)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1746\u001b[0m     metadata \u001b[38;5;241m=\u001b[39m get_hf_file_metadata(\n\u001b[1;32m   1747\u001b[0m         url\u001b[38;5;241m=\u001b[39murl, proxies\u001b[38;5;241m=\u001b[39mproxies, timeout\u001b[38;5;241m=\u001b[39metag_timeout, headers\u001b[38;5;241m=\u001b[39mheaders, token\u001b[38;5;241m=\u001b[39mtoken\n\u001b[1;32m   1748\u001b[0m     )\n\u001b[1;32m   1749\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m EntryNotFoundError \u001b[38;5;28;01mas\u001b[39;00m http_error:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/huggingface_hub/utils/_validators.py:114\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[38;5;241m=\u001b[39mfn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, has_token\u001b[38;5;241m=\u001b[39mhas_token, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[0;32m--> 114\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/huggingface_hub/file_download.py:1666\u001b[0m, in \u001b[0;36mget_hf_file_metadata\u001b[0;34m(url, token, proxies, timeout, library_name, library_version, user_agent, headers)\u001b[0m\n\u001b[1;32m   1665\u001b[0m \u001b[38;5;66;03m# Retrieve metadata\u001b[39;00m\n\u001b[0;32m-> 1666\u001b[0m r \u001b[38;5;241m=\u001b[39m _request_wrapper(\n\u001b[1;32m   1667\u001b[0m     method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHEAD\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1668\u001b[0m     url\u001b[38;5;241m=\u001b[39murl,\n\u001b[1;32m   1669\u001b[0m     headers\u001b[38;5;241m=\u001b[39mheaders,\n\u001b[1;32m   1670\u001b[0m     allow_redirects\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m   1671\u001b[0m     follow_relative_redirects\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m   1672\u001b[0m     proxies\u001b[38;5;241m=\u001b[39mproxies,\n\u001b[1;32m   1673\u001b[0m     timeout\u001b[38;5;241m=\u001b[39mtimeout,\n\u001b[1;32m   1674\u001b[0m )\n\u001b[1;32m   1675\u001b[0m hf_raise_for_status(r)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/huggingface_hub/file_download.py:364\u001b[0m, in \u001b[0;36m_request_wrapper\u001b[0;34m(method, url, follow_relative_redirects, **params)\u001b[0m\n\u001b[1;32m    363\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m follow_relative_redirects:\n\u001b[0;32m--> 364\u001b[0m     response \u001b[38;5;241m=\u001b[39m _request_wrapper(\n\u001b[1;32m    365\u001b[0m         method\u001b[38;5;241m=\u001b[39mmethod,\n\u001b[1;32m    366\u001b[0m         url\u001b[38;5;241m=\u001b[39murl,\n\u001b[1;32m    367\u001b[0m         follow_relative_redirects\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    368\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams,\n\u001b[1;32m    369\u001b[0m     )\n\u001b[1;32m    371\u001b[0m     \u001b[38;5;66;03m# If redirection, we redirect only relative paths.\u001b[39;00m\n\u001b[1;32m    372\u001b[0m     \u001b[38;5;66;03m# This is useful in case of a renamed repository.\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/huggingface_hub/file_download.py:388\u001b[0m, in \u001b[0;36m_request_wrapper\u001b[0;34m(method, url, follow_relative_redirects, **params)\u001b[0m\n\u001b[1;32m    387\u001b[0m response \u001b[38;5;241m=\u001b[39m get_session()\u001b[38;5;241m.\u001b[39mrequest(method\u001b[38;5;241m=\u001b[39mmethod, url\u001b[38;5;241m=\u001b[39murl, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams)\n\u001b[0;32m--> 388\u001b[0m hf_raise_for_status(response)\n\u001b[1;32m    389\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/huggingface_hub/utils/_http.py:454\u001b[0m, in \u001b[0;36mhf_raise_for_status\u001b[0;34m(response, endpoint_name)\u001b[0m\n\u001b[1;32m    446\u001b[0m     message \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    447\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse\u001b[38;5;241m.\u001b[39mstatus_code\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m Client Error.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    448\u001b[0m         \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    452\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m make sure you are authenticated.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    453\u001b[0m     )\n\u001b[0;32m--> 454\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m _format(RepositoryNotFoundError, message, response) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m    456\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m response\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m400\u001b[39m:\n",
      "\u001b[0;31mRepositoryNotFoundError\u001b[0m: 401 Client Error. (Request ID: Root=1-6714246e-33c72aac4912074a77e010f5;fe53025f-24dc-42bd-b0e0-ffeb58db44ba)\n\nRepository Not Found for url: https://huggingface.co/all-MiniLM-L6-v2/resolve/main/tokenizer_config.json.\nPlease make sure you specified the correct `repo_id` and `repo_type`.\nIf you are trying to access a private or gated repo, make sure you are authenticated.\nInvalid username or password.",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[86], line 9\u001b[0m\n\u001b[1;32m      6\u001b[0m medical_history \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mstr\u001b[39m(report) \u001b[38;5;28;01mfor\u001b[39;00m report \u001b[38;5;129;01min\u001b[39;00m medical_history \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(report, \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m pd\u001b[38;5;241m.\u001b[39mnotna(report)]\n\u001b[1;32m      8\u001b[0m batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m32\u001b[39m  \u001b[38;5;66;03m# Set your desired batch size\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m AutoTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mall-MiniLM-L6-v2\u001b[39m\u001b[38;5;124m'\u001b[39m)  \u001b[38;5;66;03m# Ensure you have your tokenizer defined\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# Load the data into a DataLoader\u001b[39;00m\n\u001b[1;32m     12\u001b[0m medical_reports_dataloader \u001b[38;5;241m=\u001b[39m load_preprocessed_texts(medical_history, tokenizer, batch_size)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/transformers/models/auto/tokenization_auto.py:844\u001b[0m, in \u001b[0;36mAutoTokenizer.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m    841\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tokenizer_class\u001b[38;5;241m.\u001b[39mfrom_pretrained(pretrained_model_name_or_path, \u001b[38;5;241m*\u001b[39minputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    843\u001b[0m \u001b[38;5;66;03m# Next, let's try to use the tokenizer_config file to get the tokenizer class.\u001b[39;00m\n\u001b[0;32m--> 844\u001b[0m tokenizer_config \u001b[38;5;241m=\u001b[39m get_tokenizer_config(pretrained_model_name_or_path, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    845\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_commit_hash\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m tokenizer_config:\n\u001b[1;32m    846\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_commit_hash\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m tokenizer_config[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_commit_hash\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/transformers/models/auto/tokenization_auto.py:676\u001b[0m, in \u001b[0;36mget_tokenizer_config\u001b[0;34m(pretrained_model_name_or_path, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, **kwargs)\u001b[0m\n\u001b[1;32m    673\u001b[0m     token \u001b[38;5;241m=\u001b[39m use_auth_token\n\u001b[1;32m    675\u001b[0m commit_hash \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_commit_hash\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m--> 676\u001b[0m resolved_config_file \u001b[38;5;241m=\u001b[39m cached_file(\n\u001b[1;32m    677\u001b[0m     pretrained_model_name_or_path,\n\u001b[1;32m    678\u001b[0m     TOKENIZER_CONFIG_FILE,\n\u001b[1;32m    679\u001b[0m     cache_dir\u001b[38;5;241m=\u001b[39mcache_dir,\n\u001b[1;32m    680\u001b[0m     force_download\u001b[38;5;241m=\u001b[39mforce_download,\n\u001b[1;32m    681\u001b[0m     resume_download\u001b[38;5;241m=\u001b[39mresume_download,\n\u001b[1;32m    682\u001b[0m     proxies\u001b[38;5;241m=\u001b[39mproxies,\n\u001b[1;32m    683\u001b[0m     token\u001b[38;5;241m=\u001b[39mtoken,\n\u001b[1;32m    684\u001b[0m     revision\u001b[38;5;241m=\u001b[39mrevision,\n\u001b[1;32m    685\u001b[0m     local_files_only\u001b[38;5;241m=\u001b[39mlocal_files_only,\n\u001b[1;32m    686\u001b[0m     subfolder\u001b[38;5;241m=\u001b[39msubfolder,\n\u001b[1;32m    687\u001b[0m     _raise_exceptions_for_gated_repo\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    688\u001b[0m     _raise_exceptions_for_missing_entries\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    689\u001b[0m     _raise_exceptions_for_connection_errors\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    690\u001b[0m     _commit_hash\u001b[38;5;241m=\u001b[39mcommit_hash,\n\u001b[1;32m    691\u001b[0m )\n\u001b[1;32m    692\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m resolved_config_file \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    693\u001b[0m     logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCould not locate the tokenizer configuration file, will try to use the model config instead.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/transformers/utils/hub.py:426\u001b[0m, in \u001b[0;36mcached_file\u001b[0;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[1;32m    421\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m(\n\u001b[1;32m    422\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou are trying to access a gated repo.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mMake sure to have access to it at \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    423\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://huggingface.co/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath_or_repo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mstr\u001b[39m(e)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    424\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m    425\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m RepositoryNotFoundError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m--> 426\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m(\n\u001b[1;32m    427\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath_or_repo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m is not a local folder and is not a valid model identifier \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    428\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlisted on \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://huggingface.co/models\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mIf this is a private repository, make sure to pass a token \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    429\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhaving permission to this repo either by logging in with `huggingface-cli login` or by passing \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    430\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`token=<your_token>`\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    431\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m    432\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m RevisionNotFoundError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    433\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m(\n\u001b[1;32m    434\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrevision\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m is not a valid git identifier (branch name, tag name or commit id) that exists \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    435\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfor this model name. Check the model page at \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    436\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://huggingface.co/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath_or_repo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m for available revisions.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    437\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n",
      "\u001b[0;31mOSError\u001b[0m: all-MiniLM-L6-v2 is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'\nIf this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`"
     ]
    }
   ],
   "source": [
    "'''Text Encoder - Medical History encoding using sentence encoder'''\n",
    "\n",
    "# reading and preprocessing medical history reports\n",
    "iu_xray_reports_preprocessed_df_path = os.path.join(output_directory, 'iu_xray_reports_preprocessed_df.csv')\n",
    "medical_history = pd.read_csv(iu_xray_reports_preprocessed_df_path)[\"findings\"].dropna().tolist()\n",
    "medical_history = [str(report) for report in medical_history if isinstance(report, str) or pd.notna(report)]\n",
    "\n",
    "batch_size = 32  # Set your desired batch size\n",
    "tokenizer = AutoTokenizer.from_pretrained('all-MiniLM-L6-v2')  # Ensure you have your tokenizer defined\n",
    "\n",
    "# Load the data into a DataLoader\n",
    "medical_reports_dataloader = load_preprocessed_texts(medical_history, tokenizer, batch_size)\n",
    "\n",
    "# Step 3: Print the shapes of the batches\n",
    "for batch in medical_reports_dataloader:\n",
    "    # Printing shapes of input tensors\n",
    "    print({key: tensor.shape for key, tensor in batch.items()})\n",
    "    break  # Remove this break to see all batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No invalid entries found.\n",
      "\n",
      "Error in embedding: Batch must be of type str or List[str]\n",
      "\n",
      "Already at ./datasets/iu_xray/output/historical_embeddings_b.pt\n",
      "\n",
      "Historical Embeddings Shape: torch.Size([0, 768])\n"
     ]
    }
   ],
   "source": [
    "'''Text Encoder -  Sentence-Bert Encoder using Medical History'''\n",
    "\n",
    "# reading and preprocessing medical history reports\n",
    "iu_xray_reports_preprocessed_df_path = os.path.join(output_directory, 'iu_xray_reports_preprocessed_df.csv')\n",
    "medical_history = pd.read_csv(iu_xray_reports_preprocessed_df_path)[\"findings\"].dropna().tolist()\n",
    "medical_history = [str(report) for report in medical_history if isinstance(report, str) or pd.notna(report)]\n",
    "\n",
    "invalid_entries = [report for report in medical_history if not isinstance(report, (str, list))]\n",
    "\n",
    "# Print if there are any invalid entries\n",
    "if invalid_entries:\n",
    "    print(f\"Found {len(invalid_entries)} invalid entries: {invalid_entries}\")\n",
    "else:\n",
    "    print(\"No invalid entries found.\")\n",
    "\n",
    "# encoding medical history using Sentence-BERT\n",
    "bert_model = BertModel.from_pretrained('bert-base-uncased').to(device)\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "historical_dataloader_b = load_preprocessed_texts(medical_history, tokenizer)\n",
    "historical_embeddings_b = embed_text(historical_dataloader, bert_model)\n",
    "\n",
    "# # Check for invalid entries after batching\n",
    "# invalid_entries_after_batches = [\n",
    "#     report for batch in historical_dataloader_b for report in batch.values() if not isinstance(report, (str, list))\n",
    "# ]\n",
    "\n",
    "# for batch in historical_dataloader:\n",
    "#     print(type(batch))  # Check the type\n",
    "#     print(batch)  # Print the batch content\n",
    "#     break\n",
    "\n",
    "\n",
    "\n",
    "# saving embeddings\n",
    "historical_pt_b = os.path.join(output_directory, 'historical_embeddings_b.pt')\n",
    "if not os.path.exists(historical_pt_b):\n",
    "    torch.save(historical_embeddings_b.cpu(), historical_pt_b)\n",
    "    print(f\"Historical embeddings saved to {historical_pt_b}\")\n",
    "else : print(f\"Already at {historical_pt_b}\")\n",
    "\n",
    "\n",
    "# displaying shape of embeddings\n",
    "print(f\"Historical Embeddings Shape: {historical_embeddings_b.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-19T19:11:16.520892Z",
     "iopub.status.busy": "2024-10-19T19:11:16.520038Z",
     "iopub.status.idle": "2024-10-19T19:11:16.658663Z",
     "shell.execute_reply": "2024-10-19T19:11:16.657206Z",
     "shell.execute_reply.started": "2024-10-19T19:11:16.520840Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in embedding: SentenceTransformer.forward() missing 1 required positional argument: 'input'\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (0x768 and 384x0)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 12\u001b[0m\n\u001b[1;32m      9\u001b[0m historical_embeddings_normalized \u001b[38;5;241m=\u001b[39m historical_embeddings \u001b[38;5;241m/\u001b[39m historical_embeddings\u001b[38;5;241m.\u001b[39mnorm(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, keepdim\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     10\u001b[0m current_embeddings_normalized \u001b[38;5;241m=\u001b[39m current_embeddings \u001b[38;5;241m/\u001b[39m current_embeddings\u001b[38;5;241m.\u001b[39mnorm(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, keepdim\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m---> 12\u001b[0m similarity_matrix \u001b[38;5;241m=\u001b[39m \u001b[43mcompute_cosine_similarity\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdictionary_embeddings\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhistorical_embeddings\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSimilarity Matrix Shape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msimilarity_matrix\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# finding top-k relevant entries\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[14], line 40\u001b[0m, in \u001b[0;36mcompute_cosine_similarity\u001b[0;34m(embeddings1, embeddings2)\u001b[0m\n\u001b[1;32m     38\u001b[0m embeddings1 \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mnormalize(embeddings1, p\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     39\u001b[0m embeddings2 \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mnormalize(embeddings2, p\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m---> 40\u001b[0m cosine \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmm\u001b[49m\u001b[43m(\u001b[49m\u001b[43membeddings1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membeddings2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mt\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m cosine\n",
      "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (0x768 and 384x0)"
     ]
    }
   ],
   "source": [
    "'''Text Encoder - Finding Reports Similar to Current Report'''\n",
    "\n",
    "# current report embedding\n",
    "current_report = 'Heart size pulmonary vascularity appear within normal limits mild tortuosity descending thoracic aorta lungs free focal airspace disease pleural effusion pneumothorax seen discrete nodules adenopathy noted degenerative changes present spine'\n",
    "current_embeddings = embed_text(current_report, sentence_model)\n",
    "\n",
    "\n",
    "# computing cosine similarity\n",
    "historical_embeddings_normalized = historical_embeddings / historical_embeddings.norm(dim=1, keepdim=True)\n",
    "current_embeddings_normalized = current_embeddings / current_embeddings.norm(dim=1, keepdim=True)\n",
    "\n",
    "similarity_matrix = compute_cosine_similarity(dictionary_embeddings, historical_embeddings)\n",
    "print(f\"Similarity Matrix Shape: {similarity_matrix.shape}\")\n",
    "\n",
    "\n",
    "# finding top-k relevant entries\n",
    "k = 5\n",
    "top_k_indices = similarity_matrix.topk(k=k, dim=1).indices\n",
    "\n",
    "\n",
    "# preparing relevant entries based on indices\n",
    "relevant_entries = []\n",
    "for row in top_k_indices:\n",
    "    relevant_entries.append(medical_history[row.item()])\n",
    "\n",
    "\n",
    "# printing relevant entries for each report\n",
    "print(f\"Relevant Entries for the Current Report: {relevant_entries}\")\n",
    "\n",
    "\n",
    "# update the historical embeddingx\n",
    "historical_embeddings.append(current_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.2381143867969513\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import pickle\n",
    "\n",
    "# Check if GPU is available and set the device accordingly\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Function to convert findings to list of lists\n",
    "def get_findings_list(df):\n",
    "    findings_list = df['findings'].fillna(\"\").tolist()  # Replace NaN with empty string\n",
    "    return [[finding] for finding in findings_list]  # Convert each finding into a list of strings\n",
    "\n",
    "# Dataset class to load the findings from the reports\n",
    "class FindingsDataset(Dataset):\n",
    "    def __init__(self, findings_list):\n",
    "        self.findings_list = findings_list\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.findings_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.findings_list[idx]\n",
    "\n",
    "# Define the Sentence Encoder (trainable)\n",
    "class SentenceEncoder(nn.Module):\n",
    "    def __init__(self, hidden_size=768):\n",
    "        super(SentenceEncoder, self).__init__()\n",
    "        self.encoder = nn.Linear(hidden_size, hidden_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.encoder(x)\n",
    "\n",
    "# Cosine similarity loss based on the image equation\n",
    "def cosine_similarity_loss(H, H_b):\n",
    "    cos_sim_H = F.cosine_similarity(H, H.unsqueeze(1))\n",
    "    cos_sim_H_b = F.cosine_similarity(H_b, H_b.unsqueeze(1))\n",
    "    loss = torch.mean((cos_sim_H_b - cos_sim_H) ** 2)\n",
    "    return loss\n",
    "\n",
    "# Load pre-trained BERT model and tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "bert_model = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Move the model to the appropriate device\n",
    "bert_model.to(device)\n",
    "\n",
    "# Load your dataframe\n",
    "# iu_xray_reports_preprocessed_df_path = '/kaggle/input/preprocessed-text/iu_xray_reports_df_preprocessed.csv'\n",
    "iu_xray_reports_preprocessed_df_path = os.path.join(output_directory, 'iu_xray_reports_preprocessed_df.csv')\n",
    "medical_history = pd.read_csv(iu_xray_reports_preprocessed_df_path)\n",
    "findings_list = get_findings_list(medical_history)\n",
    "\n",
    "# Custom collate function to tokenize the findings in batches\n",
    "def collate_fn(batch):\n",
    "    batch = [item[0] for item in batch]  # Flatten the batch list\n",
    "    return tokenizer(batch, padding=True, truncation=True, return_tensors=\"pt\").to(device)  # Move to GPU\n",
    "\n",
    "# Create dataset and dataloader\n",
    "findings_dataset = FindingsDataset(findings_list)\n",
    "dataloader = DataLoader(findings_dataset, batch_size=16, collate_fn=collate_fn)\n",
    "\n",
    "# Initialize Sentence Encoder and move to device\n",
    "sentence_encoder = SentenceEncoder().to(device)\n",
    "\n",
    "# Historical Knowledge Storage\n",
    "#historical_knowledge_file = 'historical_knowledge.pkl'  # File to save the historical encodings\n",
    "historical_knowledge = []  # List to store historical encodings\n",
    "\n",
    "historical_knowledge_file = os.path.join(output_directory, 'historical_knowledge.pkl')\n",
    "# if not os.path.exists(historical_pt_b):\n",
    "#     torch.save(historical_embeddings_b.cpu(), historical_pt_b)\n",
    "#     print(f\"Historical embeddings saved to {historical_pt_b}\")\n",
    "\n",
    "# Load historical knowledge if it exists\n",
    "if os.path.exists(historical_knowledge_file):\n",
    "    with open(historical_knowledge_file, 'rb') as f:\n",
    "        historical_knowledge = pickle.load(f)\n",
    "\n",
    "# Optimizer\n",
    "optimizer = torch.optim.Adam(sentence_encoder.parameters(), lr=1e-4)\n",
    "num_epochs = 10  # Set the number of epochs\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    for batch in dataloader:\n",
    "        input_ids = batch['input_ids'].to(device)  # Move input_ids to GPU\n",
    "        attention_mask = batch['attention_mask'].to(device)  # Move attention_mask to GPU\n",
    "\n",
    "        # Get historical encodings from BERT\n",
    "        with torch.no_grad():\n",
    "            bert_output = bert_model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            H_b = bert_output.last_hidden_state.mean(dim=1).to(device)  # Average pooling of BERT embeddings and move to GPU\n",
    "\n",
    "        # Get encodings from the trainable Sentence Encoder\n",
    "        H = sentence_encoder(H_b)  # Feeding the pre-trained embeddings to the trainable encoder\n",
    "\n",
    "        # final_H = H.detach().cpu()\n",
    "        # Store the current encodings in historical knowledge\n",
    "        historical_knowledge.append(H.detach().cpu())  # Detach tensor and move to CPU\n",
    "\n",
    "        # Calculate cosine similarity loss\n",
    "        loss = cosine_similarity_loss(H, H_b)\n",
    "\n",
    "        # Backpropagation and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f\"Epoch {epoch + 1}, Loss: {loss.item()}\")\n",
    "\n",
    "# final_H_file = 'final_H.pkl'\n",
    "# with open(final_H_file, 'wb') as f:\n",
    "#     pickle.dump(final_H.numpy(), f)\n",
    "\n",
    "\n",
    "# Now you can utilize historical_knowledge for further inference or report generation.\n",
    "with open(historical_knowledge_file, 'wb') as f:\n",
    "    # Convert tensors to NumPy arrays before saving\n",
    "    historical_knowledge_np = [h.numpy() for h in historical_knowledge]\n",
    "    pickle.dump(historical_knowledge_np, f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gIYzXGtsMqmQ"
   },
   "source": [
    "### **Multilevel Alignment**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-20T20:15:08.222467Z",
     "iopub.status.busy": "2024-10-20T20:15:08.221577Z",
     "iopub.status.idle": "2024-10-20T20:15:08.239307Z",
     "shell.execute_reply": "2024-10-20T20:15:08.238412Z",
     "shell.execute_reply.started": "2024-10-20T20:15:08.222425Z"
    }
   },
   "outputs": [],
   "source": [
    "'''Multilevel Alignment based on BLIP Architecture'''\n",
    "\n",
    "# function to extract image embeddings of a given file name\n",
    "def extract_image_embeddings(image_name):\n",
    "    image_embeddings = torch.load(output_directory + \"image_embeddings.pt\")\n",
    "    if image_name in image_embeddings:\n",
    "        return image_embeddings[image_name]\n",
    "    else:\n",
    "        return None\n",
    "    \n",
    "\n",
    "# function to extract text embeddings of a given text\n",
    "def extract_historical_text_embeddings(text):\n",
    "    historical_text_embeddings = torch.load(output_directory + \"historical_embeddings.pt\")\n",
    "    if text in historical_text_embeddings:\n",
    "        return historical_text_embeddings[text]\n",
    "    else :\n",
    "        return None\n",
    "\n",
    "def extract_dictionary_text_embeddings(text):\n",
    "    dictionary_text_embeddings = torch.load(output_directory + \"dictionary_embeddings.pt\")\n",
    "    if text in dictionary_text_embeddings:\n",
    "        return dictionary_text_embeddings[text]\n",
    "    else :\n",
    "        return None\n",
    "    \n",
    "\n",
    "# function to find cosine similarity between a text and an image \n",
    "def cosine_similarity(embedding1, embedding2):\n",
    "    embedding1 = np.array(embedding1)\n",
    "    embedding2 = np.array(embedding2)\n",
    "\n",
    "    dot_product = np.dot(embedding1, embedding2)\n",
    "    norm_embedding1 = np.linalg.norm(embedding1)\n",
    "    norm_embedding2 = np.linalg.norm(embedding2)\n",
    "    \n",
    "    if norm_embedding1 == 0 or norm_embedding2 == 0:\n",
    "        return None \n",
    "    \n",
    "    return dot_product / (norm_embedding1 * norm_embedding2)\n",
    "    \n",
    "\n",
    "# function to compute Image-Text-Contrastive Loss\n",
    "def batch_itc_loss(image_embeddings, text_embeddings, temperature=0.1):\n",
    "    image_embeddings = F.normalize(image_embeddings, dim=1)\n",
    "    text_embeddings = F.normalize(text_embeddings, dim=1)\n",
    "\n",
    "    similarity_matrix = torch.matmul(image_embeddings, text_embeddings.T) / temperature\n",
    "\n",
    "    labels = torch.arange(len(image_embeddings), device=image_embeddings.device)\n",
    "\n",
    "    loss = F.cross_entropy(similarity_matrix, labels)\n",
    "    return labels, loss\n",
    "\n",
    "\n",
    "# function to compute Image-Text-Matching loss\n",
    "def batch_itm_loss(image_embeddings, text_embeddings, match_labels):\n",
    "    logits = torch.matmul(image_embeddings, text_embeddings.t())\n",
    "    probabilities = torch.sigmoid(logits) \n",
    "    positive_probs = probabilities[torch.arange(len(match_labels)), match_labels]\n",
    "\n",
    "    loss = -torch.log(positive_probs + 1e-12).mean()\n",
    "    return loss\n",
    "\n",
    "\n",
    "# function to get embeddings()\n",
    "def get_embeddings(file_path):\n",
    "    df = pd.read_csv(file_path)\n",
    "    report_embeddings = []\n",
    "    image_embeddings = []\n",
    "\n",
    "    for _, row in df.iterrows():\n",
    "        report_embedding = extract_historical_text_embeddings(row['findings'])\n",
    "\n",
    "        for i in range(1, 6):\n",
    "            image_col = f'image_{i}'\n",
    "            if image_col in row and pd.notnull(row[image_col]):\n",
    "                image_embedding = extract_image_embeddings(row[image_col])\n",
    "                report_embeddings.append(report_embedding)\n",
    "                image_embeddings.append(image_embedding)\n",
    "\n",
    "    return report_embeddings, image_embeddings\n",
    "\n",
    "\n",
    "# function to do training step\n",
    "def train_step(file, optimizer):\n",
    "    blip_model.train()\n",
    "    text_embeddings, image_embeddings = get_embeddings(file_path)\n",
    "    \n",
    "    match_labels, itc_loss_value = batch_itc_loss(image_embeddings, text_embeddings)\n",
    "    itm_loss_value = batch_itm_loss(image_embeddings, text_embeddings, match_labels)\n",
    "    \n",
    "    total_loss = itc_loss_value + itm_loss_value\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    total_loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    return total_loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-20T20:15:20.977193Z",
     "iopub.status.busy": "2024-10-20T20:15:20.976387Z",
     "iopub.status.idle": "2024-10-20T20:15:21.484897Z",
     "shell.execute_reply": "2024-10-20T20:15:21.483412Z",
     "shell.execute_reply.started": "2024-10-20T20:15:20.977153Z"
    }
   },
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "Salesforce/blip-base is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'\nIf this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/huggingface_hub/utils/_http.py:406\u001b[0m, in \u001b[0;36mhf_raise_for_status\u001b[0;34m(response, endpoint_name)\u001b[0m\n\u001b[1;32m    405\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 406\u001b[0m     \u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    407\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m HTTPError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/requests/models.py:1024\u001b[0m, in \u001b[0;36mResponse.raise_for_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1023\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m http_error_msg:\n\u001b[0;32m-> 1024\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HTTPError(http_error_msg, response\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m)\n",
      "\u001b[0;31mHTTPError\u001b[0m: 401 Client Error: Unauthorized for url: https://huggingface.co/Salesforce/blip-base/resolve/main/preprocessor_config.json",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mRepositoryNotFoundError\u001b[0m                   Traceback (most recent call last)",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/utils/hub.py:403\u001b[0m, in \u001b[0;36mcached_file\u001b[0;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[1;32m    401\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    402\u001b[0m     \u001b[38;5;66;03m# Load from URL or cache if already cached\u001b[39;00m\n\u001b[0;32m--> 403\u001b[0m     resolved_file \u001b[38;5;241m=\u001b[39m \u001b[43mhf_hub_download\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    404\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpath_or_repo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    405\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    406\u001b[0m \u001b[43m        \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    407\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    408\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    409\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    410\u001b[0m \u001b[43m        \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    411\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    412\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    413\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    414\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    415\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    416\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    417\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m GatedRepoError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/huggingface_hub/utils/_deprecation.py:101\u001b[0m, in \u001b[0;36m_deprecate_arguments.<locals>._inner_deprecate_positional_args.<locals>.inner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    100\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(message, \u001b[38;5;167;01mFutureWarning\u001b[39;00m)\n\u001b[0;32m--> 101\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py:114\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[38;5;241m=\u001b[39mfn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, has_token\u001b[38;5;241m=\u001b[39mhas_token, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[0;32m--> 114\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:1232\u001b[0m, in \u001b[0;36mhf_hub_download\u001b[0;34m(repo_id, filename, subfolder, repo_type, revision, library_name, library_version, cache_dir, local_dir, user_agent, force_download, proxies, etag_timeout, token, local_files_only, headers, endpoint, legacy_cache_layout, resume_download, force_filename, local_dir_use_symlinks)\u001b[0m\n\u001b[1;32m   1231\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1232\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_hf_hub_download_to_cache_dir\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1233\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Destination\u001b[39;49;00m\n\u001b[1;32m   1234\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1235\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# File info\u001b[39;49;00m\n\u001b[1;32m   1236\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrepo_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1237\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilename\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1238\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1239\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1240\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# HTTP info\u001b[39;49;00m\n\u001b[1;32m   1241\u001b[0m \u001b[43m        \u001b[49m\u001b[43mendpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mendpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1242\u001b[0m \u001b[43m        \u001b[49m\u001b[43metag_timeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43metag_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1243\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1244\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1245\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1246\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Additional options\u001b[39;49;00m\n\u001b[1;32m   1247\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1248\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1249\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:1339\u001b[0m, in \u001b[0;36m_hf_hub_download_to_cache_dir\u001b[0;34m(cache_dir, repo_id, filename, repo_type, revision, endpoint, etag_timeout, headers, proxies, token, local_files_only, force_download)\u001b[0m\n\u001b[1;32m   1338\u001b[0m     \u001b[38;5;66;03m# Otherwise, raise appropriate error\u001b[39;00m\n\u001b[0;32m-> 1339\u001b[0m     \u001b[43m_raise_on_head_call_error\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhead_call_error\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1341\u001b[0m \u001b[38;5;66;03m# From now on, etag, commit_hash, url and size are not None.\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:1854\u001b[0m, in \u001b[0;36m_raise_on_head_call_error\u001b[0;34m(head_call_error, force_download, local_files_only)\u001b[0m\n\u001b[1;32m   1852\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(head_call_error, RepositoryNotFoundError) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(head_call_error, GatedRepoError):\n\u001b[1;32m   1853\u001b[0m     \u001b[38;5;66;03m# Repo not found or gated => let's raise the actual error\u001b[39;00m\n\u001b[0;32m-> 1854\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m head_call_error\n\u001b[1;32m   1855\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1856\u001b[0m     \u001b[38;5;66;03m# Otherwise: most likely a connection issue or Hub downtime => let's warn the user\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:1746\u001b[0m, in \u001b[0;36m_get_metadata_or_catch_error\u001b[0;34m(repo_id, filename, repo_type, revision, endpoint, proxies, etag_timeout, headers, token, local_files_only, relative_filename, storage_folder)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1746\u001b[0m     metadata \u001b[38;5;241m=\u001b[39m \u001b[43mget_hf_file_metadata\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1747\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43metag_timeout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\n\u001b[1;32m   1748\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m EntryNotFoundError \u001b[38;5;28;01mas\u001b[39;00m http_error:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py:114\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[38;5;241m=\u001b[39mfn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, has_token\u001b[38;5;241m=\u001b[39mhas_token, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[0;32m--> 114\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:1666\u001b[0m, in \u001b[0;36mget_hf_file_metadata\u001b[0;34m(url, token, proxies, timeout, library_name, library_version, user_agent, headers)\u001b[0m\n\u001b[1;32m   1665\u001b[0m \u001b[38;5;66;03m# Retrieve metadata\u001b[39;00m\n\u001b[0;32m-> 1666\u001b[0m r \u001b[38;5;241m=\u001b[39m \u001b[43m_request_wrapper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1667\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mHEAD\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1668\u001b[0m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1669\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1670\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1671\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfollow_relative_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1672\u001b[0m \u001b[43m    \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1673\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1674\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1675\u001b[0m hf_raise_for_status(r)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:364\u001b[0m, in \u001b[0;36m_request_wrapper\u001b[0;34m(method, url, follow_relative_redirects, **params)\u001b[0m\n\u001b[1;32m    363\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m follow_relative_redirects:\n\u001b[0;32m--> 364\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43m_request_wrapper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    365\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    366\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    367\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfollow_relative_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    368\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    369\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    371\u001b[0m     \u001b[38;5;66;03m# If redirection, we redirect only relative paths.\u001b[39;00m\n\u001b[1;32m    372\u001b[0m     \u001b[38;5;66;03m# This is useful in case of a renamed repository.\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:388\u001b[0m, in \u001b[0;36m_request_wrapper\u001b[0;34m(method, url, follow_relative_redirects, **params)\u001b[0m\n\u001b[1;32m    387\u001b[0m response \u001b[38;5;241m=\u001b[39m get_session()\u001b[38;5;241m.\u001b[39mrequest(method\u001b[38;5;241m=\u001b[39mmethod, url\u001b[38;5;241m=\u001b[39murl, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams)\n\u001b[0;32m--> 388\u001b[0m \u001b[43mhf_raise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    389\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/huggingface_hub/utils/_http.py:454\u001b[0m, in \u001b[0;36mhf_raise_for_status\u001b[0;34m(response, endpoint_name)\u001b[0m\n\u001b[1;32m    446\u001b[0m     message \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    447\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse\u001b[38;5;241m.\u001b[39mstatus_code\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m Client Error.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    448\u001b[0m         \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    452\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m make sure you are authenticated.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    453\u001b[0m     )\n\u001b[0;32m--> 454\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m _format(RepositoryNotFoundError, message, response) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m    456\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m response\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m400\u001b[39m:\n",
      "\u001b[0;31mRepositoryNotFoundError\u001b[0m: 401 Client Error. (Request ID: Root=1-671564d9-14412cd30925fffd6db21c63;981de6ca-cf8f-4c2a-b502-d11d6fe81bbe)\n\nRepository Not Found for url: https://huggingface.co/Salesforce/blip-base/resolve/main/preprocessor_config.json.\nPlease make sure you specified the correct `repo_id` and `repo_type`.\nIf you are trying to access a private or gated repo, make sure you are authenticated.\nInvalid username or password.",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;124;03m'''Multilevel Alignment based on BLIP Architecture - Coarse Grained Alignment (Image Text Contrastive and Image Text Matching)'''\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# BLIP architecture\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m processor \u001b[38;5;241m=\u001b[39m \u001b[43mBlipProcessor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mSalesforce/blip-base\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m blip_model \u001b[38;5;241m=\u001b[39m BlipForConditionalGeneration\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSalesforce/blip-base\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      6\u001b[0m device \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/processing_utils.py:915\u001b[0m, in \u001b[0;36mProcessorMixin.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, cache_dir, force_download, local_files_only, token, revision, **kwargs)\u001b[0m\n\u001b[1;32m    912\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m token \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    913\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtoken\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m token\n\u001b[0;32m--> 915\u001b[0m args \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_arguments_from_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    916\u001b[0m processor_dict, kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mget_processor_dict(pretrained_model_name_or_path, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    918\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mfrom_args_and_dict(args, processor_dict, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/processing_utils.py:961\u001b[0m, in \u001b[0;36mProcessorMixin._get_arguments_from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m    958\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    959\u001b[0m         attribute_class \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(transformers_module, class_name)\n\u001b[0;32m--> 961\u001b[0m     args\u001b[38;5;241m.\u001b[39mappend(\u001b[43mattribute_class\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    962\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m args\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/image_processing_base.py:206\u001b[0m, in \u001b[0;36mImageProcessingMixin.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, cache_dir, force_download, local_files_only, token, revision, **kwargs)\u001b[0m\n\u001b[1;32m    203\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m token \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    204\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtoken\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m token\n\u001b[0;32m--> 206\u001b[0m image_processor_dict, kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_image_processor_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    208\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mfrom_dict(image_processor_dict, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/image_processing_base.py:335\u001b[0m, in \u001b[0;36mImageProcessingMixin.get_image_processor_dict\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m    332\u001b[0m image_processor_file \u001b[38;5;241m=\u001b[39m IMAGE_PROCESSOR_NAME\n\u001b[1;32m    333\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    334\u001b[0m     \u001b[38;5;66;03m# Load from local folder or from cache or download from model Hub and cache\u001b[39;00m\n\u001b[0;32m--> 335\u001b[0m     resolved_image_processor_file \u001b[38;5;241m=\u001b[39m \u001b[43mcached_file\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    336\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    337\u001b[0m \u001b[43m        \u001b[49m\u001b[43mimage_processor_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    338\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    339\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    340\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    341\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    342\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    343\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    344\u001b[0m \u001b[43m        \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    345\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    346\u001b[0m \u001b[43m        \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    347\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    348\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m:\n\u001b[1;32m    349\u001b[0m     \u001b[38;5;66;03m# Raise any environment error raise by `cached_file`. It will have a helpful error message adapted to\u001b[39;00m\n\u001b[1;32m    350\u001b[0m     \u001b[38;5;66;03m# the original exception.\u001b[39;00m\n\u001b[1;32m    351\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/utils/hub.py:426\u001b[0m, in \u001b[0;36mcached_file\u001b[0;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[1;32m    421\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m(\n\u001b[1;32m    422\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou are trying to access a gated repo.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mMake sure to have access to it at \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    423\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://huggingface.co/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath_or_repo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mstr\u001b[39m(e)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    424\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m    425\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m RepositoryNotFoundError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m--> 426\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m(\n\u001b[1;32m    427\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath_or_repo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m is not a local folder and is not a valid model identifier \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    428\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlisted on \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://huggingface.co/models\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mIf this is a private repository, make sure to pass a token \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    429\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhaving permission to this repo either by logging in with `huggingface-cli login` or by passing \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    430\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`token=<your_token>`\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    431\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m    432\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m RevisionNotFoundError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    433\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m(\n\u001b[1;32m    434\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrevision\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m is not a valid git identifier (branch name, tag name or commit id) that exists \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    435\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfor this model name. Check the model page at \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    436\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://huggingface.co/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath_or_repo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m for available revisions.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    437\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n",
      "\u001b[0;31mOSError\u001b[0m: Salesforce/blip-base is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'\nIf this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`"
     ]
    }
   ],
   "source": [
    "'''Multilevel Alignment based on BLIP Architecture - Coarse Grained Alignment (Image Text Contrastive and Image Text Matching)'''\n",
    "\n",
    "# BLIP architecture\n",
    "processor = BlipProcessor.from_pretrained(\"Salesforce/blip-base\")\n",
    "blip_model = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-base\")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "blip_model.to(device)\n",
    "blip_model.eval()\n",
    "\n",
    "\n",
    "# training loop\n",
    "num_epochs = 10  \n",
    "optimizer = torch.optim.Adam(blip_model.parameters(), lr=1e-5)\n",
    "\n",
    "\n",
    "# using a data loader defined that provides (image, text) pairs\n",
    "for epoch in range(num_epochs):\n",
    "    loss = train_step(file_path, optimizer)\n",
    "    print(f\"Epoch {epoch}, Loss: {loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Multilevel Alignment based on BLIP Architecture - Fine Grained Alignment'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KL Loss for batch 0: 2.50960111618042\n",
      "KL Loss for batch 1: 2.5075905323028564\n",
      "KL Loss for batch 2: 2.5028076171875\n",
      "KL Loss for batch 3: 2.509035348892212\n",
      "KL Loss for batch 4: 2.510709762573242\n",
      "KL Loss for batch 5: 2.5108795166015625\n",
      "KL Loss for batch 6: 2.510974645614624\n",
      "KL Loss for batch 7: 2.5042688846588135\n",
      "KL Loss for batch 8: 2.5072593688964844\n",
      "KL Loss for batch 9: 2.500169277191162\n",
      "KL Loss for batch 10: 2.5004351139068604\n",
      "KL Loss for batch 11: 2.4989662170410156\n",
      "KL Loss for batch 12: 2.512904167175293\n",
      "KL Loss for batch 13: 2.506892442703247\n",
      "KL Loss for batch 14: 2.5084269046783447\n",
      "KL Loss for batch 15: 2.507992744445801\n",
      "KL Loss for batch 16: 2.505155086517334\n",
      "KL Loss for batch 17: 2.511310577392578\n",
      "KL Loss for batch 18: 2.5095150470733643\n",
      "KL Loss for batch 19: 2.513171434402466\n",
      "KL Loss for batch 20: 2.5062851905822754\n",
      "KL Loss for batch 21: 2.5028235912323\n",
      "KL Loss for batch 22: 2.510256767272949\n",
      "KL Loss for batch 23: 2.5104334354400635\n",
      "KL Loss for batch 24: 2.507218599319458\n",
      "KL Loss for batch 25: 2.501281499862671\n",
      "KL Loss for batch 26: 2.503661632537842\n",
      "KL Loss for batch 27: 2.5047688484191895\n",
      "KL Loss for batch 28: 2.506333112716675\n",
      "KL Loss for batch 29: 2.508028030395508\n",
      "KL Loss for batch 30: 2.51633620262146\n",
      "KL Loss for batch 31: 2.5105669498443604\n",
      "KL Loss for batch 32: 2.513197183609009\n",
      "KL Loss for batch 33: 2.5067684650421143\n",
      "KL Loss for batch 34: 2.511516571044922\n",
      "KL Loss for batch 35: 2.506542205810547\n",
      "KL Loss for batch 36: 2.5034873485565186\n",
      "KL Loss for batch 37: 2.5068066120147705\n",
      "KL Loss for batch 38: 2.5047945976257324\n",
      "KL Loss for batch 39: 2.513479709625244\n",
      "KL Loss for batch 40: 2.5084846019744873\n",
      "KL Loss for batch 41: 2.501063823699951\n",
      "KL Loss for batch 42: 2.506246328353882\n",
      "KL Loss for batch 43: 2.507786512374878\n",
      "KL Loss for batch 44: 2.501591444015503\n",
      "KL Loss for batch 45: 2.510796070098877\n",
      "KL Loss for batch 46: 2.511389970779419\n",
      "KL Loss for batch 47: 2.5080790519714355\n",
      "KL Loss for batch 48: 2.5058295726776123\n",
      "KL Loss for batch 49: 2.511479139328003\n",
      "KL Loss for batch 50: 2.5079152584075928\n",
      "KL Loss for batch 51: 2.499704360961914\n",
      "KL Loss for batch 52: 2.50091552734375\n",
      "KL Loss for batch 53: 2.510094165802002\n",
      "KL Loss for batch 54: 2.5117909908294678\n",
      "KL Loss for batch 55: 2.508760929107666\n",
      "KL Loss for batch 56: 2.5107779502868652\n",
      "KL Loss for batch 57: 2.5105721950531006\n",
      "KL Loss for batch 58: 2.503929615020752\n",
      "KL Loss for batch 59: 2.5082528591156006\n",
      "KL Loss for batch 60: 2.5114340782165527\n",
      "KL Loss for batch 61: 2.5063869953155518\n",
      "KL Loss for batch 62: 2.5096898078918457\n",
      "KL Loss for batch 63: 2.5104615688323975\n",
      "KL Loss for batch 64: 2.5056991577148438\n",
      "KL Loss for batch 65: 2.5050108432769775\n",
      "KL Loss for batch 66: 2.5085630416870117\n",
      "KL Loss for batch 67: 2.50586199760437\n",
      "KL Loss for batch 68: 2.50588059425354\n",
      "KL Loss for batch 69: 2.5022077560424805\n",
      "KL Loss for batch 70: 2.5127222537994385\n",
      "KL Loss for batch 71: 2.507448673248291\n",
      "KL Loss for batch 72: 2.5087318420410156\n",
      "KL Loss for batch 73: 2.5097687244415283\n",
      "KL Loss for batch 74: 2.5120129585266113\n",
      "KL Loss for batch 75: 2.517369031906128\n",
      "KL Loss for batch 76: 2.511927604675293\n",
      "KL Loss for batch 77: 2.5044169425964355\n",
      "KL Loss for batch 78: 2.5116493701934814\n",
      "KL Loss for batch 79: 2.5117175579071045\n",
      "KL Loss for batch 80: 2.507162570953369\n",
      "KL Loss for batch 81: 2.5023293495178223\n",
      "KL Loss for batch 82: 2.510446548461914\n",
      "KL Loss for batch 83: 2.508938789367676\n",
      "KL Loss for batch 84: 2.511141777038574\n",
      "KL Loss for batch 85: 2.508793354034424\n",
      "KL Loss for batch 86: 2.5095067024230957\n",
      "KL Loss for batch 87: 2.5086934566497803\n",
      "KL Loss for batch 88: 2.5044374465942383\n",
      "KL Loss for batch 89: 2.5041604042053223\n",
      "KL Loss for batch 90: 2.510058879852295\n",
      "KL Loss for batch 91: 2.5066325664520264\n",
      "KL Loss for batch 92: 2.5060036182403564\n",
      "KL Loss for batch 93: 2.509967803955078\n",
      "KL Loss for batch 94: 2.509882926940918\n",
      "KL Loss for batch 95: 2.510976791381836\n",
      "KL Loss for batch 96: 2.512115240097046\n",
      "KL Loss for batch 97: 2.5054404735565186\n",
      "KL Loss for batch 98: 2.511502742767334\n",
      "KL Loss for batch 99: 2.505554437637329\n",
      "KL Loss for batch 100: 2.5085034370422363\n",
      "KL Loss for batch 101: 2.508915424346924\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import os\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.distributions as dist\n",
    "\n",
    "\n",
    "output_aligned_features_dir = output_directory\n",
    "if(os.path.exists(output_aligned_features_dir + \"aligned_outputs.pt\")):\n",
    "    print(f\"Already at {historical_pt_b}\")\n",
    "\n",
    "batch_size = 32 \n",
    "\n",
    "final_embeddings = torch.load(output_directory + \"/final_embeddings.pt\")  # Shape: [7470, 50, 2048]\n",
    "#patch_features = torch.load(output_directory + 'patch_feats.pt')          # Shape: [7470, 49, 2048]\n",
    "dictionary_embeddings = torch.load(output_directory + \"/dictionary_embeddings.pt\")  # Shape: [47, 384]\n",
    "dictionary_embeddings = torch.tensor(dictionary_embeddings)\n",
    "\n",
    "# Project dictionary embeddings (V) to match the dimensionality of the image embeddings (2048)\n",
    "projection_layer = nn.Linear(384, 2048)\n",
    "V_projected = projection_layer(dictionary_embeddings)  # Shape: [47, 2048]\n",
    "# V_projected = F.normalize(V_projected, p=2, dim=1)\n",
    "\n",
    "# Normalize final embeddings (this is already being done)\n",
    "# I_prime = F.normalize(final_embeddings, p=2, dim=1)\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.d_k = d_model // num_heads\n",
    "        \n",
    "        # weight matrices for query, key, value\n",
    "        self.W_q = nn.Linear(d_model, d_model)\n",
    "        self.W_k = nn.Linear(d_model, d_model)\n",
    "        self.W_v = nn.Linear(d_model, d_model)\n",
    "        self.W_o = nn.Linear(d_model, d_model)\n",
    "\n",
    "    def scaled_dot_product_attention(self, Q, K, V):\n",
    "        # Compute QK^T / sqrt(d_k)\n",
    "        d_k = Q.size(-1)\n",
    "        scores = torch.matmul(Q, K.transpose(-2, -1)) / torch.sqrt(torch.tensor(d_k).float())\n",
    "        attn_weights = F.softmax(scores, dim=-1)\n",
    "        output = torch.matmul(attn_weights, V)\n",
    "        return output, attn_weights\n",
    "\n",
    "    def forward(self, V, I_prime):\n",
    "        # V:  projected dictionary embeddings\n",
    "        # I_prime: final_embeddings\n",
    "        \n",
    "        Q = self.W_q(V)  # Dictionary embeddings\n",
    "        K = self.W_k(I_prime)  # Image embeddings \n",
    "        V = self.W_v(I_prime)\n",
    "\n",
    "        Q = Q.view(Q.size(0), Q.size(1), self.num_heads, self.d_k).transpose(1, 2)\n",
    "        K = K.view(K.size(0), K.size(1), self.num_heads, self.d_k).transpose(1, 2)\n",
    "        V = V.view(V.size(0), V.size(1), self.num_heads, self.d_k).transpose(1, 2)\n",
    "       \n",
    "        attn_output, attn_weights = self.scaled_dot_product_attention(Q, K, V) # apply scaled dot-product attention\n",
    "\n",
    "        attn_output = attn_output.transpose(1, 2).contiguous().view(V.size(0), -1, self.d_model) # concatenate heads and apply final linear transformation\n",
    "        output = self.W_o(attn_output)\n",
    "        \n",
    "        return output, attn_weights\n",
    "\n",
    "# Feed-forward network\n",
    "class FeedForwardNetwork(nn.Module):\n",
    "    def __init__(self, d_model, d_ff):\n",
    "        super(FeedForwardNetwork, self).__init__()\n",
    "        self.fc1 = nn.Linear(d_model, d_ff)\n",
    "        self.fc2 = nn.Linear(d_ff, d_model)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc2(self.relu(self.fc1(x)))\n",
    "\n",
    "# KL Divergence Loss (for training)\n",
    "# def kl_divergence(mu1, logvar1, mu2, logvar2):\n",
    "#     kl_loss = 0.5 * torch.sum(logvar2 - logvar1 + (torch.exp(logvar1) + (mu1 - mu2)**2) / torch.exp(logvar2) - 1)\n",
    "#     return kl_loss\n",
    "def kl_divergence(mu1, logvar1, mu2, logvar2):\n",
    "    # kl_loss = 0.5 * torch.sum(\n",
    "    #     logvar2 - logvar1 +\n",
    "    #     (torch.exp(logvar1) + (mu1 - mu2) ** 2) / torch.exp(logvar2) - 1\n",
    "    # )\n",
    "    # def compute_kl_loss(mu1, logvar1, mu2, logvar2):\n",
    "    # Create normal distributions from the means (mu) and log-variances (logvar)\n",
    "    normal1 = dist.Normal(mu1, torch.exp(0.5 * logvar1))  # exp(0.5 * logvar) gives standard deviation\n",
    "    normal2 = dist.Normal(mu2, torch.exp(0.5 * logvar2))\n",
    "    \n",
    "    # Compute the KL divergence between the two distributions\n",
    "    kl_loss = dist.kl.kl_divergence(normal1, normal2).mean()  # Mean over the batch\n",
    "    return kl_loss\n",
    "\n",
    "    # return kl_loss\n",
    "    \n",
    "# Model setup\n",
    "d_model = 2048\n",
    "num_heads = 8\n",
    "d_ff = 4096  \n",
    "\n",
    "mha = MultiHeadAttention(d_model, num_heads)\n",
    "ffn = FeedForwardNetwork(d_model, d_ff)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "mha = mha.to(device)\n",
    "ffn = ffn.to(device)\n",
    "V_projected = V_projected.to(device)\n",
    "final_embeddings = final_embeddings.to(device)\n",
    "#patch_features = patch_features.to(device)\n",
    "\n",
    "dataset_size = final_embeddings.size(0)  # 7470\n",
    "aligned_outputs = []\n",
    "\n",
    "params = list(mha.parameters()) + list(ffn.parameters())  \n",
    "optimizer = torch.optim.Adam(params, lr=1e-4)  \n",
    "\n",
    "for i in range(0, dataset_size, batch_size):\n",
    "    \n",
    "    batch_final_embeddings = final_embeddings[i:i+batch_size]  # Shape: [batch_size, 50, 2048]\n",
    "    #batch_patch_features = patch_features[i:i+batch_size]  # Shape: [batch_size, 49, 2048]\n",
    "\n",
    "    #batch_I_prime = torch.cat((batch_final_embeddings, batch_patch_features), dim=1)  # Shape: [batch_size, 99, 2048]\n",
    "    batch_I_prime = batch_final_embeddings\n",
    "    batch_V_repeated = V_projected.unsqueeze(0).repeat(batch_final_embeddings.size(0), 1, 1)  # Shape: [batch_size, 47, 2048]\n",
    "\n",
    "    batch_I_prime = batch_I_prime.to(device)\n",
    "    batch_V_repeated = batch_V_repeated.to(device)\n",
    "\n",
    "    aligned_output, _ = mha(batch_V_repeated, batch_I_prime) #multi-head attention with dictionary embeddings and image embeddings\n",
    "\n",
    "    aligned_output_ffn = ffn(aligned_output) #feed-forward network\n",
    "\n",
    "    mu1, logvar1 = torch.randn_like(aligned_output_ffn,requires_grad=True), torch.randn_like(aligned_output_ffn,requires_grad=True)  # Priors for V'\n",
    "    mu2, logvar2 = torch.randn_like(batch_V_repeated,requires_grad=True), torch.randn_like(batch_V_repeated,requires_grad=True)  # Priors for V_label\n",
    "    kl_loss = kl_divergence(mu1, logvar1, mu2, logvar2) # KL divergence\n",
    "\n",
    "    print(f\"KL Loss for batch {i // batch_size}: {kl_loss.item()}\")\n",
    "\n",
    "    # If training, accumulate gradients and perform optimization steps\n",
    "    optimizer.zero_grad()\n",
    "    kl_loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    aligned_outputs.append(aligned_output_ffn.cpu()) \n",
    "\n",
    "#single tensor\n",
    "aligned_outputs_tensor = torch.cat(aligned_outputs, dim=0)  # Shape: [7470, 50, 2048]\n",
    "\n",
    "torch.save(aligned_outputs_tensor, os.path.join(output_aligned_features_dir, \"aligned_outputs.pt\"))\n",
    "\n",
    "print(\"Aligned features saved successfully.\")\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of I_prime: torch.Size([32, 6144])\n",
      "KL Loss for batch 0: 0.008643136359751225\n",
      "Shape of I_prime: torch.Size([32, 6144])\n",
      "KL Loss for batch 1: 0.004221990238875151\n",
      "Shape of I_prime: torch.Size([32, 6144])\n",
      "KL Loss for batch 2: 0.004354505799710751\n",
      "Shape of I_prime: torch.Size([32, 6144])\n",
      "KL Loss for batch 3: 0.0030751104932278395\n",
      "Shape of I_prime: torch.Size([32, 6144])\n",
      "KL Loss for batch 4: 0.002518102992326021\n",
      "Shape of I_prime: torch.Size([32, 6144])\n",
      "KL Loss for batch 5: 0.002542373025789857\n",
      "Shape of I_prime: torch.Size([32, 6144])\n",
      "KL Loss for batch 6: 0.0030904796440154314\n",
      "Shape of I_prime: torch.Size([32, 6144])\n",
      "KL Loss for batch 7: 0.002677188953384757\n",
      "Shape of I_prime: torch.Size([32, 6144])\n",
      "KL Loss for batch 8: 0.002040667226538062\n",
      "Shape of I_prime: torch.Size([32, 6144])\n",
      "KL Loss for batch 9: 0.002298082923516631\n",
      "Shape of I_prime: torch.Size([32, 6144])\n",
      "KL Loss for batch 10: 0.0020234431140124798\n",
      "Shape of I_prime: torch.Size([32, 6144])\n",
      "KL Loss for batch 11: 0.0021124430932104588\n",
      "Shape of I_prime: torch.Size([32, 6144])\n",
      "KL Loss for batch 12: 0.0019382386235520244\n",
      "Shape of I_prime: torch.Size([32, 6144])\n",
      "KL Loss for batch 13: 0.001991622382774949\n",
      "Shape of I_prime: torch.Size([32, 6144])\n",
      "KL Loss for batch 14: 0.0019244671566411853\n",
      "Shape of I_prime: torch.Size([32, 6144])\n",
      "KL Loss for batch 15: 0.0019134781323373318\n",
      "Shape of I_prime: torch.Size([32, 6144])\n",
      "KL Loss for batch 16: 0.0018878872506320477\n",
      "Shape of I_prime: torch.Size([32, 6144])\n",
      "KL Loss for batch 17: 0.001879701390862465\n",
      "Shape of I_prime: torch.Size([32, 6144])\n",
      "KL Loss for batch 18: 0.0018624196527525783\n",
      "Shape of I_prime: torch.Size([32, 6144])\n",
      "KL Loss for batch 19: 0.001856289803981781\n",
      "Shape of I_prime: torch.Size([32, 6144])\n",
      "KL Loss for batch 20: 0.001849841559305787\n",
      "Shape of I_prime: torch.Size([32, 6144])\n",
      "KL Loss for batch 21: 0.0018406902672722936\n",
      "Shape of I_prime: torch.Size([32, 6144])\n",
      "KL Loss for batch 22: 0.0018360527465119958\n",
      "Shape of I_prime: torch.Size([32, 6144])\n",
      "KL Loss for batch 23: 0.001826093764975667\n",
      "Shape of I_prime: torch.Size([32, 6144])\n",
      "KL Loss for batch 24: 0.0018265453400090337\n",
      "Shape of I_prime: torch.Size([32, 6144])\n",
      "KL Loss for batch 25: 0.001819682540372014\n",
      "Shape of I_prime: torch.Size([32, 6144])\n",
      "KL Loss for batch 26: 0.00182141677942127\n",
      "Shape of I_prime: torch.Size([32, 6144])\n",
      "KL Loss for batch 27: 0.0018131847027689219\n",
      "Shape of I_prime: torch.Size([32, 6144])\n",
      "KL Loss for batch 28: 0.0018168463138863444\n",
      "Shape of I_prime: torch.Size([32, 6144])\n",
      "KL Loss for batch 29: 0.0018092667451128364\n",
      "Shape of I_prime: torch.Size([32, 6144])\n",
      "KL Loss for batch 30: 0.0018137105507776141\n",
      "Shape of I_prime: torch.Size([32, 6144])\n",
      "KL Loss for batch 31: 0.0018055523978546262\n",
      "Shape of I_prime: torch.Size([32, 6144])\n",
      "KL Loss for batch 32: 0.0018105001654475927\n",
      "Shape of I_prime: torch.Size([32, 6144])\n",
      "KL Loss for batch 33: 0.001805604319088161\n",
      "Shape of I_prime: torch.Size([32, 6144])\n",
      "KL Loss for batch 34: 0.00180596683640033\n",
      "Shape of I_prime: torch.Size([32, 6144])\n",
      "KL Loss for batch 35: 0.001805596286430955\n",
      "Shape of I_prime: torch.Size([32, 6144])\n",
      "KL Loss for batch 36: 0.0018018913688138127\n",
      "Shape of I_prime: torch.Size([32, 6144])\n",
      "KL Loss for batch 37: 0.0018039536662399769\n",
      "Shape of I_prime: torch.Size([32, 6144])\n",
      "KL Loss for batch 38: 0.00180051161441952\n",
      "Shape of I_prime: torch.Size([32, 6144])\n",
      "KL Loss for batch 39: 0.0018020375864580274\n",
      "Shape of I_prime: torch.Size([32, 6144])\n",
      "KL Loss for batch 40: 0.0017999301198869944\n",
      "Shape of I_prime: torch.Size([32, 6144])\n",
      "KL Loss for batch 41: 0.0017998048570007086\n",
      "Shape of I_prime: torch.Size([32, 6144])\n",
      "KL Loss for batch 42: 0.0017994858790189028\n",
      "Shape of I_prime: torch.Size([32, 6144])\n",
      "KL Loss for batch 43: 0.0017987072933465242\n",
      "Shape of I_prime: torch.Size([32, 6144])\n",
      "KL Loss for batch 44: 0.0017994288355112076\n",
      "Shape of I_prime: torch.Size([32, 6144])\n",
      "KL Loss for batch 45: 0.0017978319665417075\n",
      "Shape of I_prime: torch.Size([32, 6144])\n",
      "KL Loss for batch 46: 0.0017979841213673353\n",
      "Shape of I_prime: torch.Size([32, 6144])\n",
      "KL Loss for batch 47: 0.0017976172966882586\n",
      "Shape of I_prime: torch.Size([32, 6144])\n",
      "KL Loss for batch 48: 0.001797271310351789\n",
      "Shape of I_prime: torch.Size([32, 6144])\n",
      "KL Loss for batch 49: 0.0017972190398722887\n",
      "Shape of I_prime: torch.Size([32, 6144])\n",
      "KL Loss for batch 50: 0.0017969144973903894\n",
      "Shape of I_prime: torch.Size([32, 6144])\n",
      "KL Loss for batch 51: 0.0017967361491173506\n",
      "Shape of I_prime: torch.Size([32, 6144])\n",
      "KL Loss for batch 52: 0.0017969763139262795\n",
      "Shape of I_prime: torch.Size([32, 6144])\n",
      "KL Loss for batch 53: 0.0017965794540941715\n",
      "Shape of I_prime: torch.Size([32, 6144])\n",
      "KL Loss for batch 54: 0.0017963381251320243\n",
      "Shape of I_prime: torch.Size([32, 6144])\n",
      "KL Loss for batch 55: 0.0017964188009500504\n",
      "Shape of I_prime: torch.Size([32, 6144])\n",
      "KL Loss for batch 56: 0.0017961779376491904\n",
      "Shape of I_prime: torch.Size([32, 6144])\n",
      "KL Loss for batch 57: 0.001796072581782937\n",
      "Shape of I_prime: torch.Size([32, 6144])\n",
      "KL Loss for batch 58: 0.0017960952827706933\n",
      "Shape of I_prime: torch.Size([32, 6144])\n",
      "KL Loss for batch 59: 0.0017963311402127147\n",
      "Shape of I_prime: torch.Size([32, 6144])\n",
      "KL Loss for batch 60: 0.0017959491815418005\n",
      "Shape of I_prime: torch.Size([32, 6144])\n",
      "KL Loss for batch 61: 0.0017963629215955734\n",
      "Shape of I_prime: torch.Size([32, 6144])\n",
      "KL Loss for batch 62: 0.0017968633910641074\n",
      "Shape of I_prime: torch.Size([32, 6144])\n",
      "KL Loss for batch 63: 0.0017958448734134436\n",
      "Shape of I_prime: torch.Size([32, 6144])\n",
      "KL Loss for batch 64: 0.0017961555859073997\n",
      "Shape of I_prime: torch.Size([32, 6144])\n",
      "KL Loss for batch 65: 0.0017966051818802953\n",
      "Shape of I_prime: torch.Size([32, 6144])\n",
      "KL Loss for batch 66: 0.0017958201933652163\n",
      "Shape of I_prime: torch.Size([32, 6144])\n",
      "KL Loss for batch 67: 0.001796320779249072\n",
      "Shape of I_prime: torch.Size([32, 6144])\n",
      "KL Loss for batch 68: 0.0017970730550587177\n",
      "Shape of I_prime: torch.Size([32, 6144])\n",
      "KL Loss for batch 69: 0.001795993302948773\n",
      "Shape of I_prime: torch.Size([32, 6144])\n",
      "KL Loss for batch 70: 0.0017956726951524615\n",
      "Shape of I_prime: torch.Size([32, 6144])\n",
      "KL Loss for batch 71: 0.0017969165928661823\n",
      "Shape of I_prime: torch.Size([32, 6144])\n",
      "KL Loss for batch 72: 0.001796230673789978\n",
      "Shape of I_prime: torch.Size([32, 6144])\n",
      "KL Loss for batch 73: 0.0017956934170797467\n",
      "Shape of I_prime: torch.Size([32, 6144])\n",
      "KL Loss for batch 74: 0.0017977859824895859\n",
      "Shape of I_prime: torch.Size([32, 6144])\n",
      "KL Loss for batch 75: 0.001797578064724803\n",
      "Shape of I_prime: torch.Size([32, 6144])\n",
      "KL Loss for batch 76: 0.0017955618677660823\n",
      "Shape of I_prime: torch.Size([32, 6144])\n",
      "KL Loss for batch 77: 0.0017967615276575089\n",
      "Shape of I_prime: torch.Size([32, 6144])\n",
      "KL Loss for batch 78: 0.0017981736455112696\n",
      "Shape of I_prime: torch.Size([32, 6144])\n",
      "KL Loss for batch 79: 0.0017961220582947135\n",
      "Shape of I_prime: torch.Size([32, 6144])\n",
      "KL Loss for batch 80: 0.0017956445226445794\n",
      "Shape of I_prime: torch.Size([32, 6144])\n",
      "KL Loss for batch 81: 0.0017969743348658085\n",
      "Shape of I_prime: torch.Size([32, 6144])\n",
      "KL Loss for batch 82: 0.0017965069273486733\n",
      "Shape of I_prime: torch.Size([32, 6144])\n",
      "KL Loss for batch 83: 0.0017954643117263913\n",
      "Shape of I_prime: torch.Size([17, 6144])\n",
      "KL Loss for batch 84: 0.001796278520487249\n",
      "Aligned features saved successfully.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import os\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.distributions as dist\n",
    "from transformers import BertModel, BertTokenizer\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "class TextEncoder(nn.Module):\n",
    "    def __init__(self, bert_model=\"bert-base-uncased\", output_dim=384):\n",
    "        super(TextEncoder, self).__init__()\n",
    "        self.bert = BertModel.from_pretrained(bert_model)\n",
    "        self.tokenizer = BertTokenizer.from_pretrained(bert_model)\n",
    "        self.projection = nn.Linear(self.bert.config.hidden_size, output_dim)\n",
    "        \n",
    "    def encode_dictionary(self, dictionary):\n",
    "        \"\"\"\n",
    "        Encodes dictionary entries using BERT and combines key-value pairs\n",
    "        dictionary: Dict with medical terms as keys and list of related terms as values\n",
    "        Returns: Tensor of shape [num_entries, output_dim]\n",
    "        \"\"\"\n",
    "        encoded_entries = []\n",
    "        \n",
    "        for key, values in dictionary.items():\n",
    "            # Combine key with its values into a single text\n",
    "            if values:  # If values list is not empty\n",
    "                text = key + \": \" + \", \".join(values)\n",
    "            else:\n",
    "                text = key\n",
    "                \n",
    "            # Tokenize and encode\n",
    "            inputs = self.tokenizer(text, \n",
    "                                  padding=True, \n",
    "                                  truncation=True, \n",
    "                                  return_tensors=\"pt\")\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                outputs = self.bert(**inputs)\n",
    "                # Use [CLS] token embedding\n",
    "                embedding = outputs.last_hidden_state[:, 0, :]\n",
    "                \n",
    "            # Project to desired dimension\n",
    "            projected = self.projection(embedding)\n",
    "            encoded_entries.append(projected)\n",
    "            \n",
    "        return torch.cat(encoded_entries, dim=0)\n",
    "\n",
    "# Your original alignment code\n",
    "output_aligned_features_dir = output_directory\n",
    "if(os.path.exists(output_aligned_features_dir + \"aligned_outputs.pt\")):\n",
    "    print(f\"Already at {historical_pt_b}\")\n",
    "\n",
    "batch_size = 32 \n",
    "\n",
    "# Load the medical dictionary\n",
    "medical_dict = {\n",
    "    \"pleural\": [\"hemithorax\", \"effusion\", \"pneumothorax\", \"parenchymal\"],\n",
    "    \"lung\": [\"lungs\", \"pulmonary\", \"hilar\", \"lobe\", \"consolidation\", \n",
    "             \"atelectasis\", \"edema\", \"opacity\", \"pneumonia\"],\n",
    "    \"mediastinal\": [\"mediastinum\", \"diaphragm\", \"hemidiaphragm\"],\n",
    "    \"cardiac\": [\"heart\", \"cardiomegaly\", \"cardiomediastinal\", \"atrium\",\n",
    "                \"ventricle\", \"retrocardiac\"],\n",
    "    \"vascular\": [\"aorta\", \"venous\", \"jugular\", \"aortic\", \"vasculature\", \"cabg\"],\n",
    "    \"osseous\": [\"rib\", \"sternal\", \"subclavian\", \"thoracic\"],\n",
    "    \"trachea\": [\"endotrachea\"],\n",
    "    \"stomach\": [],\n",
    "    \"abdomen\": [],\n",
    "    \"tube\": [\"clips\"],\n",
    "    \"spine\": [\"vertebral\", \"degenerative\"],\n",
    "    \"nodule\": [\"mass\"],\n",
    "    \"chest\": [\"small\", \"enlarged\", \"unchanged\", \"stable\", \"silhouette\",\n",
    "              \"contours\", \"size\", \"focal\", \"mild\", \"acute\"]\n",
    "}\n",
    "\n",
    "# Initialize text encoder and encode dictionary\n",
    "text_encoder = TextEncoder()\n",
    "dictionary_embeddings = text_encoder.encode_dictionary(medical_dict)\n",
    "\n",
    "# final_embeddings = torch.load(output_directory + \"/final_embeddings.pt\")\n",
    "final_embeddings = torch.load(output_directory + \"/final_embeddings.pt\", map_location=torch.device('cpu'))# Shape: [7470, 50, 2048]\n",
    "dictionary_embeddings = dictionary_embeddings.to(torch.float32)  # Convert to float32\n",
    "\n",
    "# Project dictionary embeddings (V) to match the dimensionality of the image embeddings (2048)\n",
    "projection_layer = nn.Linear(384, 6144)\n",
    "V_projected = projection_layer(dictionary_embeddings)  # Shape: [47, 2048]\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.d_k = d_model // num_heads\n",
    "        \n",
    "        # weight matrices for query, key, value\n",
    "        self.W_q = nn.Linear(d_model, d_model)\n",
    "        self.W_k = nn.Linear(d_model, d_model)\n",
    "        self.W_v = nn.Linear(d_model, d_model)\n",
    "        self.W_o = nn.Linear(d_model, d_model)\n",
    "\n",
    "    def scaled_dot_product_attention(self, Q, K, V):\n",
    "        # Compute QK^T / sqrt(d_k)\n",
    "        d_k = Q.size(-1)\n",
    "        scores = torch.matmul(Q, K.transpose(-2, -1)) / torch.sqrt(torch.tensor(d_k).float())\n",
    "        attn_weights = F.softmax(scores, dim=-1)\n",
    "        output = torch.matmul(attn_weights, V)\n",
    "        return output, attn_weights\n",
    "\n",
    "    def forward(self, V, I_prime):\n",
    "        # V:  projected dictionary embeddings\n",
    "        # I_prime: final_embeddings\n",
    "        # print(\"Shape of I_prime:\", I_prime.shape)\n",
    "        I_prime = I_prime.unsqueeze(1) \n",
    "        Q = self.W_q(V)  # Dictionary embeddings\n",
    "        K = self.W_k(I_prime)  # Image embeddings \n",
    "        V = self.W_v(I_prime)\n",
    "        batch_size, seq_len, d_model = K.size()\n",
    "        Q = Q.view(Q.size(0), Q.size(1), self.num_heads, self.d_k).transpose(1, 2)\n",
    "        K = K.view(K.size(0), K.size(1), self.num_heads, self.d_k).transpose(1, 2)\n",
    "        V = V.view(V.size(0), V.size(1), self.num_heads, self.d_k).transpose(1, 2)\n",
    "       \n",
    "        attn_output, attn_weights = self.scaled_dot_product_attention(Q, K, V)\n",
    "        attn_output = attn_output.transpose(1, 2).contiguous().view(V.size(0), -1, self.d_model)\n",
    "        output = self.W_o(attn_output)\n",
    "        \n",
    "        return output, attn_weights\n",
    "\n",
    "class FeedForwardNetwork(nn.Module):\n",
    "    def __init__(self, d_model, d_ff):\n",
    "        super(FeedForwardNetwork, self).__init__()\n",
    "        self.fc1 = nn.Linear(d_model, d_ff)\n",
    "        self.fc2 = nn.Linear(d_ff, d_model)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc2(self.relu(self.fc1(x)))\n",
    "\n",
    "def kl_divergence(mu1, logvar1, mu2, logvar2):\n",
    "    normal1 = dist.Normal(mu1, torch.exp(0.5 * logvar1))\n",
    "    normal2 = dist.Normal(mu2, torch.exp(0.5 * logvar2))\n",
    "    kl_loss = dist.kl.kl_divergence(normal1, normal2).mean()\n",
    "    return kl_loss\n",
    "\n",
    "class Piror(nn.Module):\n",
    "    \"\"\"Fully connected layer to convert encodings to mean and variance\"\"\"\n",
    "    def __init__(self, input_dim=6144, hidden_dim=512):\n",
    "        super(Piror, self).__init__()\n",
    "        self.fc_mu = nn.Linear(input_dim, hidden_dim)\n",
    "        self.fc_var = nn.Linear(input_dim, hidden_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Generate mean and log variance\n",
    "        mu = self.fc_mu(x)\n",
    "        logvar = self.fc_var(x)\n",
    "        return mu, logvar\n",
    "\n",
    "# def compute_kl_loss(mu1, logvar1, mu2, logvar2):\n",
    "#     \"\"\"\n",
    "#     Compute KL divergence between two normal distributions N1(mu1, sigma1) and N2(mu2, sigma2)\n",
    "#     as per equation (12) in the paper\n",
    "#     \"\"\"\n",
    "#     # Convert log variance to variance\n",
    "#     var1 = torch.exp(logvar1)\n",
    "#     var2 = torch.exp(logvar2)\n",
    "    \n",
    "#     # Compute KL divergence according to equation (12)\n",
    "#     kl_div = 0.5 * torch.sum(\n",
    "#         logvar2 - logvar1 + \n",
    "#         (var1 + (mu1 - mu2).pow(2)) / var2 - 1\n",
    "#     )\n",
    "    \n",
    "#     return kl_div    \n",
    "# Model setup\n",
    "d_model = 6144\n",
    "num_heads = 8\n",
    "d_ff = 4096  \n",
    "\n",
    "mha = MultiHeadAttention(d_model, num_heads)\n",
    "ffn = FeedForwardNetwork(d_model, d_ff)\n",
    "piror = Piror(d_model)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "mha = mha.to(device)\n",
    "ffn = ffn.to(device)\n",
    "V_projected = V_projected.to(device)\n",
    "final_embeddings = final_embeddings.to(device)\n",
    "piror = piror.to(device)\n",
    "dataset_size = final_embeddings.size(0)\n",
    "aligned_outputs = []\n",
    "\n",
    "params = list(mha.parameters()) + list(ffn.parameters())  \n",
    "optimizer = torch.optim.Adam(params, lr=1e-4)  \n",
    "\n",
    "for i in range(0, dataset_size, batch_size):\n",
    "    optimizer.zero_grad()\n",
    "    batch_final_embeddings = final_embeddings[i:i+batch_size]\n",
    "    batch_I_prime = batch_final_embeddings\n",
    "    batch_V_repeated = V_projected.unsqueeze(0).repeat(batch_final_embeddings.size(0), 1, 1)\n",
    "\n",
    "    batch_I_prime = batch_I_prime.to(device)\n",
    "    batch_V_repeated = batch_V_repeated.to(device)\n",
    "\n",
    "    aligned_output, _ = mha(batch_V_repeated, batch_I_prime)\n",
    "    aligned_output_ffn = ffn(aligned_output)\n",
    "    V_prime = aligned_output_ffn\n",
    "    V_label = batch_V_repeated \n",
    "    mu1, logvar1 = piror(V_prime)\n",
    "    mu2, logvar2 = piror(V_label)\n",
    "    \n",
    "    # Compute KL divergence loss according to equation (12)\n",
    "    # kl_loss = compute_kl_loss(mu1, logvar1, mu2, logvar2)\n",
    "\n",
    "    # print(f\"KL Loss for batch {i // batch_size}: {kl_loss.item()}\")\n",
    "\n",
    "    #     mu1, logvar1 = torch.randn_like(aligned_output_ffn,requires_grad=True), torch.randn_like(aligned_output_ffn,requires_grad=True)\n",
    "    # mu2, logvar2 = torch.randn_like(batch_V_repeated,requires_grad=True), torch.randn_like(batch_V_repeated,requires_grad=True)\n",
    "    kl_loss = kl_divergence(mu1, logvar1, mu2, logvar2)\n",
    "\n",
    "    print(f\"KL Loss for batch {i // batch_size}: {kl_loss.item()}\")\n",
    "\n",
    "    \n",
    "    kl_loss.backward(retain_graph=True)\n",
    "    optimizer.step()\n",
    "    \n",
    "    aligned_outputs.append(aligned_output_ffn.cpu())\n",
    "\n",
    "aligned_outputs_tensor = torch.cat(aligned_outputs, dim=0)\n",
    "torch.save(aligned_outputs_tensor, os.path.join(output_aligned_features_dir, \"aligned_outputs.pt\"))\n",
    "# Saving each model's state_dict separately\n",
    "torch.save(mha.state_dict(), \"mha.pth\")\n",
    "torch.save(ffn.state_dict(), \"ffn.pth\")\n",
    "torch.save(piror.state_dict(), \"piror.pth\")\n",
    "\n",
    "# Or saving all models together\n",
    "torch.save({\n",
    "    'mha': mha.state_dict(),\n",
    "    'ffn': ffn.state_dict(),\n",
    "    'piror': piror.state_dict(),\n",
    "}, \"model_parameters.pth\")\n",
    "\n",
    "print(\"Aligned features saved successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Uz4pEhW2MuBF"
   },
   "source": [
    "### **Report Generator**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z_WeRBlDMwX2"
   },
   "source": [
    "### **Complete Model**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NZjhDeJxMzgg"
   },
   "source": [
    "## **Training**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7Uk_G0K8M2H9"
   },
   "source": [
    "### **Training**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OXhkaJVvM3Uu"
   },
   "source": [
    "## **Testing**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bzTZZUlQM401"
   },
   "source": [
    "### **Testing**"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "MlOl6I0rMa68",
    "PsELnlXpMjGX",
    "D2kBZTiCMmsE",
    "gIYzXGtsMqmQ",
    "Uz4pEhW2MuBF",
    "Z_WeRBlDMwX2",
    "7Uk_G0K8M2H9",
    "OXhkaJVvM3Uu"
   ],
   "gpuType": "T4",
   "provenance": []
  },
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [],
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
