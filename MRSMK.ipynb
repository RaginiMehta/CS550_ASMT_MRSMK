{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Bnpd7Zh0NPl2"
   },
   "source": [
    "## **Medical Report Summarisation using Medical Knowledge**\n",
    "\n",
    "### **References**\n",
    "\n",
    "**Main Reference**\n",
    "- Radiology report generation with medical knowledge and multilevel image-report alignment: A new method and its verification\n",
    "https://www.sciencedirect.com/science/article/pii/S0933365723002282\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oKdcyk1sMEtD"
   },
   "source": [
    "## **Data Collection**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p81rfgckMR9a"
   },
   "source": [
    "### **Collect Datasets**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2024-11-06T00:57:44.212421Z",
     "iopub.status.busy": "2024-11-06T00:57:44.212032Z",
     "iopub.status.idle": "2024-11-06T00:58:15.371827Z",
     "shell.execute_reply": "2024-11-06T00:58:15.370990Z",
     "shell.execute_reply.started": "2024-11-06T00:57:44.212385Z"
    },
    "id": "AYR_B6u1ojJt",
    "outputId": "faaac1ed-b54b-4cfd-b3e4-cca51372318e"
   },
   "outputs": [],
   "source": [
    "'''Libraries Installation and Import'''\n",
    "\n",
    "# installling necessary libraries\n",
    "!pip -q install --user requests numpy pandas matplotlib tqdm Pillow opencv-python nltk pyspellchecker torch torchvision torchaudio transformers scikit-learn sentence-transformers\n",
    "\n",
    "# importing required libraries\n",
    "import os\n",
    "import re\n",
    "import csv\n",
    "import requests\n",
    "import tarfile\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import xml.etree.ElementTree as ET\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "import cv2\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from spellchecker import SpellChecker\n",
    "from collections import Counter\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torch.distributions as dist\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import datasets\n",
    "from torchvision import models, transforms\n",
    "\n",
    "import transformers\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from transformers import BlipProcessor, BlipForConditionalGeneration\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import DistilBertModel, DistilBertTokenizer\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-05T19:37:40.284922Z",
     "iopub.status.busy": "2024-11-05T19:37:40.284313Z",
     "iopub.status.idle": "2024-11-05T19:37:40.293106Z",
     "shell.execute_reply": "2024-11-05T19:37:40.292076Z",
     "shell.execute_reply.started": "2024-11-05T19:37:40.284886Z"
    }
   },
   "outputs": [],
   "source": [
    "'''Setting Paths'''\n",
    "\n",
    "# project directory\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')\n",
    "# project_directory = '/content/drive/Othercomputers/My Laptop/CS550_ASMT_MRSMK/datasets'\n",
    "# project_directory = '/content/drive/MyDrive/Academics/CS550 Machine Learning/CS550 ASMT MRSMK/datasets'\n",
    "\n",
    "project_directory = \"./datasets\"\n",
    "dataset = 'iu_xray/'\n",
    "iu_xray_dataset = os.path.join(project_directory, dataset)\n",
    "\n",
    "\n",
    "# input directory\n",
    "input_directory = os.path.join(iu_xray_dataset, \"input\")\n",
    "\n",
    "images_dir = os.path.join(input_directory, \"images\")\n",
    "reports_dir = os.path.join(input_directory, \"reports\")\n",
    "iu_xray_images = images_dir\n",
    "iu_xray_reports = os.path.join(reports_dir, 'ecgen-radiology')\n",
    "\n",
    "\n",
    "# output directory \n",
    "output_directory = os.path.join(iu_xray_dataset, \"output\")\n",
    "os.makedirs(output_directory, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2024-11-05T19:46:05.767945Z",
     "iopub.status.busy": "2024-11-05T19:46:05.767128Z",
     "iopub.status.idle": "2024-11-05T19:46:05.783259Z",
     "shell.execute_reply": "2024-11-05T19:46:05.782357Z",
     "shell.execute_reply.started": "2024-11-05T19:46:05.767905Z"
    },
    "id": "49-0xiXplwH8",
    "outputId": "ea63eda3-908a-4f5f-eeec-a4019130e57f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://openi.nlm.nih.gov/imgs/collections/NLMCXR_png.tgz already exists at: ./datasets/iu_xray/input/images\n",
      "https://openi.nlm.nih.gov/imgs/collections/NLMCXR_reports.tgz already exists at: ./datasets/iu_xray/input/reports\n"
     ]
    }
   ],
   "source": [
    "'''Setup - Generalized'''\n",
    "\n",
    "# setup to download the IU X-Ray Dataset\n",
    "images_url = \"https://openi.nlm.nih.gov/imgs/collections/NLMCXR_png.tgz\"\n",
    "reports_url = \"https://openi.nlm.nih.gov/imgs/collections/NLMCXR_reports.tgz\"\n",
    "\n",
    "\n",
    "# function to check the file size of a given URL\n",
    "def get_file_size(url):\n",
    "    response = requests.head(url)\n",
    "    size_in_bytes = int(response.headers.get('Content-Length', 0))\n",
    "    size_in_mb = size_in_bytes / (1024 * 1024)\n",
    "    return size_in_mb\n",
    "\n",
    "\n",
    "# function to download and extract from a given url to a given directory\n",
    "def download_and_extract(url, save_dir):\n",
    "    file_name = url.split('/')[-1]\n",
    "    file_path = os.path.join(save_dir, file_name)\n",
    "\n",
    "    response = requests.get(url, stream=True)\n",
    "    total_size = int(response.headers.get('Content-Length', 0))\n",
    "    downloaded_size = 0\n",
    "\n",
    "    with open(file_path, 'wb') as file:\n",
    "        for chunk in response.iter_content(chunk_size=1024):\n",
    "            if chunk:\n",
    "                file.write(chunk)\n",
    "                downloaded_size += len(chunk)\n",
    "                percent_complete = (downloaded_size / total_size) * 100\n",
    "                print(f\"Downloaded {downloaded_size / (1024*1024):.2f} MB out of {total_size / (1024*1024):.2f} MB: {percent_complete:.2f}% complete\", end=\"\\r\")\n",
    "\n",
    "    print(\"\\nDownload complete!\")\n",
    "\n",
    "    with tarfile.open(file_path, 'r:gz') as tar:\n",
    "        members = tar.getmembers()\n",
    "        total_files = len(members)\n",
    "\n",
    "        for idx, member in enumerate(members, start=1):\n",
    "            tar.extract(member, path=save_dir)\n",
    "            print(f\"Extracting File {idx} out of {total_files}: {member.name}\", end=\"\\r\")\n",
    "\n",
    "    os.remove(file_path)\n",
    "\n",
    "\n",
    "# downloading  IU X-Ray dataset\n",
    "if not os.path.exists(images_dir):\n",
    "    images_size = get_file_size(images_url)\n",
    "    print(f\"Downloading {images_url} to: {images_dir} ({images_size:.2f} MB)\")\n",
    "    os.makedirs(images_dir, exist_ok=True)\n",
    "    download_and_extract(images_url, images_dir)\n",
    "    print(f\"Downloaded {images_url} to: {images_dir}\")\n",
    "else:\n",
    "    print(f\"{images_url} already exists at: {images_dir}\")\n",
    "\n",
    "if not os.path.exists(reports_dir):\n",
    "    reports_size = get_file_size(reports_url)\n",
    "    print(f\"Downloading {reports_url} to: {reports_dir} ({reports_size:.2f} MB)\")\n",
    "    os.makedirs(reports_dir, exist_ok=True)\n",
    "    download_and_extract(reports_url, reports_dir)\n",
    "    print(f\"Downloaded {reports_url} to: {reports_dir}\")\n",
    "else:\n",
    "    print(f\"{reports_url} already exists at: {reports_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2024-11-05T19:49:31.242765Z",
     "iopub.status.busy": "2024-11-05T19:49:31.242003Z",
     "iopub.status.idle": "2024-11-05T19:49:31.256253Z",
     "shell.execute_reply": "2024-11-05T19:49:31.255411Z",
     "shell.execute_reply.started": "2024-11-05T19:49:31.242726Z"
    },
    "id": "5ZSA8Jyowaoh",
    "outputId": "dffa4899-00a7-4d49-a118-8dd85400ba06"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Path:  ./datasets/iu_xray/input/images\n",
      "Directory Contents: 7471 Images\n",
      "\n",
      "Path:  ./datasets/iu_xray/input/reports/ecgen-radiology\n",
      "Directory Contents: 3955 Reports\n"
     ]
    }
   ],
   "source": [
    "'''Exploring the IU X-Ray Dataset Contents'''\n",
    "\n",
    "# displaying directory and subdirectory contents\n",
    "print(\"\\nPath: \", iu_xray_images)\n",
    "print(f\"Directory Contents: {len(os.listdir(iu_xray_images))} Images\")\n",
    "\n",
    "print(\"\\nPath: \", iu_xray_reports)\n",
    "print(f\"Directory Contents: {len(os.listdir(iu_xray_reports))} Reports\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 822
    },
    "execution": {
     "iopub.execute_input": "2024-11-05T19:50:00.028492Z",
     "iopub.status.busy": "2024-11-05T19:50:00.027625Z",
     "iopub.status.idle": "2024-11-05T19:50:35.022522Z",
     "shell.execute_reply": "2024-11-05T19:50:35.021545Z",
     "shell.execute_reply.started": "2024-11-05T19:50:00.028452Z"
    },
    "id": "qGczexPLUaN5",
    "outputId": "8c7528a1-7064-4802-c276-edef004f0d5b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataframe saved to ./datasets/iu_xray/output/iu_xray_images_df.csv\n",
      "\n",
      "\n",
      "Dataframe Shape: (7470, 9)\n",
      "\n",
      "\n",
      "Dataframe Information:\n",
      "\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 7470 entries, 0 to 7469\n",
      "Data columns (total 9 columns):\n",
      " #   Column          Non-Null Count  Dtype \n",
      "---  ------          --------------  ----- \n",
      " 0   pmc_id          7470 non-null   object\n",
      " 1   image_filename  7470 non-null   object\n",
      " 2   caption         7468 non-null   object\n",
      " 3   comparison      6313 non-null   object\n",
      " 4   indication      7311 non-null   object\n",
      " 5   findings        6473 non-null   object\n",
      " 6   impression      7418 non-null   object\n",
      " 7   height          7470 non-null   int64 \n",
      " 8   width           7470 non-null   int64 \n",
      "dtypes: int64(2), object(7)\n",
      "memory usage: 525.4+ KB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Displaying Dataframe:\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pmc_id</th>\n",
       "      <th>image_filename</th>\n",
       "      <th>caption</th>\n",
       "      <th>comparison</th>\n",
       "      <th>indication</th>\n",
       "      <th>findings</th>\n",
       "      <th>impression</th>\n",
       "      <th>height</th>\n",
       "      <th>width</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>718</td>\n",
       "      <td>CXR718_IM-2280-1001.png</td>\n",
       "      <td>Xray Chest PA and Lateral</td>\n",
       "      <td>None</td>\n",
       "      <td>XXXX for 3 weeks</td>\n",
       "      <td>The lungs are clear. There is no pleural effus...</td>\n",
       "      <td>No acute pulmonary disease.</td>\n",
       "      <td>512</td>\n",
       "      <td>512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>718</td>\n",
       "      <td>CXR718_IM-2280-3001.png</td>\n",
       "      <td>Xray Chest PA and Lateral</td>\n",
       "      <td>None</td>\n",
       "      <td>XXXX for 3 weeks</td>\n",
       "      <td>The lungs are clear. There is no pleural effus...</td>\n",
       "      <td>No acute pulmonary disease.</td>\n",
       "      <td>513</td>\n",
       "      <td>512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2793</td>\n",
       "      <td>CXR2793_IM-1226-1001.png</td>\n",
       "      <td>Radiographs of the chest, 2 views, dated XXXX,...</td>\n",
       "      <td>None.</td>\n",
       "      <td>XXXX-year-old female. Dyspnea.</td>\n",
       "      <td>The cardiomediastinal silhouette is normal in ...</td>\n",
       "      <td>Mild lung hyperexpansion, otherwise clear.</td>\n",
       "      <td>512</td>\n",
       "      <td>512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2793</td>\n",
       "      <td>CXR2793_IM-1226-2001.png</td>\n",
       "      <td>Radiographs of the chest, 2 views, dated XXXX,...</td>\n",
       "      <td>None.</td>\n",
       "      <td>XXXX-year-old female. Dyspnea.</td>\n",
       "      <td>The cardiomediastinal silhouette is normal in ...</td>\n",
       "      <td>Mild lung hyperexpansion, otherwise clear.</td>\n",
       "      <td>512</td>\n",
       "      <td>512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3773</td>\n",
       "      <td>CXR3773_IM-1891-1001.png</td>\n",
       "      <td>Xray Chest PA and Lateral</td>\n",
       "      <td>None.</td>\n",
       "      <td>Numbness and tingling in the left arm, nausea ...</td>\n",
       "      <td>The lungs and pleural spaces show no acute abn...</td>\n",
       "      <td>1. No acute pulmonary abnormality.</td>\n",
       "      <td>511</td>\n",
       "      <td>512</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  pmc_id            image_filename  \\\n",
       "0    718   CXR718_IM-2280-1001.png   \n",
       "1    718   CXR718_IM-2280-3001.png   \n",
       "2   2793  CXR2793_IM-1226-1001.png   \n",
       "3   2793  CXR2793_IM-1226-2001.png   \n",
       "4   3773  CXR3773_IM-1891-1001.png   \n",
       "\n",
       "                                             caption comparison  \\\n",
       "0                          Xray Chest PA and Lateral       None   \n",
       "1                          Xray Chest PA and Lateral       None   \n",
       "2  Radiographs of the chest, 2 views, dated XXXX,...      None.   \n",
       "3  Radiographs of the chest, 2 views, dated XXXX,...      None.   \n",
       "4                          Xray Chest PA and Lateral      None.   \n",
       "\n",
       "                                          indication  \\\n",
       "0                                   XXXX for 3 weeks   \n",
       "1                                   XXXX for 3 weeks   \n",
       "2                     XXXX-year-old female. Dyspnea.   \n",
       "3                     XXXX-year-old female. Dyspnea.   \n",
       "4  Numbness and tingling in the left arm, nausea ...   \n",
       "\n",
       "                                            findings  \\\n",
       "0  The lungs are clear. There is no pleural effus...   \n",
       "1  The lungs are clear. There is no pleural effus...   \n",
       "2  The cardiomediastinal silhouette is normal in ...   \n",
       "3  The cardiomediastinal silhouette is normal in ...   \n",
       "4  The lungs and pleural spaces show no acute abn...   \n",
       "\n",
       "                                   impression  height  width  \n",
       "0                 No acute pulmonary disease.     512    512  \n",
       "1                 No acute pulmonary disease.     513    512  \n",
       "2  Mild lung hyperexpansion, otherwise clear.     512    512  \n",
       "3  Mild lung hyperexpansion, otherwise clear.     512    512  \n",
       "4          1. No acute pulmonary abnormality.     511    512  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "'''Processing Textual Data from each .xml Report File and Storing it in a .csv File'''\n",
    "\n",
    "# function to iterate through all .xml report files and storing them in a dataframe\n",
    "def save_images_df():\n",
    "    data = []\n",
    "    cnt = 0\n",
    "    for file in os.listdir(iu_xray_reports):\n",
    "        if file.endswith(\".xml\"):\n",
    "            cnt += 1\n",
    "            print(f\"Processing .xml File {cnt} out of {len(os.listdir(iu_xray_reports))}: {file}\", end=\"\\r\")\n",
    "\n",
    "            file_path = os.path.join(iu_xray_reports, file)\n",
    "            try:\n",
    "                tree = ET.parse(file_path)\n",
    "                root = tree.getroot()\n",
    "\n",
    "                pmc_id = root.find('.//pmcId').attrib.get('id')\n",
    "\n",
    "                comparison = indication = findings = impression = None\n",
    "\n",
    "                for abstract in root.findall('.//AbstractText'):\n",
    "                    if abstract.attrib.get('Label') == 'COMPARISON':\n",
    "                        comparison = abstract.text\n",
    "                    elif abstract.attrib.get('Label') == 'INDICATION':\n",
    "                        indication = abstract.text\n",
    "                    elif abstract.attrib.get('Label') == 'FINDINGS':\n",
    "                        findings = abstract.text\n",
    "                    elif abstract.attrib.get('Label') == 'IMPRESSION':\n",
    "                        impression = abstract.text\n",
    "\n",
    "                for parent_image in root.findall('parentImage'):\n",
    "                    image_file = parent_image.attrib['id'] + \".png\"\n",
    "                    image_path = os.path.join(iu_xray_images, image_file)\n",
    "                    image = cv2.imread(image_path)\n",
    "\n",
    "                    if image is not None:\n",
    "                        height, width, channels = image.shape\n",
    "                        caption = parent_image.find('caption').text if parent_image.find('caption') is not None else None\n",
    "                        data.append([pmc_id, image_file, caption, comparison, indication, findings, impression, height, width])\n",
    "                    else:\n",
    "                        print(f\"Warning: Unable to read image {image_path}\")\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing file {file}: {e}\")\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "# creating a dataframe and saving it as .csv\n",
    "iu_xray_images_df_path = os.path.join(output_directory, 'iu_xray_images_df.csv')\n",
    "if os.path.exists(iu_xray_images_df_path):\n",
    "    data = save_images_df()\n",
    "    columns = ['pmc_id', 'image_filename', 'caption', 'comparison', 'indication', 'findings', 'impression', 'height', 'width']\n",
    "    iu_xray_images_df = pd.DataFrame(data, columns=columns)\n",
    "    iu_xray_images_df.to_csv(iu_xray_images_df_path, index=False)\n",
    "    print(f\"Dataframe saved to {iu_xray_images_df_path}\")\n",
    "else:\n",
    "    print(f\"Dataframe already exists at {iu_xray_images_df_path}\")\n",
    "    iu_xray_images_df = pd.read_csv(iu_xray_images_df_path)\n",
    "\n",
    "\n",
    "# displaying the stored dataframe\n",
    "print(\"\\n\\nDataframe Shape:\", iu_xray_images_df.shape)\n",
    "\n",
    "print(\"\\n\\nDataframe Information:\\n\")\n",
    "display(iu_xray_images_df.info())\n",
    "\n",
    "print(\"\\n\\nDisplaying Dataframe:\\n\")\n",
    "display(iu_xray_images_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 893
    },
    "execution": {
     "iopub.execute_input": "2024-11-05T19:38:14.890381Z",
     "iopub.status.busy": "2024-11-05T19:38:14.890074Z",
     "iopub.status.idle": "2024-11-05T19:38:24.677102Z",
     "shell.execute_reply": "2024-11-05T19:38:24.676278Z",
     "shell.execute_reply.started": "2024-11-05T19:38:14.890348Z"
    },
    "id": "vYnfzXXT0O6w",
    "outputId": "d2692046-ebe0-45ac-8291-37641f6d5958"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataframe saved to ./datasets/iu_xray/output/iu_xray_reports_df.csv\n",
      "\n",
      "\n",
      "Dataframe Shape: (3955, 11)\n",
      "\n",
      "\n",
      "Dataframe Information:\n",
      "\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 3955 entries, 0 to 3954\n",
      "Data columns (total 11 columns):\n",
      " #   Column       Non-Null Count  Dtype \n",
      "---  ------       --------------  ----- \n",
      " 0   pmc_id       3955 non-null   object\n",
      " 1   findings     3425 non-null   object\n",
      " 2   impression   3921 non-null   object\n",
      " 3   comparison   3333 non-null   object\n",
      " 4   indication   3865 non-null   object\n",
      " 5   image_count  3955 non-null   int64 \n",
      " 6   image_1      3851 non-null   object\n",
      " 7   image_2      3405 non-null   object\n",
      " 8   image_3      197 non-null    object\n",
      " 9   image_4      16 non-null     object\n",
      " 10  image_5      1 non-null      object\n",
      "dtypes: int64(1), object(10)\n",
      "memory usage: 340.0+ KB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Displaying Dataframe:\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pmc_id</th>\n",
       "      <th>findings</th>\n",
       "      <th>impression</th>\n",
       "      <th>comparison</th>\n",
       "      <th>indication</th>\n",
       "      <th>image_count</th>\n",
       "      <th>image_1</th>\n",
       "      <th>image_2</th>\n",
       "      <th>image_3</th>\n",
       "      <th>image_4</th>\n",
       "      <th>image_5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>718</td>\n",
       "      <td>The lungs are clear. There is no pleural effus...</td>\n",
       "      <td>No acute pulmonary disease.</td>\n",
       "      <td>None</td>\n",
       "      <td>XXXX for 3 weeks</td>\n",
       "      <td>2</td>\n",
       "      <td>CXR718_IM-2280-1001.jpg: Xray Chest PA and Lat...</td>\n",
       "      <td>CXR718_IM-2280-3001.jpg: Xray Chest PA and Lat...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2793</td>\n",
       "      <td>The cardiomediastinal silhouette is normal in ...</td>\n",
       "      <td>Mild lung hyperexpansion, otherwise clear.</td>\n",
       "      <td>None.</td>\n",
       "      <td>XXXX-year-old female. Dyspnea.</td>\n",
       "      <td>2</td>\n",
       "      <td>CXR2793_IM-1226-1001.jpg: Radiographs of the c...</td>\n",
       "      <td>CXR2793_IM-1226-2001.jpg: Radiographs of the c...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3773</td>\n",
       "      <td>The lungs and pleural spaces show no acute abn...</td>\n",
       "      <td>1. No acute pulmonary abnormality.</td>\n",
       "      <td>None.</td>\n",
       "      <td>Numbness and tingling in the left arm, nausea ...</td>\n",
       "      <td>2</td>\n",
       "      <td>CXR3773_IM-1891-1001.jpg: Xray Chest PA and La...</td>\n",
       "      <td>CXR3773_IM-1891-2001.jpg: Xray Chest PA and La...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3785</td>\n",
       "      <td>No pneumothorax or large pleural effusion. Mil...</td>\n",
       "      <td>No acute cardiopulmonary disease.</td>\n",
       "      <td>None available</td>\n",
       "      <td>XXXX-year-old female with one-XXXX history of ...</td>\n",
       "      <td>2</td>\n",
       "      <td>CXR3785_IM-1898-1001.jpg: Chest XXXX and lateral</td>\n",
       "      <td>CXR3785_IM-1898-2001.jpg: Chest XXXX and lateral</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>The cardiac contours are normal. XXXX basilar ...</td>\n",
       "      <td>Basilar atelectasis. No confluent lobar consol...</td>\n",
       "      <td>XXXX, XXXX</td>\n",
       "      <td>Preop lumbar surgery</td>\n",
       "      <td>2</td>\n",
       "      <td>CXR7_IM-2263-1001.jpg: Xray Chest PA and Lateral</td>\n",
       "      <td>CXR7_IM-2263-2001.jpg: Xray Chest PA and Lateral</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  pmc_id                                           findings  \\\n",
       "0    718  The lungs are clear. There is no pleural effus...   \n",
       "1   2793  The cardiomediastinal silhouette is normal in ...   \n",
       "2   3773  The lungs and pleural spaces show no acute abn...   \n",
       "3   3785  No pneumothorax or large pleural effusion. Mil...   \n",
       "4      7  The cardiac contours are normal. XXXX basilar ...   \n",
       "\n",
       "                                          impression      comparison  \\\n",
       "0                        No acute pulmonary disease.            None   \n",
       "1         Mild lung hyperexpansion, otherwise clear.           None.   \n",
       "2                 1. No acute pulmonary abnormality.           None.   \n",
       "3                  No acute cardiopulmonary disease.  None available   \n",
       "4  Basilar atelectasis. No confluent lobar consol...      XXXX, XXXX   \n",
       "\n",
       "                                          indication  image_count  \\\n",
       "0                                   XXXX for 3 weeks            2   \n",
       "1                     XXXX-year-old female. Dyspnea.            2   \n",
       "2  Numbness and tingling in the left arm, nausea ...            2   \n",
       "3  XXXX-year-old female with one-XXXX history of ...            2   \n",
       "4                               Preop lumbar surgery            2   \n",
       "\n",
       "                                             image_1  \\\n",
       "0  CXR718_IM-2280-1001.jpg: Xray Chest PA and Lat...   \n",
       "1  CXR2793_IM-1226-1001.jpg: Radiographs of the c...   \n",
       "2  CXR3773_IM-1891-1001.jpg: Xray Chest PA and La...   \n",
       "3  CXR3785_IM-1898-1001.jpg: Chest XXXX and lateral    \n",
       "4   CXR7_IM-2263-1001.jpg: Xray Chest PA and Lateral   \n",
       "\n",
       "                                             image_2 image_3 image_4 image_5  \n",
       "0  CXR718_IM-2280-3001.jpg: Xray Chest PA and Lat...     NaN     NaN     NaN  \n",
       "1  CXR2793_IM-1226-2001.jpg: Radiographs of the c...     NaN     NaN     NaN  \n",
       "2  CXR3773_IM-1891-2001.jpg: Xray Chest PA and La...     NaN     NaN     NaN  \n",
       "3  CXR3785_IM-1898-2001.jpg: Chest XXXX and lateral      NaN     NaN     NaN  \n",
       "4   CXR7_IM-2263-2001.jpg: Xray Chest PA and Lateral     NaN     NaN     NaN  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "'''Processing Textual Data from each .xml Report File and Storing it in a .csv File'''\n",
    "\n",
    "# function to iterate through all .xml report files and storing them in a dataframe\n",
    "def save_reports_df():\n",
    "    data = []\n",
    "    cnt = 0\n",
    "    for file in os.listdir(iu_xray_reports):\n",
    "        if file.endswith(\".xml\"):\n",
    "            cnt += 1\n",
    "            print(f\"Processing .xml File {cnt} out of {len(os.listdir(iu_xray_reports))}: {file}\", end=\"\\r\")\n",
    "\n",
    "            file_path = os.path.join(iu_xray_reports, file)\n",
    "            try:\n",
    "                tree = ET.parse(file_path)\n",
    "                root = tree.getroot()\n",
    "\n",
    "                pmc_id = root.find('.//pmcId').attrib.get('id')\n",
    "\n",
    "                comparison = indication = findings = impression = None\n",
    "\n",
    "                for abstract in root.findall('.//AbstractText'):\n",
    "                    if abstract.attrib.get('Label') == 'COMPARISON':\n",
    "                        comparison = abstract.text\n",
    "                    elif abstract.attrib.get('Label') == 'INDICATION':\n",
    "                        indication = abstract.text\n",
    "                    elif abstract.attrib.get('Label') == 'FINDINGS':\n",
    "                        findings = abstract.text\n",
    "                    elif abstract.attrib.get('Label') == 'IMPRESSION':\n",
    "                        impression = abstract.text\n",
    "\n",
    "                report_data = {\n",
    "                    'pmc_id': pmc_id,\n",
    "                    'findings': findings,\n",
    "                    'impression': impression,\n",
    "                    'comparison': comparison,\n",
    "                    'indication': indication,\n",
    "                }\n",
    "\n",
    "                parent_images = root.findall('parentImage')\n",
    "                report_data['image_count'] = len(parent_images)\n",
    "\n",
    "                for i, parent_image in enumerate(parent_images, start=1):\n",
    "                    image_file = parent_image.attrib['id'] + \".jpg\"\n",
    "                    caption = parent_image.find('caption').text if parent_image.find('caption') is not None else None\n",
    "                    report_data[f'image_{i}'] = f\"{image_file}: {caption}\" if caption else image_file\n",
    "\n",
    "                data.append(report_data)\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing file {file}: {e}\")\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "# creating a dataframe and saving it as .csv\n",
    "iu_xray_reports_df_path = os.path.join(output_directory, 'iu_xray_reports_df.csv')\n",
    "if not os.path.exists(iu_xray_reports_df_path):\n",
    "    data = save_reports_df()\n",
    "    iu_xray_reports_df = pd.DataFrame(data)\n",
    "    iu_xray_reports_df.to_csv(iu_xray_reports_df_path, index=False)\n",
    "    print(f\"Dataframe saved to {iu_xray_reports_df_path}\")\n",
    "else:\n",
    "    print(f\"Dataframe already exists at {iu_xray_reports_df_path}\")\n",
    "    iu_xray_reports_df = pd.read_csv(iu_xray_reports_df_path)\n",
    "\n",
    "\n",
    "# displaying the stored dataframe\n",
    "print(\"\\n\\nDataframe Shape:\", iu_xray_reports_df.shape)\n",
    "\n",
    "print(\"\\n\\nDataframe Information:\\n\")\n",
    "display(iu_xray_reports_df.info())\n",
    "\n",
    "print(\"\\n\\nDisplaying Dataframe:\\n\")\n",
    "display(iu_xray_reports_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 310
    },
    "execution": {
     "iopub.execute_input": "2024-11-05T19:38:24.678660Z",
     "iopub.status.busy": "2024-11-05T19:38:24.678330Z",
     "iopub.status.idle": "2024-11-05T19:38:24.690982Z",
     "shell.execute_reply": "2024-11-05T19:38:24.689927Z",
     "shell.execute_reply.started": "2024-11-05T19:38:24.678625Z"
    },
    "id": "bw4Ylfa94M1o",
    "outputId": "5ef503a0-265c-423b-8cae-1d36b136134d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Number of Images per Report:\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>images_qty</th>\n",
       "      <th>reports_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>3208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>446</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   images_qty  reports_count\n",
       "0           2           3208\n",
       "1           1            446\n",
       "2           3            181\n",
       "3           0            104\n",
       "4           4             15\n",
       "5           5              1"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "'''Displaying the Number of Images per Report'''\n",
    "\n",
    "# displaying the distribution of number of images per report\n",
    "reports_count = iu_xray_reports_df['image_count'].value_counts().rename_axis('images_qty').reset_index(name='reports_count')\n",
    "print(\"\\n\\nNumber of Images per Report:\\n\")\n",
    "display(reports_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-05T19:38:24.692631Z",
     "iopub.status.busy": "2024-11-05T19:38:24.692318Z",
     "iopub.status.idle": "2024-11-05T19:38:24.701661Z",
     "shell.execute_reply": "2024-11-05T19:38:24.700684Z",
     "shell.execute_reply.started": "2024-11-05T19:38:24.692597Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of duplicates in 'pmc_id' column: 0\n",
      "Duplicated rows in 'pmc_id' column:\n",
      "Empty DataFrame\n",
      "Columns: [pmc_id, findings, impression, comparison, indication, image_count, image_1, image_2, image_3, image_4, image_5]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "'''Checking for Duplicates'''\n",
    "\n",
    "# Check for duplicate values in the 'pmc_id' column\n",
    "duplicates_in_pmc_id = iu_xray_reports_df['pmc_id'].duplicated()\n",
    "num_duplicates = duplicates_in_pmc_id.sum()\n",
    "\n",
    "# Display the duplicated rows\n",
    "duplicated_rows = iu_xray_reports_df[duplicates_in_pmc_id]\n",
    "print(f\"Number of duplicates in 'pmc_id' column: {num_duplicates}\")\n",
    "print(\"Duplicated rows in 'pmc_id' column:\")\n",
    "print(duplicated_rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6UouFxQwMNeo"
   },
   "source": [
    "## **Data Preprocessing**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-cGf99_CMV47"
   },
   "source": [
    "### **Preprocess Images**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2024-11-05T19:50:54.681672Z",
     "iopub.status.busy": "2024-11-05T19:50:54.681077Z",
     "iopub.status.idle": "2024-11-05T19:54:41.097480Z",
     "shell.execute_reply": "2024-11-05T19:54:41.096566Z",
     "shell.execute_reply.started": "2024-11-05T19:50:54.681618Z"
    },
    "id": "JTClOSxeSCOM",
    "outputId": "811cbc93-70e7-4d60-f213-26238488c44d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing Images to: ./datasets/iu_xray/output/images_preprocessed\n",
      "Preprocessed Images saved to: ./datasets/iu_xray/output/images_preprocessed\n"
     ]
    }
   ],
   "source": [
    "'''Preprocessing Images - Resizing, Tensor Conversion and Normalization'''\n",
    "\n",
    "# function to preprocess and save images\n",
    "def preprocess_images(input_dir, output_dir):\n",
    "    preprocess = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ])\n",
    "\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    cnt = 0\n",
    "    for filename in os.listdir(input_dir):\n",
    "        if filename.endswith('.png'):\n",
    "            cnt += 1\n",
    "            print(f\"Preprocessing File {cnt} out of {len(os.listdir(input_dir))}: {filename}\", end=\"\\r\")\n",
    "\n",
    "            image_path = os.path.join(input_dir, filename)\n",
    "            image = Image.open(image_path).convert('RGB')\n",
    "            processed_image = preprocess(image)\n",
    "\n",
    "            processed_image_path = os.path.join(output_dir, filename)\n",
    "\n",
    "            processed_image_pil = transforms.ToPILImage()(processed_image)\n",
    "            processed_image_pil.save(processed_image_path)\n",
    "\n",
    "\n",
    "# preprocessing images\n",
    "iu_xray_images_preprocessed = os.path.join(output_directory, 'images_preprocessed')\n",
    "if not os.path.exists(iu_xray_images_preprocessed):\n",
    "    print(f\"Preprocessing Images to: {iu_xray_images_preprocessed}\")\n",
    "    preprocess_images(iu_xray_images, iu_xray_images_preprocessed)\n",
    "    print(f\"Preprocessed Images saved to: {iu_xray_images_preprocessed}\")\n",
    "else:\n",
    "    print(f\"Preprocessed Images already exist at: {iu_xray_images_preprocessed}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lvdRoNqXMZlN"
   },
   "source": [
    "### **Preprocess Text**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-05T19:55:07.711161Z",
     "iopub.status.busy": "2024-11-05T19:55:07.710785Z",
     "iopub.status.idle": "2024-11-05T19:55:08.086572Z",
     "shell.execute_reply": "2024-11-05T19:55:08.085752Z",
     "shell.execute_reply.started": "2024-11-05T19:55:07.711125Z"
    },
    "id": "M8sW8uz8-plE"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /usr/share/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /usr/share/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "'''Preprocessing Text - Lowercasing, Decontracting, Punctuation Removal, Number Removal, Two-Letter Word Removal, Stop Word Removal, Spell Checking, Extra Space Removal'''\n",
    "\n",
    "# download nltk resources and initialize spell checker\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "spell = SpellChecker()\n",
    "\n",
    "\n",
    "# function to convert text to lowercase\n",
    "def lowercase(text):\n",
    "    return text.lower() if isinstance(text, str) else text\n",
    "\n",
    "\n",
    "# function to decontract words\n",
    "def decontracted(text):\n",
    "    if not isinstance(text, str):\n",
    "        return text\n",
    "    contractions = {\n",
    "        \"won't\": \"will not\", \"can't\": \"can not\", \"couldn't\": \"could not\",\n",
    "        \"shouldn't\": \"should not\", \"wouldn't\": \"would not\", \"n't\": \" not\",\n",
    "        \"'re\": \" are\", \"'s\": \" is\", \"'d\": \" would\", \"'ll\": \" will\",\n",
    "        \"'t\": \" not\", \"'ve\": \" have\", \"'m\": \" am\"\n",
    "    }\n",
    "    for contraction, full_form in contractions.items():\n",
    "        text = text.replace(contraction, full_form)\n",
    "    return text\n",
    "\n",
    "\n",
    "# function to remove punctuations\n",
    "def rem_punctuations(text):\n",
    "    return re.sub(r'[^\\w\\s]', ' ', text) if isinstance(text, str) else text\n",
    "\n",
    "\n",
    "# function to remove numbers\n",
    "def rem_numbers(text):\n",
    "    if not isinstance(text, str):\n",
    "        return text\n",
    "    text = re.sub(r'[xX]{2,}', '', text)\n",
    "    return re.sub(r'\\d+', '', text)\n",
    "\n",
    "\n",
    "# function to remove two-letter words except \"no\" and \"ct\"\n",
    "def rem_two_letter_words(text):\n",
    "    if not isinstance(text, str):\n",
    "        return text\n",
    "    return ' '.join(word for word in text.split() if len(word) > 2 or word in [\"no\", \"ct\"])\n",
    "\n",
    "\n",
    "# function to remove stop words\n",
    "def rem_stop_words(text):\n",
    "    if not isinstance(text, str):\n",
    "        return text\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    return ' '.join(word for word in text.split() if word not in stop_words)\n",
    "\n",
    "\n",
    "# function to correct spelling\n",
    "def correct_spelling(text):\n",
    "    if not isinstance(text, str):\n",
    "        return text\n",
    "    corrected = []\n",
    "    for word in text.split():\n",
    "        corrected_word = list(spell.candidates(word))[0] if spell.candidates(word) else word\n",
    "        corrected.append(corrected_word)\n",
    "    return ' '.join(corrected)\n",
    "\n",
    "\n",
    "# function to remove extra spaces\n",
    "def rem_extra_spaces(text):\n",
    "    return ' '.join(text.split()) if isinstance(text, str) else text\n",
    "\n",
    "\n",
    "# function to handle full stops\n",
    "def handle_fullstops(text):\n",
    "    if not isinstance(text, str):\n",
    "        return text\n",
    "    text = re.sub(r'\\.\\.+', '.', text) \n",
    "    return re.sub(r'\\.', ' . ', text) \n",
    "\n",
    "\n",
    "# function to remove apostrophes\n",
    "def rem_apostrophes(text):\n",
    "    return re.sub(\"'\", '', text) if isinstance(text, str) else text\n",
    "\n",
    "\n",
    "# function to preprocess text\n",
    "def preprocess_text(data):\n",
    "    preprocessed = []\n",
    "    for sentence in tqdm(data.values):\n",
    "        sentence = str(sentence)\n",
    "        sentence = lowercase(sentence)\n",
    "        sentence = decontracted(sentence)\n",
    "        sentence = rem_punctuations(sentence)\n",
    "        sentence = rem_numbers(sentence)\n",
    "        sentence = rem_two_letter_words(sentence)\n",
    "        sentence = rem_stop_words(sentence)\n",
    "        sentence = correct_spelling(sentence)\n",
    "        sentence = rem_extra_spaces(sentence)\n",
    "        sentence = handle_fullstops(sentence)\n",
    "        sentence = rem_apostrophes(sentence)\n",
    "        \n",
    "        preprocessed.append(sentence)\n",
    "\n",
    "    return preprocessed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-06T00:51:02.841124Z",
     "iopub.status.busy": "2024-11-06T00:51:02.840312Z",
     "iopub.status.idle": "2024-11-06T00:51:03.040397Z",
     "shell.execute_reply": "2024-11-06T00:51:03.039198Z",
     "shell.execute_reply.started": "2024-11-06T00:51:02.841062Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pmc_id</th>\n",
       "      <th>findings</th>\n",
       "      <th>impression</th>\n",
       "      <th>comparison</th>\n",
       "      <th>indication</th>\n",
       "      <th>image_count</th>\n",
       "      <th>image_1</th>\n",
       "      <th>image_2</th>\n",
       "      <th>image_3</th>\n",
       "      <th>image_4</th>\n",
       "      <th>image_5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3185</th>\n",
       "      <td>755</td>\n",
       "      <td>none</td>\n",
       "      <td>Lungs clear. Heart size normal. Flowing syndes...</td>\n",
       "      <td>XXXX, XXXX</td>\n",
       "      <td>heart murmur?</td>\n",
       "      <td>2</td>\n",
       "      <td>CXR755_IM-2306-1001.jpg: PA and lateral views ...</td>\n",
       "      <td>CXR755_IM-2306-2001.jpg: PA and lateral views ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3187</th>\n",
       "      <td>2093</td>\n",
       "      <td>lungs clear cardiomediastinal silhouette withi...</td>\n",
       "      <td>No focal lung infiltrates.</td>\n",
       "      <td>CT chest XXXX</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>CXR2093_IM-0723-1001.jpg:  PA and lateral view...</td>\n",
       "      <td>CXR2093_IM-0723-2001.jpg:  PA and lateral view...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111</th>\n",
       "      <td>1464</td>\n",
       "      <td>heart pulmonary mediastinum within normal limi...</td>\n",
       "      <td>No acute cardiopulmonary disease. Evidence of ...</td>\n",
       "      <td>None.</td>\n",
       "      <td>XXXX-year-old with initiation of XXXX medicati...</td>\n",
       "      <td>2</td>\n",
       "      <td>CXR1464_IM-0301-1001.jpg: Xray Chest PA and La...</td>\n",
       "      <td>CXR1464_IM-0301-2001.jpg: Xray Chest PA and La...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2320</th>\n",
       "      <td>2419</td>\n",
       "      <td>none</td>\n",
       "      <td>Normal heart size. Normal pulmonary vasculatur...</td>\n",
       "      <td>XXXX.</td>\n",
       "      <td>PNUEMONIA exsmoker x 10 yrs hx XXXX onset asth...</td>\n",
       "      <td>2</td>\n",
       "      <td>CXR2419_IM-0963-1001.jpg: Xray Chest PA and La...</td>\n",
       "      <td>CXR2419_IM-0963-2001.jpg: Xray Chest PA and La...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1089</th>\n",
       "      <td>1233</td>\n",
       "      <td>enlarged cardiomediastinal silhouette low lung...</td>\n",
       "      <td>Cardiomegaly without heart failure. Minimal XX...</td>\n",
       "      <td>XXXX, XXXX.</td>\n",
       "      <td>XXXX-year-old. Chest pain.</td>\n",
       "      <td>2</td>\n",
       "      <td>CXR1233_IM-0157-1001.jpg:  PA and lateral views.</td>\n",
       "      <td>CXR1233_IM-0157-2001.jpg:  PA and lateral views.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      pmc_id                                           findings  \\\n",
       "3185     755                                               none   \n",
       "3187    2093  lungs clear cardiomediastinal silhouette withi...   \n",
       "111     1464  heart pulmonary mediastinum within normal limi...   \n",
       "2320    2419                                               none   \n",
       "1089    1233  enlarged cardiomediastinal silhouette low lung...   \n",
       "\n",
       "                                             impression     comparison  \\\n",
       "3185  Lungs clear. Heart size normal. Flowing syndes...     XXXX, XXXX   \n",
       "3187                         No focal lung infiltrates.  CT chest XXXX   \n",
       "111   No acute cardiopulmonary disease. Evidence of ...          None.   \n",
       "2320  Normal heart size. Normal pulmonary vasculatur...          XXXX.   \n",
       "1089  Cardiomegaly without heart failure. Minimal XX...    XXXX, XXXX.   \n",
       "\n",
       "                                             indication  image_count  \\\n",
       "3185                                      heart murmur?            2   \n",
       "3187                                                NaN            2   \n",
       "111   XXXX-year-old with initiation of XXXX medicati...            2   \n",
       "2320  PNUEMONIA exsmoker x 10 yrs hx XXXX onset asth...            2   \n",
       "1089                         XXXX-year-old. Chest pain.            2   \n",
       "\n",
       "                                                image_1  \\\n",
       "3185  CXR755_IM-2306-1001.jpg: PA and lateral views ...   \n",
       "3187  CXR2093_IM-0723-1001.jpg:  PA and lateral view...   \n",
       "111   CXR1464_IM-0301-1001.jpg: Xray Chest PA and La...   \n",
       "2320  CXR2419_IM-0963-1001.jpg: Xray Chest PA and La...   \n",
       "1089  CXR1233_IM-0157-1001.jpg:  PA and lateral views.    \n",
       "\n",
       "                                                image_2 image_3 image_4  \\\n",
       "3185  CXR755_IM-2306-2001.jpg: PA and lateral views ...     NaN     NaN   \n",
       "3187  CXR2093_IM-0723-2001.jpg:  PA and lateral view...     NaN     NaN   \n",
       "111   CXR1464_IM-0301-2001.jpg: Xray Chest PA and La...     NaN     NaN   \n",
       "2320  CXR2419_IM-0963-2001.jpg: Xray Chest PA and La...     NaN     NaN   \n",
       "1089  CXR1233_IM-0157-2001.jpg:  PA and lateral views.      NaN     NaN   \n",
       "\n",
       "     image_5  \n",
       "3185     NaN  \n",
       "3187     NaN  \n",
       "111      NaN  \n",
       "2320     NaN  \n",
       "1089     NaN  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data saved to: ./datasets/iu_xray/output/train_data.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pmc_id</th>\n",
       "      <th>findings</th>\n",
       "      <th>impression</th>\n",
       "      <th>comparison</th>\n",
       "      <th>indication</th>\n",
       "      <th>image_count</th>\n",
       "      <th>image_1</th>\n",
       "      <th>image_2</th>\n",
       "      <th>image_3</th>\n",
       "      <th>image_4</th>\n",
       "      <th>image_5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>299</th>\n",
       "      <td>3070</td>\n",
       "      <td>heart size within normal limits focal airspace...</td>\n",
       "      <td>No acute cardiopulmonary findings.</td>\n",
       "      <td>None available.</td>\n",
       "      <td>XXXX-year-old male, reactive PPD.</td>\n",
       "      <td>2</td>\n",
       "      <td>CXR3070_IM-1432-1001.jpg: PA and lateral views...</td>\n",
       "      <td>CXR3070_IM-1432-1002.jpg: PA and lateral views...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3900</th>\n",
       "      <td>345</td>\n",
       "      <td>cardiomediastinal silhouette pulmonary muscula...</td>\n",
       "      <td>No acute cardiopulmonary findings.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Costochondral chest pain</td>\n",
       "      <td>2</td>\n",
       "      <td>CXR345_IM-1672-1001.jpg: Chest, 2 views, XXXX ...</td>\n",
       "      <td>CXR345_IM-1672-2001.jpg: Chest, 2 views, XXXX ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>442</th>\n",
       "      <td>1137</td>\n",
       "      <td>none</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>CXR1137_IM-0093-12012.jpg: Xray Chest PA and L...</td>\n",
       "      <td>CXR1137_IM-0093-4004.jpg: Xray Chest PA and La...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3406</th>\n",
       "      <td>935</td>\n",
       "      <td>none</td>\n",
       "      <td>Comparison XXXX, XXXX. Well-expanded and clear...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>found unresponsive,</td>\n",
       "      <td>2</td>\n",
       "      <td>CXR935_IM-2432-1001.jpg: Xray Chest PA and Lat...</td>\n",
       "      <td>CXR935_IM-2432-2001.jpg: Xray Chest PA and Lat...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3023</th>\n",
       "      <td>303</td>\n",
       "      <td>heart normal size mediastinum stable rectal ba...</td>\n",
       "      <td>Mild bilateral streaky opacities, XXXX atelect...</td>\n",
       "      <td>XXXX</td>\n",
       "      <td>XXXX, asthma, preop hip replacement</td>\n",
       "      <td>1</td>\n",
       "      <td>CXR303_IM-1404-1001.jpg: CHEST 2V FRONTAL/LATE...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      pmc_id                                           findings  \\\n",
       "299     3070  heart size within normal limits focal airspace...   \n",
       "3900     345  cardiomediastinal silhouette pulmonary muscula...   \n",
       "442     1137                                               none   \n",
       "3406     935                                               none   \n",
       "3023     303  heart normal size mediastinum stable rectal ba...   \n",
       "\n",
       "                                             impression       comparison  \\\n",
       "299                  No acute cardiopulmonary findings.  None available.   \n",
       "3900                 No acute cardiopulmonary findings.              NaN   \n",
       "442                                                 NaN              NaN   \n",
       "3406  Comparison XXXX, XXXX. Well-expanded and clear...              NaN   \n",
       "3023  Mild bilateral streaky opacities, XXXX atelect...             XXXX   \n",
       "\n",
       "                               indication  image_count  \\\n",
       "299     XXXX-year-old male, reactive PPD.            2   \n",
       "3900             Costochondral chest pain            2   \n",
       "442                                   NaN            2   \n",
       "3406                  found unresponsive,            2   \n",
       "3023  XXXX, asthma, preop hip replacement            1   \n",
       "\n",
       "                                                image_1  \\\n",
       "299   CXR3070_IM-1432-1001.jpg: PA and lateral views...   \n",
       "3900  CXR345_IM-1672-1001.jpg: Chest, 2 views, XXXX ...   \n",
       "442   CXR1137_IM-0093-12012.jpg: Xray Chest PA and L...   \n",
       "3406  CXR935_IM-2432-1001.jpg: Xray Chest PA and Lat...   \n",
       "3023  CXR303_IM-1404-1001.jpg: CHEST 2V FRONTAL/LATE...   \n",
       "\n",
       "                                                image_2 image_3 image_4  \\\n",
       "299   CXR3070_IM-1432-1002.jpg: PA and lateral views...     NaN     NaN   \n",
       "3900  CXR345_IM-1672-2001.jpg: Chest, 2 views, XXXX ...     NaN     NaN   \n",
       "442   CXR1137_IM-0093-4004.jpg: Xray Chest PA and La...     NaN     NaN   \n",
       "3406  CXR935_IM-2432-2001.jpg: Xray Chest PA and Lat...     NaN     NaN   \n",
       "3023                                                NaN     NaN     NaN   \n",
       "\n",
       "     image_5  \n",
       "299      NaN  \n",
       "3900     NaN  \n",
       "442      NaN  \n",
       "3406     NaN  \n",
       "3023     NaN  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation data saved to: ./datasets/iu_xray/output/val_data.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pmc_id</th>\n",
       "      <th>findings</th>\n",
       "      <th>impression</th>\n",
       "      <th>comparison</th>\n",
       "      <th>indication</th>\n",
       "      <th>image_count</th>\n",
       "      <th>image_1</th>\n",
       "      <th>image_2</th>\n",
       "      <th>image_3</th>\n",
       "      <th>image_4</th>\n",
       "      <th>image_5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>134</th>\n",
       "      <td>3790</td>\n",
       "      <td>low lung volumes elevation right hemidiaphragm...</td>\n",
       "      <td>Right lower lobe airspace disease. .</td>\n",
       "      <td>PA and lateral views of the chest dated XXXX. ...</td>\n",
       "      <td>XXXX-year-old female, pain, seen on XXXX for r...</td>\n",
       "      <td>2</td>\n",
       "      <td>CXR3790_IM-1904-0001-0001.jpg: Xray Chest PA a...</td>\n",
       "      <td>CXR3790_IM-1904-0001-0002.jpg: Xray Chest PA a...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3415</th>\n",
       "      <td>2282</td>\n",
       "      <td>lungs clear bilaterally specifically evidence ...</td>\n",
       "      <td>No acute cardiopulmonary abnormality..</td>\n",
       "      <td>Two-view chest radiograph dated XXXX, XXXX..</td>\n",
       "      <td>XXXX-year-old male, XXXX 2 XXXX ago, rib pain..</td>\n",
       "      <td>2</td>\n",
       "      <td>CXR2282_IM-0869-1001.jpg: PA and lateral chest...</td>\n",
       "      <td>CXR2282_IM-0869-2001.jpg: PA and lateral chest...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>921</th>\n",
       "      <td>2841</td>\n",
       "      <td>heart normal size contour lungs clear without ...</td>\n",
       "      <td>No acute cardiopulmonary disease.</td>\n",
       "      <td>None.</td>\n",
       "      <td>XXXX year old mid to lower back pain since XXXX.</td>\n",
       "      <td>1</td>\n",
       "      <td>CXR2841_IM-1253-2001.jpg: Xray Chest PA and La...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1029</th>\n",
       "      <td>2192</td>\n",
       "      <td>focal lung consolidation pneumothorax pleural ...</td>\n",
       "      <td>No acute cardiopulmonary process.</td>\n",
       "      <td>XXXX performed XXXX/XXXX</td>\n",
       "      <td>XXXX-year-old with XXXX, history of lung nodules.</td>\n",
       "      <td>2</td>\n",
       "      <td>CXR2192_IM-0802-2002.jpg: Xray Chest PA and La...</td>\n",
       "      <td>CXR2192_IM-0802-3003.jpg: Xray Chest PA and La...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2509</th>\n",
       "      <td>3149</td>\n",
       "      <td>lungs hyperexpanded cardiomediastinal silhouet...</td>\n",
       "      <td>Lung hyperexpansion. No focal air space disease.</td>\n",
       "      <td>XXXX, XXXX.</td>\n",
       "      <td>XXXX-year-old male with XXXX and asthma.</td>\n",
       "      <td>2</td>\n",
       "      <td>CXR3149_IM-1480-1001.jpg: PA and Lateral views...</td>\n",
       "      <td>CXR3149_IM-1480-2001.jpg: PA and Lateral views...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      pmc_id                                           findings  \\\n",
       "134     3790  low lung volumes elevation right hemidiaphragm...   \n",
       "3415    2282  lungs clear bilaterally specifically evidence ...   \n",
       "921     2841  heart normal size contour lungs clear without ...   \n",
       "1029    2192  focal lung consolidation pneumothorax pleural ...   \n",
       "2509    3149  lungs hyperexpanded cardiomediastinal silhouet...   \n",
       "\n",
       "                                            impression  \\\n",
       "134               Right lower lobe airspace disease. .   \n",
       "3415            No acute cardiopulmonary abnormality..   \n",
       "921                  No acute cardiopulmonary disease.   \n",
       "1029                 No acute cardiopulmonary process.   \n",
       "2509  Lung hyperexpansion. No focal air space disease.   \n",
       "\n",
       "                                             comparison  \\\n",
       "134   PA and lateral views of the chest dated XXXX. ...   \n",
       "3415       Two-view chest radiograph dated XXXX, XXXX..   \n",
       "921                                               None.   \n",
       "1029                           XXXX performed XXXX/XXXX   \n",
       "2509                                        XXXX, XXXX.   \n",
       "\n",
       "                                             indication  image_count  \\\n",
       "134   XXXX-year-old female, pain, seen on XXXX for r...            2   \n",
       "3415    XXXX-year-old male, XXXX 2 XXXX ago, rib pain..            2   \n",
       "921    XXXX year old mid to lower back pain since XXXX.            1   \n",
       "1029  XXXX-year-old with XXXX, history of lung nodules.            2   \n",
       "2509           XXXX-year-old male with XXXX and asthma.            2   \n",
       "\n",
       "                                                image_1  \\\n",
       "134   CXR3790_IM-1904-0001-0001.jpg: Xray Chest PA a...   \n",
       "3415  CXR2282_IM-0869-1001.jpg: PA and lateral chest...   \n",
       "921   CXR2841_IM-1253-2001.jpg: Xray Chest PA and La...   \n",
       "1029  CXR2192_IM-0802-2002.jpg: Xray Chest PA and La...   \n",
       "2509  CXR3149_IM-1480-1001.jpg: PA and Lateral views...   \n",
       "\n",
       "                                                image_2 image_3 image_4  \\\n",
       "134   CXR3790_IM-1904-0001-0002.jpg: Xray Chest PA a...     NaN     NaN   \n",
       "3415  CXR2282_IM-0869-2001.jpg: PA and lateral chest...     NaN     NaN   \n",
       "921                                                 NaN     NaN     NaN   \n",
       "1029  CXR2192_IM-0802-3003.jpg: Xray Chest PA and La...     NaN     NaN   \n",
       "2509  CXR3149_IM-1480-2001.jpg: PA and Lateral views...     NaN     NaN   \n",
       "\n",
       "     image_5  \n",
       "134      NaN  \n",
       "3415     NaN  \n",
       "921      NaN  \n",
       "1029     NaN  \n",
       "2509     NaN  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test data saved to: ./datasets/iu_xray/output/test_data.csv\n"
     ]
    }
   ],
   "source": [
    "'''Preprocessing Text - Lowercasing, Decontracting, Punctuation Removal, Number Removal, Two-Letter Word Removal, Stop Word Removal, Spell Checking, Extra Space Removal'''\n",
    "\n",
    "# load your DataFrame (replace with actual path)\n",
    "iu_xray_reports_df = os.path.join(output_directory, 'iu_xray_reports_df.csv')\n",
    "df = pd.read_csv(iu_xray_reports_df)\n",
    "\n",
    "\n",
    "# apply preprocessing on specific columns if they exist\n",
    "preprocess_columns = ['findings']\n",
    "for column in preprocess_columns:\n",
    "    if column in df.columns:\n",
    "        print(f\"Preprocessing Column: {column}\")\n",
    "        df[column] = df[column].fillna('none').astype(str)\n",
    "        df[column] = preprocess_text(df[column])\n",
    "        output_path = os.path.join(output_directory, f'preprocessed_{column}.csv')\n",
    "        df.to_csv(output_path, index=False)\n",
    "        print(f\"Saved preprocessed '{column}' column to: {output_path}\")\n",
    "        \n",
    "        \n",
    "# split into Train/Validation/Test (70%/10%/20%)\n",
    "output_path = os.path.join(output_directory, f'preprocessed_findings.csv')\n",
    "df = pd.read_csv(output_path)        \n",
    "train_df, temp_df = train_test_split(df, test_size=0.3, random_state=42)\n",
    "val_df, test_df = train_test_split(temp_df, test_size=(2/3), random_state=42)\n",
    "\n",
    "\n",
    "# save the splits\n",
    "train_path = os.path.join(output_directory, 'train_data.csv')\n",
    "train_df.to_csv(train_path, index=False)\n",
    "display(train_df.head())\n",
    "print(f\"Train data saved to: {train_path}\")\n",
    "\n",
    "val_path = os.path.join(output_directory, 'val_data.csv')\n",
    "val_df.to_csv(val_path, index=False)\n",
    "display(val_df.head())\n",
    "print(f\"Validation data saved to: {val_path}\")\n",
    "\n",
    "test_path = os.path.join(output_directory, 'test_data.csv')\n",
    "test_df.to_csv(test_path, index=False)\n",
    "display(test_df.head())\n",
    "print(f\"Test data saved to: {test_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-06T00:55:05.663136Z",
     "iopub.status.busy": "2024-11-06T00:55:05.662277Z",
     "iopub.status.idle": "2024-11-06T00:55:05.740096Z",
     "shell.execute_reply": "2024-11-06T00:55:05.739119Z",
     "shell.execute_reply.started": "2024-11-06T00:55:05.663090Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the DataFrame: (2768, 5)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pmc_id</th>\n",
       "      <th>findings</th>\n",
       "      <th>image_1</th>\n",
       "      <th>image_2</th>\n",
       "      <th>index</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>755</td>\n",
       "      <td>none</td>\n",
       "      <td>CXR755_IM-2306-1001.jpg: PA and lateral views ...</td>\n",
       "      <td>CXR755_IM-2306-2001.jpg: PA and lateral views ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2093</td>\n",
       "      <td>lungs clear cardiomediastinal silhouette withi...</td>\n",
       "      <td>CXR2093_IM-0723-1001.jpg:  PA and lateral view...</td>\n",
       "      <td>CXR2093_IM-0723-2001.jpg:  PA and lateral view...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1464</td>\n",
       "      <td>heart pulmonary mediastinum within normal limi...</td>\n",
       "      <td>CXR1464_IM-0301-1001.jpg: Xray Chest PA and La...</td>\n",
       "      <td>CXR1464_IM-0301-2001.jpg: Xray Chest PA and La...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2419</td>\n",
       "      <td>none</td>\n",
       "      <td>CXR2419_IM-0963-1001.jpg: Xray Chest PA and La...</td>\n",
       "      <td>CXR2419_IM-0963-2001.jpg: Xray Chest PA and La...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1233</td>\n",
       "      <td>enlarged cardiomediastinal silhouette low lung...</td>\n",
       "      <td>CXR1233_IM-0157-1001.jpg:  PA and lateral views.</td>\n",
       "      <td>CXR1233_IM-0157-2001.jpg:  PA and lateral views.</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>847</td>\n",
       "      <td>trachea midline heart slightly large low lung ...</td>\n",
       "      <td>CXR847_IM-2369-1001.jpg: CHEST 2V FRONTAL/LATE...</td>\n",
       "      <td>CXR847_IM-2369-1002.jpg: CHEST 2V FRONTAL/LATE...</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2914</td>\n",
       "      <td>small area scarring lateral right mid lower lu...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2637</td>\n",
       "      <td>moderate marked enlargement cardiac silhouette...</td>\n",
       "      <td>CXR2637_IM-1122-1001.jpg: Xray Chest PA and La...</td>\n",
       "      <td>CXR2637_IM-1122-2001.jpg: Xray Chest PA and La...</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2197</td>\n",
       "      <td>heart size normal parities appear right fissur...</td>\n",
       "      <td>CXR2197_IM-0807-1001.jpg: Chest x-XXXX, 2 view...</td>\n",
       "      <td>CXR2197_IM-0807-2001.jpg: Chest x-XXXX, 2 view...</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2532</td>\n",
       "      <td>stable enlargement cardiac silhouette lateral ...</td>\n",
       "      <td>CXR2532_IM-1046-1001.jpg: Xray Chest PA and La...</td>\n",
       "      <td>CXR2532_IM-1046-2001.jpg: Xray Chest PA and La...</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>3658</td>\n",
       "      <td>cardiomediastinal silhouette normal size conto...</td>\n",
       "      <td>CXR3658_IM-1819-1001.jpg: Xray Chest PA and La...</td>\n",
       "      <td>CXR3658_IM-1819-2001.jpg: Xray Chest PA and La...</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1449</td>\n",
       "      <td>cardiac silhouette enlarged comparison studies...</td>\n",
       "      <td>CXR1449_IM-0290-1001.jpg: Xray Chest PA and La...</td>\n",
       "      <td>CXR1449_IM-0290-2001.jpg: Xray Chest PA and La...</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1929</td>\n",
       "      <td>fracture deformity proximal right humerus hype...</td>\n",
       "      <td>CXR1929_IM-0600-1001.jpg: Xray Chest PA and La...</td>\n",
       "      <td>CXR1929_IM-0600-2001.jpg: Xray Chest PA and La...</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2030</td>\n",
       "      <td>hyperinflation lungs due small calcification s...</td>\n",
       "      <td>CXR2030_IM-0675-1001.jpg: Xray Chest PA and La...</td>\n",
       "      <td>CXR2030_IM-0675-2001.jpg: Xray Chest PA and La...</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1258</td>\n",
       "      <td>heart lungs interval lungs clear expanded hear...</td>\n",
       "      <td>CXR1258_IM-0175-1001.jpg: CHEST 2V FRONTAL/LAT...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>1519</td>\n",
       "      <td>lungs mildly hypoinflated asymmetric elevation...</td>\n",
       "      <td>CXR1519_IM-0335-1001.jpg: Xray Chest PA and La...</td>\n",
       "      <td>CXR1519_IM-0335-4004.jpg: Xray Chest PA and La...</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>3099</td>\n",
       "      <td>lungs clear without focal consolidation effusi...</td>\n",
       "      <td>CXR3099_IM-1450-1001.jpg: Chest x-XXXX XXXX an...</td>\n",
       "      <td>CXR3099_IM-1450-2001.jpg: Chest x-XXXX XXXX an...</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>1195</td>\n",
       "      <td>coronary artery stets visualized cardiomediast...</td>\n",
       "      <td>CXR1195_IM-0131-1001.jpg: PA and lateral chest...</td>\n",
       "      <td>CXR1195_IM-0131-2001.jpg: PA and lateral chest...</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>2388</td>\n",
       "      <td>heart size normal lungs clear normal pneumonia...</td>\n",
       "      <td>CXR2388_IM-0944-1001.jpg:  PA and lateral ches...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>3328</td>\n",
       "      <td>none</td>\n",
       "      <td>CXR3328_IM-1594-1001.jpg: PA lateral chest XXX...</td>\n",
       "      <td>CXR3328_IM-1594-1002.jpg: PA lateral chest XXX...</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    pmc_id                                           findings  \\\n",
       "0      755                                               none   \n",
       "1     2093  lungs clear cardiomediastinal silhouette withi...   \n",
       "2     1464  heart pulmonary mediastinum within normal limi...   \n",
       "3     2419                                               none   \n",
       "4     1233  enlarged cardiomediastinal silhouette low lung...   \n",
       "5      847  trachea midline heart slightly large low lung ...   \n",
       "6     2914  small area scarring lateral right mid lower lu...   \n",
       "7     2637  moderate marked enlargement cardiac silhouette...   \n",
       "8     2197  heart size normal parities appear right fissur...   \n",
       "9     2532  stable enlargement cardiac silhouette lateral ...   \n",
       "10    3658  cardiomediastinal silhouette normal size conto...   \n",
       "11    1449  cardiac silhouette enlarged comparison studies...   \n",
       "12    1929  fracture deformity proximal right humerus hype...   \n",
       "13    2030  hyperinflation lungs due small calcification s...   \n",
       "14    1258  heart lungs interval lungs clear expanded hear...   \n",
       "15    1519  lungs mildly hypoinflated asymmetric elevation...   \n",
       "16    3099  lungs clear without focal consolidation effusi...   \n",
       "17    1195  coronary artery stets visualized cardiomediast...   \n",
       "18    2388  heart size normal lungs clear normal pneumonia...   \n",
       "19    3328                                               none   \n",
       "\n",
       "                                              image_1  \\\n",
       "0   CXR755_IM-2306-1001.jpg: PA and lateral views ...   \n",
       "1   CXR2093_IM-0723-1001.jpg:  PA and lateral view...   \n",
       "2   CXR1464_IM-0301-1001.jpg: Xray Chest PA and La...   \n",
       "3   CXR2419_IM-0963-1001.jpg: Xray Chest PA and La...   \n",
       "4   CXR1233_IM-0157-1001.jpg:  PA and lateral views.    \n",
       "5   CXR847_IM-2369-1001.jpg: CHEST 2V FRONTAL/LATE...   \n",
       "6                                                 NaN   \n",
       "7   CXR2637_IM-1122-1001.jpg: Xray Chest PA and La...   \n",
       "8   CXR2197_IM-0807-1001.jpg: Chest x-XXXX, 2 view...   \n",
       "9   CXR2532_IM-1046-1001.jpg: Xray Chest PA and La...   \n",
       "10  CXR3658_IM-1819-1001.jpg: Xray Chest PA and La...   \n",
       "11  CXR1449_IM-0290-1001.jpg: Xray Chest PA and La...   \n",
       "12  CXR1929_IM-0600-1001.jpg: Xray Chest PA and La...   \n",
       "13  CXR2030_IM-0675-1001.jpg: Xray Chest PA and La...   \n",
       "14  CXR1258_IM-0175-1001.jpg: CHEST 2V FRONTAL/LAT...   \n",
       "15  CXR1519_IM-0335-1001.jpg: Xray Chest PA and La...   \n",
       "16  CXR3099_IM-1450-1001.jpg: Chest x-XXXX XXXX an...   \n",
       "17  CXR1195_IM-0131-1001.jpg: PA and lateral chest...   \n",
       "18  CXR2388_IM-0944-1001.jpg:  PA and lateral ches...   \n",
       "19  CXR3328_IM-1594-1001.jpg: PA lateral chest XXX...   \n",
       "\n",
       "                                              image_2  index  \n",
       "0   CXR755_IM-2306-2001.jpg: PA and lateral views ...      1  \n",
       "1   CXR2093_IM-0723-2001.jpg:  PA and lateral view...      2  \n",
       "2   CXR1464_IM-0301-2001.jpg: Xray Chest PA and La...      3  \n",
       "3   CXR2419_IM-0963-2001.jpg: Xray Chest PA and La...      4  \n",
       "4   CXR1233_IM-0157-2001.jpg:  PA and lateral views.       5  \n",
       "5   CXR847_IM-2369-1002.jpg: CHEST 2V FRONTAL/LATE...      6  \n",
       "6                                                 NaN      7  \n",
       "7   CXR2637_IM-1122-2001.jpg: Xray Chest PA and La...      8  \n",
       "8   CXR2197_IM-0807-2001.jpg: Chest x-XXXX, 2 view...      9  \n",
       "9   CXR2532_IM-1046-2001.jpg: Xray Chest PA and La...     10  \n",
       "10  CXR3658_IM-1819-2001.jpg: Xray Chest PA and La...     11  \n",
       "11  CXR1449_IM-0290-2001.jpg: Xray Chest PA and La...     12  \n",
       "12  CXR1929_IM-0600-2001.jpg: Xray Chest PA and La...     13  \n",
       "13  CXR2030_IM-0675-2001.jpg: Xray Chest PA and La...     14  \n",
       "14                                                NaN     15  \n",
       "15  CXR1519_IM-0335-4004.jpg: Xray Chest PA and La...     16  \n",
       "16  CXR3099_IM-1450-2001.jpg: Chest x-XXXX XXXX an...     17  \n",
       "17  CXR1195_IM-0131-2001.jpg: PA and lateral chest...     18  \n",
       "18                                                NaN     19  \n",
       "19  CXR3328_IM-1594-1002.jpg: PA lateral chest XXX...     20  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data saved to: ./datasets/iu_xray/output/train_data.csv\n"
     ]
    }
   ],
   "source": [
    "'''Indexing the Training Data'''\n",
    "\n",
    "# indexing the train data\n",
    "df = pd.read_csv(train_path)     \n",
    "df = df[['pmc_id', 'findings', 'image_1', 'image_2']]\n",
    "df['index'] = range(1, len(df) + 1)\n",
    "\n",
    "\n",
    "# display the modified DataFrame to check the output\n",
    "print(\"Shape of the DataFrame:\", df.shape)\n",
    "display(df.head(20))\n",
    "df.to_csv(train_path, index=False)\n",
    "print(f\"Train data saved to: {train_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-06T00:55:36.893311Z",
     "iopub.status.busy": "2024-11-06T00:55:36.892933Z",
     "iopub.status.idle": "2024-11-06T00:55:37.000919Z",
     "shell.execute_reply": "2024-11-06T00:55:36.999896Z",
     "shell.execute_reply.started": "2024-11-06T00:55:36.893276Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the DataFrame Before: (2768, 5)\n",
      "Shape of the DataFrame After: (2700, 5)\n",
      "Dataframe saved to: ./datasets/iu_xray/output/filtered_train_data.csv\n",
      "Shape of the DataFrame Before: (395, 11)\n",
      "Shape of the DataFrame After: (385, 11)\n",
      "Dataframe saved to: ./datasets/iu_xray/output/filtered_val_data.csv\n",
      "Shape of the DataFrame Before: (792, 11)\n",
      "Shape of the DataFrame After: (766, 11)\n",
      "Dataframe saved to: ./datasets/iu_xray/output/filtered_test_data.csv\n"
     ]
    }
   ],
   "source": [
    "'''Creating Filtered Dataframes'''\n",
    "\n",
    "# filtering the data\n",
    "filtered_train_path = os.path.join(output_directory, 'filtered_train_data.csv')\n",
    "df = pd.read_csv(train_path)  \n",
    "print(\"Shape of the DataFrame Before:\", df.shape)\n",
    "\n",
    "filtered_df = df.dropna(subset=['image_1', 'image_2'], how='all')\n",
    "print(\"Shape of the DataFrame After:\", filtered_df.shape)\n",
    "\n",
    "filtered_df.to_csv(filtered_train_path, index=False)\n",
    "print(f\"Dataframe saved to: {filtered_train_path}\")\n",
    "\n",
    "\n",
    "# filtering the data\n",
    "filtered_val_path = os.path.join(output_directory, 'filtered_val_data.csv')\n",
    "df = pd.read_csv(val_path)  \n",
    "print(\"Shape of the DataFrame Before:\", df.shape)\n",
    "\n",
    "filtered_df = df.dropna(subset=['image_1', 'image_2'], how='all')\n",
    "print(\"Shape of the DataFrame After:\", filtered_df.shape)\n",
    "\n",
    "filtered_df.to_csv(filtered_val_path, index=False)\n",
    "print(f\"Dataframe saved to: {filtered_val_path}\")\n",
    "\n",
    "\n",
    "# filtering the data\n",
    "filtered_test_path = os.path.join(output_directory, 'filtered_test_data.csv')\n",
    "df = pd.read_csv(test_path)  \n",
    "print(\"Shape of the DataFrame Before:\", df.shape)\n",
    "\n",
    "filtered_df = df.dropna(subset=['image_1', 'image_2'], how='all')\n",
    "print(\"Shape of the DataFrame After:\", filtered_df.shape)\n",
    "\n",
    "filtered_df.to_csv(filtered_test_path, index=False)\n",
    "print(f\"Dataframe saved to: {filtered_test_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "M8sW8uz8-plE"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/shivangi/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/shivangi/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/shivangi/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing Text of DataFrame ./datasets/iu_xray/output/iu_xray_reports_df.csv to: ./datasets/iu_xray/output/iu_xray_reports_preprocessed_sorted_df.csv\n",
      "Preprocessing Column: comparison\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|                                     | 283/3955 [00:23<05:00, 12.23it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 116\u001b[0m\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPreprocessing Column: comparison\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    115\u001b[0m iu_xray_reports_preprocessed_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcomparison\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m iu_xray_reports_preprocessed_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcomparison\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mfillna(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnone\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mstr\u001b[39m)\n\u001b[0;32m--> 116\u001b[0m iu_xray_reports_preprocessed_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcomparison\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mpreprocess_text\u001b[49m\u001b[43m(\u001b[49m\u001b[43miu_xray_reports_preprocessed_df\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcomparison\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    117\u001b[0m iu_xray_reports_preprocessed_df\u001b[38;5;241m.\u001b[39mto_csv(iu_xray_reports_preprocessed_df_path, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSaved preprocessed \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcomparison\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m column to: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00miu_xray_reports_preprocessed_df_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[13], line 82\u001b[0m, in \u001b[0;36mpreprocess_text\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m     80\u001b[0m sentence \u001b[38;5;241m=\u001b[39m rem_two_letter_words(sentence)\n\u001b[1;32m     81\u001b[0m sentence \u001b[38;5;241m=\u001b[39m rem_stop_words(sentence)\n\u001b[0;32m---> 82\u001b[0m sentence \u001b[38;5;241m=\u001b[39m \u001b[43mcorrect_spelling\u001b[49m\u001b[43m(\u001b[49m\u001b[43msentence\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     83\u001b[0m sentence \u001b[38;5;241m=\u001b[39m rem_extra_spaces(sentence)\n\u001b[1;32m     85\u001b[0m preprocessed\u001b[38;5;241m.\u001b[39mappend(sentence)\n",
      "Cell \u001b[0;32mIn[13], line 61\u001b[0m, in \u001b[0;36mcorrect_spelling\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m     59\u001b[0m corrected \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     60\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m text\u001b[38;5;241m.\u001b[39msplit():\n\u001b[0;32m---> 61\u001b[0m     corrected_word \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(spell\u001b[38;5;241m.\u001b[39mcandidates(word))[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mspell\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcandidates\u001b[49m\u001b[43m(\u001b[49m\u001b[43mword\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01melse\u001b[39;00m word\n\u001b[1;32m     62\u001b[0m     corrected\u001b[38;5;241m.\u001b[39mappend(corrected_word)\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(corrected)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/spellchecker/spellchecker.py:185\u001b[0m, in \u001b[0;36mSpellChecker.candidates\u001b[0;34m(self, word)\u001b[0m\n\u001b[1;32m    183\u001b[0m \u001b[38;5;66;03m# if still not found, use the edit distance 1 to calc edit distance 2\u001b[39;00m\n\u001b[1;32m    184\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_distance \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[0;32m--> 185\u001b[0m     tmp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mknown(\u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__edit_distance_alt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mres\u001b[49m\u001b[43m)\u001b[49m))\n\u001b[1;32m    186\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m tmp:\n\u001b[1;32m    187\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m tmp\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/spellchecker/spellchecker.py:252\u001b[0m, in \u001b[0;36mSpellChecker.__edit_distance_alt\u001b[0;34m(self, words)\u001b[0m\n\u001b[1;32m    250\u001b[0m tmp_words \u001b[38;5;241m=\u001b[39m [ensure_unicode(w) \u001b[38;5;28;01mfor\u001b[39;00m w \u001b[38;5;129;01min\u001b[39;00m words]\n\u001b[1;32m    251\u001b[0m tmp \u001b[38;5;241m=\u001b[39m [w \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_case_sensitive \u001b[38;5;28;01melse\u001b[39;00m w\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;28;01mfor\u001b[39;00m w \u001b[38;5;129;01min\u001b[39;00m tmp_words \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_if_should_check(w)]\n\u001b[0;32m--> 252\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m [e2 \u001b[38;5;28;01mfor\u001b[39;00m e1 \u001b[38;5;129;01min\u001b[39;00m tmp \u001b[38;5;28;01mfor\u001b[39;00m e2 \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mknown(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39medit_distance_1(e1))]\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/spellchecker/spellchecker.py:252\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    250\u001b[0m tmp_words \u001b[38;5;241m=\u001b[39m [ensure_unicode(w) \u001b[38;5;28;01mfor\u001b[39;00m w \u001b[38;5;129;01min\u001b[39;00m words]\n\u001b[1;32m    251\u001b[0m tmp \u001b[38;5;241m=\u001b[39m [w \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_case_sensitive \u001b[38;5;28;01melse\u001b[39;00m w\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;28;01mfor\u001b[39;00m w \u001b[38;5;129;01min\u001b[39;00m tmp_words \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_if_should_check(w)]\n\u001b[0;32m--> 252\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m [e2 \u001b[38;5;28;01mfor\u001b[39;00m e1 \u001b[38;5;129;01min\u001b[39;00m tmp \u001b[38;5;28;01mfor\u001b[39;00m e2 \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mknown\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43medit_distance_1\u001b[49m\u001b[43m(\u001b[49m\u001b[43me1\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m]\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/spellchecker/spellchecker.py:197\u001b[0m, in \u001b[0;36mSpellChecker.known\u001b[0;34m(self, words)\u001b[0m\n\u001b[1;32m    190\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mknown\u001b[39m(\u001b[38;5;28mself\u001b[39m, words: typing\u001b[38;5;241m.\u001b[39mIterable[KeyT]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m typing\u001b[38;5;241m.\u001b[39mSet[\u001b[38;5;28mstr\u001b[39m]:\n\u001b[1;32m    191\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"The subset of `words` that appear in the dictionary of words\u001b[39;00m\n\u001b[1;32m    192\u001b[0m \n\u001b[1;32m    193\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m    194\u001b[0m \u001b[38;5;124;03m        words (list): List of words to determine which are in the corpus\u001b[39;00m\n\u001b[1;32m    195\u001b[0m \u001b[38;5;124;03m    Returns:\u001b[39;00m\n\u001b[1;32m    196\u001b[0m \u001b[38;5;124;03m        set: The set of those words from the input that are in the corpus\"\"\"\u001b[39;00m\n\u001b[0;32m--> 197\u001b[0m     tmp_words \u001b[38;5;241m=\u001b[39m [ensure_unicode(w) \u001b[38;5;28;01mfor\u001b[39;00m w \u001b[38;5;129;01min\u001b[39;00m words]\n\u001b[1;32m    198\u001b[0m     tmp \u001b[38;5;241m=\u001b[39m [w \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_case_sensitive \u001b[38;5;28;01melse\u001b[39;00m w\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;28;01mfor\u001b[39;00m w \u001b[38;5;129;01min\u001b[39;00m tmp_words]\n\u001b[1;32m    199\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m {w \u001b[38;5;28;01mfor\u001b[39;00m w \u001b[38;5;129;01min\u001b[39;00m tmp \u001b[38;5;28;01mif\u001b[39;00m w \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_word_frequency\u001b[38;5;241m.\u001b[39mdictionary \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_if_should_check(w)}\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/spellchecker/spellchecker.py:197\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    190\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mknown\u001b[39m(\u001b[38;5;28mself\u001b[39m, words: typing\u001b[38;5;241m.\u001b[39mIterable[KeyT]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m typing\u001b[38;5;241m.\u001b[39mSet[\u001b[38;5;28mstr\u001b[39m]:\n\u001b[1;32m    191\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"The subset of `words` that appear in the dictionary of words\u001b[39;00m\n\u001b[1;32m    192\u001b[0m \n\u001b[1;32m    193\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m    194\u001b[0m \u001b[38;5;124;03m        words (list): List of words to determine which are in the corpus\u001b[39;00m\n\u001b[1;32m    195\u001b[0m \u001b[38;5;124;03m    Returns:\u001b[39;00m\n\u001b[1;32m    196\u001b[0m \u001b[38;5;124;03m        set: The set of those words from the input that are in the corpus\"\"\"\u001b[39;00m\n\u001b[0;32m--> 197\u001b[0m     tmp_words \u001b[38;5;241m=\u001b[39m [\u001b[43mensure_unicode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mw\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m w \u001b[38;5;129;01min\u001b[39;00m words]\n\u001b[1;32m    198\u001b[0m     tmp \u001b[38;5;241m=\u001b[39m [w \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_case_sensitive \u001b[38;5;28;01melse\u001b[39;00m w\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;28;01mfor\u001b[39;00m w \u001b[38;5;129;01min\u001b[39;00m tmp_words]\n\u001b[1;32m    199\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m {w \u001b[38;5;28;01mfor\u001b[39;00m w \u001b[38;5;129;01min\u001b[39;00m tmp \u001b[38;5;28;01mif\u001b[39;00m w \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_word_frequency\u001b[38;5;241m.\u001b[39mdictionary \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_if_should_check(w)}\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "<<<<<<< REMOTE CELL DELETED >>>>>>>\n",
    "'''Preprocessing Text - Lowercasing, Decontracting, Punctuation Removal, Number Removal, Two-Letter Word Removal, Stop Word Removal, Spell Checking, Extra Space Removal'''\n",
    "\n",
    "# download nltk resources and initialize spell checker\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "spell = SpellChecker()\n",
    "\n",
    "\n",
    "# function to convert text to lowercase\n",
    "def lowercase(text):\n",
    "    return text.lower() if isinstance(text, str) else text\n",
    "\n",
    "\n",
    "# function to decontract words\n",
    "def decontracted(text):\n",
    "    if not isinstance(text, str):\n",
    "        return text\n",
    "    contractions = {\n",
    "        \"won't\": \"will not\", \"can't\": \"can not\", \"couldn't\": \"could not\",\n",
    "        \"shouldn't\": \"should not\", \"wouldn't\": \"would not\", \"n't\": \" not\",\n",
    "        \"'re\": \" are\", \"'s\": \" is\", \"'d\": \" would\", \"'ll\": \" will\",\n",
    "        \"'t\": \" not\", \"'ve\": \" have\", \"'m\": \" am\"\n",
    "    }\n",
    "    for contraction, full_form in contractions.items():\n",
    "        text = text.replace(contraction, full_form)\n",
    "    return text\n",
    "\n",
    "\n",
    "# function to remove punctuations\n",
    "def rem_punctuations(text):\n",
    "    return re.sub(r'[^\\w\\s]', ' ', text) if isinstance(text, str) else text\n",
    "\n",
    "\n",
    "# function to remove numbers\n",
    "def rem_numbers(text):\n",
    "    return re.sub(r'\\d+', ' ', text) if isinstance(text, str) else text\n",
    "\n",
    "\n",
    "# function to remove two-letter words except \"no\" and \"ct\"\n",
    "def rem_two_letter_words(text):\n",
    "    if not isinstance(text, str):\n",
    "        return text\n",
    "    return ' '.join(word for word in text.split() if len(word) > 2 or word in [\"no\", \"ct\"])\n",
    "\n",
    "\n",
    "# function to remove stop words\n",
    "def rem_stop_words(text):\n",
    "    if not isinstance(text, str):\n",
    "        return text\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    return ' '.join(word for word in text.split() if word not in stop_words)\n",
    "\n",
    "\n",
    "# function to correct spelling\n",
    "def correct_spelling(text):\n",
    "    if not isinstance(text, str):\n",
    "        return text\n",
    "    corrected = []\n",
    "    for word in text.split():\n",
    "        corrected_word = list(spell.candidates(word))[0] if spell.candidates(word) else word\n",
    "        corrected.append(corrected_word)\n",
    "    return ' '.join(corrected)\n",
    "\n",
    "\n",
    "# function to remove extra spaces\n",
    "def rem_extra_spaces(text):\n",
    "    return ' '.join(text.split()) if isinstance(text, str) else text\n",
    "\n",
    "\n",
    "# function to preprocess text\n",
    "def preprocess_text(data):\n",
    "    preprocessed = []\n",
    "    for sentence in tqdm(data.values):\n",
    "        sentence = str(sentence)\n",
    "        sentence = lowercase(sentence)\n",
    "        sentence = decontracted(sentence)\n",
    "        sentence = rem_punctuations(sentence)\n",
    "        sentence = rem_numbers(sentence)\n",
    "        sentence = rem_two_letter_words(sentence)\n",
    "        sentence = rem_stop_words(sentence)\n",
    "        sentence = correct_spelling(sentence)\n",
    "        sentence = rem_extra_spaces(sentence)\n",
    "        \n",
    "        preprocessed.append(sentence)\n",
    "\n",
    "    return preprocessed\n",
    "\n",
    "\n",
    "# path to the preprocessed dataframe\n",
    "iu_xray_reports_preprocessed_df_path = os.path.join(output_directory, 'iu_xray_reports_preprocessed_sorted_df.csv')\n",
    "# report_data_path = os.path.join(output_directory,'iu_xray_reports_preprocessed_sorted_df.csv')\n",
    "iu_xray_reports_preprocessed_df = iu_xray_reports_df.copy()\n",
    "\n",
    "\n",
    "# preprocessing text columns in the dataframe\n",
    "if os.path.exists(iu_xray_reports_preprocessed_df_path):\n",
    "    print(f\"Preprocessing Text of DataFrame {iu_xray_reports_df_path} to: {iu_xray_reports_preprocessed_df_path}\")\n",
    "    \n",
    "    preprocess_caption = True\n",
    "    preprocess_comparison = True\n",
    "    preprocess_indication = True\n",
    "    preprocess_findings = True\n",
    "    preprocess_impression = True\n",
    "    \n",
    "    if preprocess_caption and 'caption' in iu_xray_reports_preprocessed_df.columns:\n",
    "        print(\"Preprocessing Column: caption\")\n",
    "        iu_xray_reports_preprocessed_df['caption'] = iu_xray_reports_preprocessed_df['caption'].fillna('unknown').astype(str)\n",
    "        iu_xray_reports_preprocessed_df['caption'] = preprocess_text(iu_xray_reports_preprocessed_df['caption'])\n",
    "        iu_xray_reports_preprocessed_df.to_csv(iu_xray_reports_preprocessed_df_path, index=False)\n",
    "        print(f\"Saved preprocessed 'caption' column to: {iu_xray_reports_preprocessed_df_path}\")\n",
    "    \n",
    "    if preprocess_comparison and 'comparison' in iu_xray_reports_preprocessed_df.columns:\n",
    "        print(\"Preprocessing Column: comparison\")\n",
    "        iu_xray_reports_preprocessed_df['comparison'] = iu_xray_reports_preprocessed_df['comparison'].fillna('none').astype(str)\n",
    "        iu_xray_reports_preprocessed_df['comparison'] = preprocess_text(iu_xray_reports_preprocessed_df['comparison'])\n",
    "        iu_xray_reports_preprocessed_df.to_csv(iu_xray_reports_preprocessed_df_path, index=False)\n",
    "        print(f\"Saved preprocessed 'comparison' column to: {iu_xray_reports_preprocessed_df_path}\")\n",
    "    \n",
    "    if preprocess_indication and 'indication' in iu_xray_reports_preprocessed_df.columns:\n",
    "        print(\"Preprocessing Column: indication\")\n",
    "        iu_xray_reports_preprocessed_df['indication'] = iu_xray_reports_preprocessed_df['indication'].fillna('none').astype(str)\n",
    "        iu_xray_reports_preprocessed_df['indication'] = preprocess_text(iu_xray_reports_preprocessed_df['indication'])\n",
    "        iu_xray_reports_preprocessed_df.to_csv(iu_xray_reports_preprocessed_df_path, index=False)\n",
    "        print(f\"Saved preprocessed 'indication' column to: {iu_xray_reports_preprocessed_df_path}\")\n",
    "    \n",
    "    if preprocess_findings and 'findings' in iu_xray_reports_preprocessed_df.columns:\n",
    "        print(\"Preprocessing Column: findings\")\n",
    "        iu_xray_reports_preprocessed_df['findings'] = iu_xray_reports_preprocessed_df['findings'].fillna('none').astype(str)\n",
    "        iu_xray_reports_preprocessed_df['findings'] = preprocess_text(iu_xray_reports_preprocessed_df['findings'])\n",
    "        iu_xray_reports_preprocessed_df.to_csv(iu_xray_reports_preprocessed_df_path, index=False)\n",
    "        print(f\"Saved preprocessed 'findings' column to: {iu_xray_reports_preprocessed_df_path}\")\n",
    "    \n",
    "    if preprocess_impression and 'impression' in iu_xray_reports_preprocessed_df.columns:\n",
    "        print(\"Preprocessing Column: impression\")\n",
    "        iu_xray_reports_preprocessed_df['impression'] = iu_xray_reports_preprocessed_df['impression'].fillna('none').astype(str)\n",
    "        iu_xray_reports_preprocessed_df['impression'] = preprocess_text(iu_xray_reports_preprocessed_df['impression'])\n",
    "        iu_xray_reports_preprocessed_df.to_csv(iu_xray_reports_preprocessed_df_path, index=False)\n",
    "        print(f\"Saved preprocessed 'impression' column to: {iu_xray_reports_preprocessed_df_path}\")\n",
    "else:\n",
    "    print(f\"Preprocessed Text of DataFrame {iu_xray_reports_df_path} already exists at: {iu_xray_reports_preprocessed_df_path}\")\n",
    "    \n",
    "\n",
    "# displaying the preprocessed dataframe\n",
    "iu_xray_reports_preprocessed_df = pd.read_csv(iu_xray_reports_preprocessed_df_path)\n",
    "display(iu_xray_reports_preprocessed_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/shivangi/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/shivangi/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/shivangi/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   pmc_id                                           findings   \n",
      "0       1  cardiac silhouette mediastinum size within nor...  \\\n",
      "1       2  borderline cardiomegaly midline stereotomy xxx...   \n",
      "2       3                                               none   \n",
      "3       4  diffuse bilateral interstitial alveolar capaci...   \n",
      "4       5  cardiomediastinal silhouette pulmonary muscula...   \n",
      "\n",
      "                                             image_1   \n",
      "0  CXR1_1_IM-0001-3001.jpg: Xray Chest PA and Lat...  \\\n",
      "1  CXR2_IM-0652-1001.jpg: Chest, 2 views, frontal...   \n",
      "2   CXR3_IM-1384-1001.jpg: Xray Chest PA and Lateral   \n",
      "3  CXR4_IM-2050-1001.jpg: PA and lateral views of...   \n",
      "4  CXR5_IM-2117-1003002.jpg: Xray Chest PA and La...   \n",
      "\n",
      "                                             image_2  \n",
      "0  CXR1_1_IM-0001-4001.jpg: Xray Chest PA and Lat...  \n",
      "1  CXR2_IM-0652-2001.jpg: Chest, 2 views, frontal...  \n",
      "2   CXR3_IM-1384-2001.jpg: Xray Chest PA and Lateral  \n",
      "3  CXR4_IM-2050-2001.jpg: PA and lateral views of...  \n",
      "4  CXR5_IM-2117-1004003.jpg: Xray Chest PA and La...  \n",
      "Preprocessing Column: findings\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 3955/3955 [1:27:33<00:00,  1.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved preprocessed 'findings' column to: ./datasets/iu_xray/output/preprocessed_findings.csv\n",
      "Train data saved to: ./datasets/iu_xray/output/train_data.csv\n",
      "Validation data saved to: ./datasets/iu_xray/output/val_data.csv\n",
      "Test data saved to: ./datasets/iu_xray/output/test_data.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import nltk\n",
    "from spellchecker import SpellChecker\n",
    "from sklearn.model_selection import train_test_split\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Download necessary NLTK resources\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Initialize spell checker\n",
    "spell = SpellChecker()\n",
    "\n",
    "# Combined preprocessing functions\n",
    "def lowercase(text):\n",
    "    \"\"\"Convert text to lowercase.\"\"\"\n",
    "    return text.lower() if isinstance(text, str) else text\n",
    "\n",
    "def decontracted(text):\n",
    "    \"\"\"Decontract phrases in the text.\"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return text\n",
    "    contractions = {\n",
    "        \"won't\": \"will not\", \"can't\": \"can not\", \"couldn't\": \"could not\",\n",
    "        \"shouldn't\": \"should not\", \"wouldn't\": \"would not\", \"n't\": \" not\",\n",
    "        \"'re\": \" are\", \"'s\": \" is\", \"'d\": \" would\", \"'ll\": \" will\",\n",
    "        \"'t\": \" not\", \"'ve\": \" have\", \"'m\": \" am\"\n",
    "    }\n",
    "    for contraction, full_form in contractions.items():\n",
    "        text = text.replace(contraction, full_form)\n",
    "    return text\n",
    "\n",
    "def rem_punctuations(text):\n",
    "    \"\"\"Remove punctuations except for full stops.\"\"\"\n",
    "    return re.sub(r'[^\\w\\s.]', '', text) if isinstance(text, str) else text\n",
    "\n",
    "def rem_numbers(text):\n",
    "    \"\"\"Remove numbers and irrelevant text like 'XXXX'.\"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return text\n",
    "    text = re.sub(r'[xX]{2,}', '', text)  # Removes sequences like 'XXXX'\n",
    "    return re.sub(r'\\d+', '', text)\n",
    "\n",
    "def rem_two_letter_words(text):\n",
    "    \"\"\"Remove words with fewer than 2 characters except 'no' and 'ct'.\"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return text\n",
    "    return ' '.join(word for word in text.split() if len(word) > 2 or word in [\"no\", \"ct\"])\n",
    "\n",
    "def rem_stop_words(text):\n",
    "    \"\"\"Remove stop words.\"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return text\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    return ' '.join(word for word in text.split() if word not in stop_words)\n",
    "\n",
    "def correct_spelling(text):\n",
    "    \"\"\"Correct spelling using a spell checker.\"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return text\n",
    "    corrected = []\n",
    "    for word in text.split():\n",
    "        corrected_word = list(spell.candidates(word))[0] if spell.candidates(word) else word\n",
    "        corrected.append(corrected_word)\n",
    "    return ' '.join(corrected)\n",
    "\n",
    "def rem_extra_spaces(text):\n",
    "    \"\"\"Remove extra spaces.\"\"\"\n",
    "    return ' '.join(text.split()) if isinstance(text, str) else text\n",
    "\n",
    "def handle_fullstops(text):\n",
    "    \"\"\"Handle full stops, spacing around them, and remove multiple consecutive stops.\"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return text\n",
    "    text = re.sub(r'\\.\\.+', '.', text)  # Convert multiple full stops to single\n",
    "    return re.sub(r'\\.', ' . ', text)  # Add space around full stops\n",
    "\n",
    "def rem_apostrophes(text):\n",
    "    \"\"\"Remove apostrophes.\"\"\"\n",
    "    return re.sub(\"'\", '', text) if isinstance(text, str) else text\n",
    "\n",
    "# Combined text preprocessing function\n",
    "def preprocess_text(data):\n",
    "    \"\"\"Apply combined preprocessing steps.\"\"\"\n",
    "    preprocessed = []\n",
    "    for sentence in tqdm(data.values):\n",
    "        sentence = str(sentence)\n",
    "        sentence = lowercase(sentence)\n",
    "        sentence = decontracted(sentence)\n",
    "        sentence = rem_punctuations(sentence)\n",
    "        sentence = rem_numbers(sentence)\n",
    "        sentence = rem_two_letter_words(sentence)\n",
    "        sentence = rem_stop_words(sentence)\n",
    "        sentence = correct_spelling(sentence)\n",
    "        sentence = rem_apostrophes(sentence)\n",
    "        sentence = handle_fullstops(sentence)\n",
    "        sentence = rem_extra_spaces(sentence)\n",
    "        \n",
    "        preprocessed.append(sentence)\n",
    "\n",
    "    return preprocessed\n",
    "\n",
    "# Load your DataFrame (replace with actual path)\n",
    "input_path = os.path.join(output_directory, 'iu_xray_reports_sorted_df.csv')\n",
    "# output_directory = os.path.join(output_directory, 'iu_xray_reports_sorted_preprocessed_df.csv')\n",
    "df = pd.read_csv(input_path)\n",
    "\n",
    "# Select only the columns 'pmc_id', 'findings', 'image_1', and 'image_2'\n",
    "\n",
    "# Apply preprocessing on specific columns if they exist\n",
    "preprocess_columns = ['findings']\n",
    "for column in preprocess_columns:\n",
    "    if column in df.columns:\n",
    "        print(f\"Preprocessing Column: {column}\")\n",
    "        df[column] = df[column].fillna('none').astype(str)\n",
    "        df[column] = preprocess_text(df[column])\n",
    "        output_path = os.path.join(output_directory, f'preprocessed_{column}.csv')\n",
    "        df.to_csv(output_path, index=False)\n",
    "        print(f\"Saved preprocessed '{column}' column to: {output_path}\")\n",
    "\n",
    "# Split into Train/Validation/Test (70%/10%/20%)\n",
    "train_df, temp_df = train_test_split(df, test_size=0.3, random_state=42)\n",
    "val_df, test_df = train_test_split(temp_df, test_size=(2/3), random_state=42)\n",
    "\n",
    "# Save the splits\n",
    "train_path = os.path.join(output_directory, 'train_data.csv')\n",
    "val_path = os.path.join(output_directory, 'val_data.csv')\n",
    "test_path = os.path.join(output_directory, 'test_data.csv')\n",
    "\n",
    "train_df.to_csv(train_path, index=False)\n",
    "val_df.to_csv(val_path, index=False)\n",
    "test_df.to_csv(test_path, index=False)\n",
    "\n",
    "print(f\"Train data saved to: {train_path}\")\n",
    "print(f\"Validation data saved to: {val_path}\")\n",
    "print(f\"Test data saved to: {test_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the DataFrame: (792, 5)\n",
      "   pmc_id                                           findings   \n",
      "0     136  heart size normal lungs clear normal pneumonia...  \\\n",
      "1    3457                                               none   \n",
      "2     933  change lungs clear expanded heart mediastinum ...   \n",
      "3    1041  lucency crosses left posterior rib visualized ...   \n",
      "4    2538  normal heart size mediastinal contours abnorma...   \n",
      "\n",
      "                                             image_1   \n",
      "0   CXR136_IM-0233-1001.jpg:  PA and lateral chest.   \\\n",
      "1  CXR3457_IM-1678-1001.jpg: History preop XXXX f...   \n",
      "2  CXR933_IM-2431-1001.jpg: CHEST 2V FRONTAL/LATE...   \n",
      "3  CXR1041_IM-0033-1001.jpg: PA and lateral views...   \n",
      "4  CXR2538_IM-1050-1001.jpg: PA and lateral chest...   \n",
      "\n",
      "                                             image_2  index  \n",
      "0   CXR136_IM-0233-1002.jpg:  PA and lateral chest.       1  \n",
      "1  CXR3457_IM-1678-2001.jpg: History preop XXXX f...      2  \n",
      "2  CXR933_IM-2431-1002.jpg: CHEST 2V FRONTAL/LATE...      3  \n",
      "3  CXR1041_IM-0033-2001.jpg: PA and lateral views...      4  \n",
      "4  CXR2538_IM-1050-1002.jpg: PA and lateral chest...      5  \n",
      "Train data saved to: ./datasets/iu_xray/output/test_data.csv\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"./datasets/iu_xray/output/test_data.csv\")     \n",
    "df = df[['pmc_id', 'findings', 'image_1', 'image_2']]\n",
    "\n",
    "# Display the modified DataFrame to check the output\n",
    "\n",
    "df['index'] = range(1, len(df) + 1)\n",
    "print(\"Shape of the DataFrame:\", df.shape)\n",
    "print(df.head())\n",
    "df.to_csv(test_path, index=False)\n",
    "\n",
    "print(f\"Train data saved to: {test_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the DataFrame: (395, 5)\n",
      "Shape of the DataFrame: (383, 5)\n",
      "Train data saved to: ./datasets/iu_xray/output/filtered_val_data.csv\n"
     ]
    }
   ],
   "source": [
    "filtered_train_path = os.path.join(output_directory, 'filtered_train_data.csv')\n",
    "filtered_val_path = os.path.join(output_directory, 'filtered_val_data.csv')\n",
    "filtered_test_path = os.path.join(output_directory, 'filtered_test_data.csv')\n",
    "\n",
    "\n",
    "\n",
    "df = pd.read_csv(\"./datasets/iu_xray/output/val_data.csv\")  \n",
    "print(\"Shape of the DataFrame:\", df.shape)\n",
    "filtered_df = df.dropna(subset=['image_1', 'image_2'], how='all')\n",
    "print(\"Shape of the DataFrame:\", filtered_df.shape)\n",
    "filtered_df.to_csv(filtered_val_path, index=False)\n",
    "\n",
    "print(f\"Train data saved to: {filtered_val_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MlOl6I0rMa68"
   },
   "source": [
    "### **Create Data Loaders**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-06T00:58:15.385936Z",
     "iopub.status.busy": "2024-11-06T00:58:15.385308Z",
     "iopub.status.idle": "2024-11-06T00:58:15.405340Z",
     "shell.execute_reply": "2024-11-06T00:58:15.404469Z",
     "shell.execute_reply.started": "2024-11-06T00:58:15.385891Z"
    }
   },
   "outputs": [],
   "source": [
    "'''Image Data Loaders to Supply Dataset to Model in Batches'''\n",
    "\n",
    "# classes in dataset\n",
    "class CustomImageDataset(Dataset):\n",
    "    def __init__(self, image_dir, transform=None):\n",
    "        self.image_dir = image_dir\n",
    "        self.transform = transform\n",
    "        self.image_files = [f for f in os.listdir(image_dir) if f.endswith(('.png', '.jpg', '.jpeg'))]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.image_dir, self.image_files[idx])\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image\n",
    "\n",
    "\n",
    "# function to load image data with transformation and batching\n",
    "def load_preprocessed_images(image_dir, batch_size=32):\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),  \n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ])\n",
    "\n",
    "    dataset = CustomImageDataset(image_dir, transform=transform)\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-06T00:58:15.418177Z",
     "iopub.status.busy": "2024-11-06T00:58:15.417812Z",
     "iopub.status.idle": "2024-11-06T00:58:15.427555Z",
     "shell.execute_reply": "2024-11-06T00:58:15.426675Z",
     "shell.execute_reply.started": "2024-11-06T00:58:15.418135Z"
    }
   },
   "outputs": [],
   "source": [
    "'''Text Data Loaders to Supply Dataset to Model in Batches'''\n",
    "\n",
    "# classes in dataset\n",
    "class CustomTextDataset(Dataset):\n",
    "    def __init__(self, text_list, tokenizer, max_length=512):\n",
    "        self.text_list = text_list\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.text_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.text_list[idx]\n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            max_length=self.max_length,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        return {key: tensor.squeeze(0) for key, tensor in encoding.items()}\n",
    "\n",
    "\n",
    "# function to load text data with batching\n",
    "def load_preprocessed_texts(text_list, tokenizer, batch_size=32, max_length=512):\n",
    "    dataset = CustomTextDataset(text_list, tokenizer, max_length)\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wQ0H-X3HMgS1"
   },
   "source": [
    "## **Model Implementation**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PsELnlXpMjGX"
   },
   "source": [
    "### **Visual Extractor**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\"><b><<<<<<< local</b></span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shivangi/.local/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/shivangi/.local/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet101_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet101_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pmc_id</th>\n",
       "      <th>findings</th>\n",
       "      <th>image_1</th>\n",
       "      <th>image_2</th>\n",
       "      <th>index</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3224</td>\n",
       "      <td>cardiomediastinal silhouette normal size conto...</td>\n",
       "      <td>CXR3224_IM-1524-1001.jpg: Radiographs of the c...</td>\n",
       "      <td>CXR3224_IM-1524-2001.jpg: Radiographs of the c...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3226</td>\n",
       "      <td>cardiac contours normal lungs clear thoracic s...</td>\n",
       "      <td>CXR3226_IM-1525-1001.jpg: Xray Chest PA and La...</td>\n",
       "      <td>CXR3226_IM-1525-2001.jpg: Xray Chest PA and La...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>113</td>\n",
       "      <td>heart mediastinum unremarkable two subcentimet...</td>\n",
       "      <td>CXR113_IM-0086-1001.jpg: PA and lateral views ...</td>\n",
       "      <td>CXR113_IM-0086-2001.jpg: PA and lateral views ...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2347</td>\n",
       "      <td>cardiomediastinal silhouette normal size conto...</td>\n",
       "      <td>CXR2347_IM-0912-1001.jpg:  PA and lateral views.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1101</td>\n",
       "      <td>heart pulmonary mediastinum within normal limi...</td>\n",
       "      <td>CXR1101_IM-0068-1001.jpg: Xray Chest PA and La...</td>\n",
       "      <td>CXR1101_IM-0068-2001.jpg: Xray Chest PA and La...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>3305</td>\n",
       "      <td>right pics tip overlying right brachycephalic ...</td>\n",
       "      <td>CXR3305_IM-1581-1001.jpg: Xray Chest PA and La...</td>\n",
       "      <td>CXR3305_IM-1581-4001.jpg: Xray Chest PA and La...</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1994</td>\n",
       "      <td>lungs clear pneumothorax pleural effusion norm...</td>\n",
       "      <td>CXR1994_IM-0651-1001.jpg: Two-view chest XXXX,...</td>\n",
       "      <td>CXR1994_IM-0651-2001.jpg: Two-view chest XXXX,...</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>3645</td>\n",
       "      <td>none</td>\n",
       "      <td>CXR3645_IM-1807-2001.jpg: Xray Chest PA and La...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>3523</td>\n",
       "      <td>heart lungs interval lungs clear expanded hear...</td>\n",
       "      <td>CXR3523_IM-1721-1001.jpg:  CHEST 2V FRONTAL/LA...</td>\n",
       "      <td>CXR3523_IM-1721-1002.jpg:  CHEST 2V FRONTAL/LA...</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>480</td>\n",
       "      <td>none</td>\n",
       "      <td>CXR480_IM-2104-1001.jpg: 2 view CHEST: XXXX, X...</td>\n",
       "      <td>CXR480_IM-2104-2001.jpg: 2 view CHEST: XXXX, X...</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   pmc_id                                           findings   \n",
       "0    3224  cardiomediastinal silhouette normal size conto...  \\\n",
       "1    3226  cardiac contours normal lungs clear thoracic s...   \n",
       "2     113  heart mediastinum unremarkable two subcentimet...   \n",
       "3    2347  cardiomediastinal silhouette normal size conto...   \n",
       "4    1101  heart pulmonary mediastinum within normal limi...   \n",
       "5    3305  right pics tip overlying right brachycephalic ...   \n",
       "6    1994  lungs clear pneumothorax pleural effusion norm...   \n",
       "7    3645                                               none   \n",
       "8    3523  heart lungs interval lungs clear expanded hear...   \n",
       "9     480                                               none   \n",
       "\n",
       "                                             image_1   \n",
       "0  CXR3224_IM-1524-1001.jpg: Radiographs of the c...  \\\n",
       "1  CXR3226_IM-1525-1001.jpg: Xray Chest PA and La...   \n",
       "2  CXR113_IM-0086-1001.jpg: PA and lateral views ...   \n",
       "3  CXR2347_IM-0912-1001.jpg:  PA and lateral views.    \n",
       "4  CXR1101_IM-0068-1001.jpg: Xray Chest PA and La...   \n",
       "5  CXR3305_IM-1581-1001.jpg: Xray Chest PA and La...   \n",
       "6  CXR1994_IM-0651-1001.jpg: Two-view chest XXXX,...   \n",
       "7  CXR3645_IM-1807-2001.jpg: Xray Chest PA and La...   \n",
       "8  CXR3523_IM-1721-1001.jpg:  CHEST 2V FRONTAL/LA...   \n",
       "9  CXR480_IM-2104-1001.jpg: 2 view CHEST: XXXX, X...   \n",
       "\n",
       "                                             image_2  index  \n",
       "0  CXR3224_IM-1524-2001.jpg: Radiographs of the c...      1  \n",
       "1  CXR3226_IM-1525-2001.jpg: Xray Chest PA and La...      2  \n",
       "2  CXR113_IM-0086-2001.jpg: PA and lateral views ...      3  \n",
       "3                                                NaN      4  \n",
       "4  CXR1101_IM-0068-2001.jpg: Xray Chest PA and La...      5  \n",
       "5  CXR3305_IM-1581-4001.jpg: Xray Chest PA and La...      6  \n",
       "6  CXR1994_IM-0651-2001.jpg: Two-view chest XXXX,...      7  \n",
       "7                                                NaN      8  \n",
       "8  CXR3523_IM-1721-1002.jpg:  CHEST 2V FRONTAL/LA...      9  \n",
       "9  CXR480_IM-2104-2001.jpg: 2 view CHEST: XXXX, X...     10  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting features since they are not precomputed...\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'image_count'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/core/indexes/base.py:3652\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3651\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 3652\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3653\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/_libs/index.pyx:147\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/_libs/index.pyx:176\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7080\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7088\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'image_count'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[33], line 136\u001b[0m\n\u001b[1;32m    131\u001b[0m \u001b[38;5;66;03m# os.makedirs(feature_dir, exist_ok=True)\u001b[39;00m\n\u001b[1;32m    132\u001b[0m \n\u001b[1;32m    133\u001b[0m \u001b[38;5;66;03m# Here you might want to set the model to training mode\u001b[39;00m\n\u001b[1;32m    134\u001b[0m visual_extractor\u001b[38;5;241m.\u001b[39mtrain()  \n\u001b[0;32m--> 136\u001b[0m patch_feats, avg_feats, final_embeddings \u001b[38;5;241m=\u001b[39m \u001b[43mextract_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43miu_xray_reports_df\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    137\u001b[0m patch_feats \u001b[38;5;241m=\u001b[39m patch_feats\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# Shape: (3851, 4096)\u001b[39;00m\n\u001b[1;32m    138\u001b[0m avg_feats \u001b[38;5;241m=\u001b[39m avg_feats\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m1\u001b[39m)      \u001b[38;5;66;03m# Shape: (3851, 2048)\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[33], line 82\u001b[0m, in \u001b[0;36mextract_features\u001b[0;34m(data_loader)\u001b[0m\n\u001b[1;32m     79\u001b[0m patch_feats, avg_feats, final_embeddings \u001b[38;5;241m=\u001b[39m [], [], []\n\u001b[1;32m     81\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m idx, row \u001b[38;5;129;01min\u001b[39;00m data_loader\u001b[38;5;241m.\u001b[39miterrows():\n\u001b[0;32m---> 82\u001b[0m     images \u001b[38;5;241m=\u001b[39m \u001b[43mload_images\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrow\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43miu_xray_images_preprocessed\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mto(device)  \u001b[38;5;66;03m# Load up to 2 images\u001b[39;00m\n\u001b[1;32m     84\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m images\u001b[38;5;241m.\u001b[39mnumel() \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:  \u001b[38;5;66;03m# Skip if no images were loaded\u001b[39;00m\n\u001b[1;32m     85\u001b[0m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[33], line 46\u001b[0m, in \u001b[0;36mload_images\u001b[0;34m(report_row, img_folder)\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_images\u001b[39m(report_row, img_folder):\n\u001b[1;32m     45\u001b[0m     images \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m---> 46\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mmin\u001b[39m(\u001b[43mreport_row\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mimage_count\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m, \u001b[38;5;241m2\u001b[39m)):  \u001b[38;5;66;03m# Limit to 2 images max\u001b[39;00m\n\u001b[1;32m     47\u001b[0m         img_filename \u001b[38;5;241m=\u001b[39m report_row[\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimage_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.png\u001b[39m\u001b[38;5;124m'\u001b[39m  \u001b[38;5;66;03m# Replace with .png\u001b[39;00m\n\u001b[1;32m     48\u001b[0m         img_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(img_folder, img_filename)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/core/series.py:1012\u001b[0m, in \u001b[0;36mSeries.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   1009\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_values[key]\n\u001b[1;32m   1011\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m key_is_scalar:\n\u001b[0;32m-> 1012\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_value\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1014\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_hashable(key):\n\u001b[1;32m   1015\u001b[0m     \u001b[38;5;66;03m# Otherwise index.get_value will raise InvalidIndexError\u001b[39;00m\n\u001b[1;32m   1016\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1017\u001b[0m         \u001b[38;5;66;03m# For labels that don't resolve as scalars like tuples and frozensets\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/core/series.py:1121\u001b[0m, in \u001b[0;36mSeries._get_value\u001b[0;34m(self, label, takeable)\u001b[0m\n\u001b[1;32m   1118\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_values[label]\n\u001b[1;32m   1120\u001b[0m \u001b[38;5;66;03m# Similar to Index.get_value, but we do not fall back to positional\u001b[39;00m\n\u001b[0;32m-> 1121\u001b[0m loc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1123\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(loc):\n\u001b[1;32m   1124\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_values[loc]\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/core/indexes/base.py:3654\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3652\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine\u001b[38;5;241m.\u001b[39mget_loc(casted_key)\n\u001b[1;32m   3653\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[0;32m-> 3654\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[1;32m   3655\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m   3656\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[1;32m   3657\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[1;32m   3658\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[1;32m   3659\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'image_count'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import models, transforms\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "import torch.optim as optim\n",
    "\n",
    "# Define paths for saving features\n",
    "patch_feats_file = os.path.join(output_directory, 'patch_feats.pt')\n",
    "avg_feats_file = os.path.join(output_directory, 'avg_feats.pt')\n",
    "final_embeddings_file = os.path.join(output_directory, 'final_embeddings.pt')\n",
    "\n",
    "# Define the transform for image preprocessing\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),  # Adjust size based on model input requirement\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # Normalization for ResNet\n",
    "])\n",
    "\n",
    "# Define VisualExtractor class\n",
    "class VisualExtractor(nn.Module):\n",
    "    def __init__(self, visual_extractor='resnet101', pretrained=True):\n",
    "        super(VisualExtractor, self).__init__()\n",
    "\n",
    "        model = getattr(models, visual_extractor)(pretrained=pretrained)\n",
    "        \n",
    "        # Remove the last fully connected layer\n",
    "        modules = list(model.children())[:-2]  \n",
    "        self.model = nn.Sequential(*modules)\n",
    "        \n",
    "        # Average pooling and a fully connected layer to transform the features\n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.fc_layer = nn.Linear(model.fc.in_features, 2048)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.model(x)\n",
    "        avg_feats = self.avg_pool(x).view(x.size(0), -1)\n",
    "        patch_feats = self.fc_layer(avg_feats)\n",
    "        return patch_feats, avg_feats\n",
    "\n",
    "# Load images function that handles the new DataFrame structure\n",
    "def load_images(report_row, img_folder):\n",
    "    images = []\n",
    "    for i in range(1, 3):  # image_1 and image_2\n",
    "        img_filename = report_row[f'image_{i}'].split('.')[0] + '.png'  # Replace with .png\n",
    "        img_path = os.path.join(img_folder, img_filename)\n",
    "        \n",
    "        # Check if the image file exists before attempting to open it\n",
    "        if os.path.exists(img_path):\n",
    "            img = Image.open(img_path).convert(\"RGB\")\n",
    "            images.append(transform(img))\n",
    "        else:\n",
    "            print(f\"Warning: Image file not found: {img_path}\")  # Warning if file not found\n",
    "\n",
    "    # If only one image was loaded, duplicate it\n",
    "    if len(images) == 1:\n",
    "        images.append(images[0].clone())  # Duplicate the single available image\n",
    "    \n",
    "    return torch.stack(images) if images else torch.tensor([])  # Return empty tensor if no images loaded\n",
    "\n",
    "# Initialize visual extractor\n",
    "visual_extractor = VisualExtractor()\n",
    "\n",
    "# Move the model to GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "visual_extractor.to(device)\n",
    "\n",
    "# Set learning rates and other parameters\n",
    "learning_rate = 5e-5  # 5  10^5\n",
    "other_parameter = 1e-4  # Example of another parameter (like weight decay)\n",
    "\n",
    "# Create an optimizer\n",
    "optimizer = optim.Adam(visual_extractor.parameters(), lr=learning_rate, weight_decay=other_parameter)\n",
    "\n",
    "def extract_features(data_loader):\n",
    "    patch_feats, avg_feats, final_embeddings = [], [], []\n",
    "    \n",
    "    for idx, row in data_loader.iterrows():\n",
    "        images = load_images(row, iu_xray_images_preprocessed).to(device)  # Load up to 2 images\n",
    "        \n",
    "        if images.numel() == 0:  # Skip if no images were loaded\n",
    "            continue\n",
    "        \n",
    "        report_patch_feats, report_avg_feats = [], []\n",
    "        \n",
    "        # Iterate directly over the images tensor\n",
    "        for image in images:  # Process each image in the loaded images tensor\n",
    "            image = image.unsqueeze(0).to(device)  # Move single image to device\n",
    "            pf, af = visual_extractor(image)  # Extract patch_feats, avg_feats\n",
    "            print(f\"{row['index']}, {row['pmc_id']} Patch Features Shape: {pf.shape}, Avg Features Shape: {af.shape}\")\n",
    "            \n",
    "            # Store the output temporarily, releasing earlier references\n",
    "            report_patch_feats.append(pf.detach())\n",
    "            report_avg_feats.append(af.detach())\n",
    "\n",
    "        # Concatenate patch features and average the avg_feats\n",
    "        concatenated_patch_feats = torch.cat(report_patch_feats, dim=1)  # Concatenate along feature dimension\n",
    "        averaged_avg_feats = torch.mean(torch.stack(report_avg_feats), dim=0)  # Average the avg_feats\n",
    "\n",
    "        # Combine avg_feats with patch_feats to create final embedding\n",
    "        final_embedding = torch.cat((averaged_avg_feats, concatenated_patch_feats), dim=1)\n",
    "        patch_feats.append(concatenated_patch_feats)\n",
    "        avg_feats.append(averaged_avg_feats)\n",
    "        final_embeddings.append(final_embedding)\n",
    "        \n",
    "    return torch.stack(patch_feats), torch.stack(avg_feats), torch.stack(final_embeddings)\n",
    "\n",
    "# Functions to load and save features\n",
    "def load_features(file_path):\n",
    "    return torch.load(file_path)\n",
    "\n",
    "def save_features(file_path, features):\n",
    "    torch.save(features, file_path)\n",
    "\n",
    "# Load data\n",
    "report_data_path = os.path.join(output_directory, 'filtered_train_data.csv')\n",
    "iu_xray_reports_df = pd.read_csv(report_data_path)\n",
    "display(iu_xray_reports_df.head(10))\n",
    "\n",
    "# Check if features are already saved; if not, extract and save them\n",
    "if not os.path.exists(patch_feats_file) and os.path.exists(avg_feats_file) and os.path.exists(final_embeddings_file):\n",
    "    print(\"All features are already precomputed and will be loaded.\")\n",
    "    patch_feats = load_features(patch_feats_file)\n",
    "    avg_feats = load_features(avg_feats_file)\n",
    "    final_embeddings = load_features(final_embeddings_file)\n",
    "else:\n",
    "    print(\"Extracting features since they are not precomputed...\")\n",
    "    \n",
    "    # Set the model to training mode\n",
    "    visual_extractor.train()  \n",
    "    \n",
    "    patch_feats, avg_feats, final_embeddings = extract_features(iu_xray_reports_df)\n",
    "    patch_feats = patch_feats.squeeze(1)  # Shape: (num_reports, 4096)\n",
    "    avg_feats = avg_feats.squeeze(1)      # Shape: (num_reports, 2048)\n",
    "    final_embeddings = final_embeddings.squeeze(1)  # Shape: (num_reports, 6144)\n",
    "\n",
    "    save_features(patch_feats_file, patch_feats)\n",
    "    save_features(avg_feats_file, avg_feats)\n",
    "    save_features(final_embeddings_file, final_embeddings)\n",
    "\n",
    "# Displaying the shapes of the feature dataframes\n",
    "print(\"Patch Features Shape:\", patch_feats.shape)\n",
    "print(\"Average Features Shape:\", avg_feats.shape)\n",
    "print(\"Final Embedding Shape:\", final_embeddings.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patch_feats = patch_feats.squeeze(1)  # Shape: (3851, 4096)\n",
    "avg_feats = avg_feats.squeeze(1)      # Shape: (3851, 2048)\n",
    "final_embeddings = final_embeddings.squeeze(1)  # Shape: (3851, 6144)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Patch Features Shape: torch.Size([2705, 4096])\n",
      "Average Features Shape: torch.Size([2705, 2048])\n",
      "Final Embedding Shape: torch.Size([2705, 6144])\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "\n",
    "# Define paths for the saved features\n",
    "patch_feats_file = os.path.join(output_directory, 'patch_feats.pt')\n",
    "avg_feats_file = os.path.join(output_directory, 'avg_feats.pt')\n",
    "final_embeddings_file = os.path.join(output_directory, 'final_embeddings.pt')\n",
    "\n",
    "# Function to load features and print their shapes\n",
    "def print_tensor_shapes():\n",
    "    # Load patch features\n",
    "    if os.path.exists(patch_feats_file):\n",
    "        patch_feats = torch.load(patch_feats_file, map_location=torch.device('cpu'))\n",
    "        print(\"Patch Features Shape:\", patch_feats.shape)\n",
    "    else:\n",
    "        print(f\"Patch features file not found: {patch_feats_file}\")\n",
    "\n",
    "    # Load average features\n",
    "    if os.path.exists(avg_feats_file):\n",
    "        avg_feats = torch.load(avg_feats_file, map_location=torch.device('cpu'))\n",
    "        print(\"Average Features Shape:\", avg_feats.shape)\n",
    "    else:\n",
    "        print(f\"Average features file not found: {avg_feats_file}\")\n",
    "\n",
    "    # Load final embeddings\n",
    "    if os.path.exists(final_embeddings_file):\n",
    "        final_embeddings = torch.load(final_embeddings_file, map_location=torch.device('cpu'))\n",
    "        print(\"Final Embedding Shape:\", final_embeddings.shape)\n",
    "    else:\n",
    "        print(f\"Final embeddings file not found: {final_embeddings_file}\")\n",
    "\n",
    "# Call the function to print shapes\n",
    "print_tensor_shapes()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\"><b>=======</b></span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-06T01:07:55.002100Z",
     "iopub.status.busy": "2024-11-06T01:07:55.001714Z",
     "iopub.status.idle": "2024-11-06T01:10:05.841903Z",
     "shell.execute_reply": "2024-11-06T01:10:05.840888Z",
     "shell.execute_reply.started": "2024-11-06T01:07:55.002064Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet101_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet101_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pmc_id</th>\n",
       "      <th>findings</th>\n",
       "      <th>image_1</th>\n",
       "      <th>image_2</th>\n",
       "      <th>index</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>755</td>\n",
       "      <td>none</td>\n",
       "      <td>CXR755_IM-2306-1001.jpg: PA and lateral views ...</td>\n",
       "      <td>CXR755_IM-2306-2001.jpg: PA and lateral views ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2093</td>\n",
       "      <td>lungs clear cardiomediastinal silhouette withi...</td>\n",
       "      <td>CXR2093_IM-0723-1001.jpg:  PA and lateral view...</td>\n",
       "      <td>CXR2093_IM-0723-2001.jpg:  PA and lateral view...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1464</td>\n",
       "      <td>heart pulmonary mediastinum within normal limi...</td>\n",
       "      <td>CXR1464_IM-0301-1001.jpg: Xray Chest PA and La...</td>\n",
       "      <td>CXR1464_IM-0301-2001.jpg: Xray Chest PA and La...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2419</td>\n",
       "      <td>none</td>\n",
       "      <td>CXR2419_IM-0963-1001.jpg: Xray Chest PA and La...</td>\n",
       "      <td>CXR2419_IM-0963-2001.jpg: Xray Chest PA and La...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1233</td>\n",
       "      <td>enlarged cardiomediastinal silhouette low lung...</td>\n",
       "      <td>CXR1233_IM-0157-1001.jpg:  PA and lateral views.</td>\n",
       "      <td>CXR1233_IM-0157-2001.jpg:  PA and lateral views.</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>847</td>\n",
       "      <td>trachea midline heart slightly large low lung ...</td>\n",
       "      <td>CXR847_IM-2369-1001.jpg: CHEST 2V FRONTAL/LATE...</td>\n",
       "      <td>CXR847_IM-2369-1002.jpg: CHEST 2V FRONTAL/LATE...</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2637</td>\n",
       "      <td>moderate marked enlargement cardiac silhouette...</td>\n",
       "      <td>CXR2637_IM-1122-1001.jpg: Xray Chest PA and La...</td>\n",
       "      <td>CXR2637_IM-1122-2001.jpg: Xray Chest PA and La...</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2197</td>\n",
       "      <td>heart size normal parities appear right fissur...</td>\n",
       "      <td>CXR2197_IM-0807-1001.jpg: Chest x-XXXX, 2 view...</td>\n",
       "      <td>CXR2197_IM-0807-2001.jpg: Chest x-XXXX, 2 view...</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2532</td>\n",
       "      <td>stable enlargement cardiac silhouette lateral ...</td>\n",
       "      <td>CXR2532_IM-1046-1001.jpg: Xray Chest PA and La...</td>\n",
       "      <td>CXR2532_IM-1046-2001.jpg: Xray Chest PA and La...</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>3658</td>\n",
       "      <td>cardiomediastinal silhouette normal size conto...</td>\n",
       "      <td>CXR3658_IM-1819-1001.jpg: Xray Chest PA and La...</td>\n",
       "      <td>CXR3658_IM-1819-2001.jpg: Xray Chest PA and La...</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   pmc_id                                           findings  \\\n",
       "0     755                                               none   \n",
       "1    2093  lungs clear cardiomediastinal silhouette withi...   \n",
       "2    1464  heart pulmonary mediastinum within normal limi...   \n",
       "3    2419                                               none   \n",
       "4    1233  enlarged cardiomediastinal silhouette low lung...   \n",
       "5     847  trachea midline heart slightly large low lung ...   \n",
       "6    2637  moderate marked enlargement cardiac silhouette...   \n",
       "7    2197  heart size normal parities appear right fissur...   \n",
       "8    2532  stable enlargement cardiac silhouette lateral ...   \n",
       "9    3658  cardiomediastinal silhouette normal size conto...   \n",
       "\n",
       "                                             image_1  \\\n",
       "0  CXR755_IM-2306-1001.jpg: PA and lateral views ...   \n",
       "1  CXR2093_IM-0723-1001.jpg:  PA and lateral view...   \n",
       "2  CXR1464_IM-0301-1001.jpg: Xray Chest PA and La...   \n",
       "3  CXR2419_IM-0963-1001.jpg: Xray Chest PA and La...   \n",
       "4  CXR1233_IM-0157-1001.jpg:  PA and lateral views.    \n",
       "5  CXR847_IM-2369-1001.jpg: CHEST 2V FRONTAL/LATE...   \n",
       "6  CXR2637_IM-1122-1001.jpg: Xray Chest PA and La...   \n",
       "7  CXR2197_IM-0807-1001.jpg: Chest x-XXXX, 2 view...   \n",
       "8  CXR2532_IM-1046-1001.jpg: Xray Chest PA and La...   \n",
       "9  CXR3658_IM-1819-1001.jpg: Xray Chest PA and La...   \n",
       "\n",
       "                                             image_2  index  \n",
       "0  CXR755_IM-2306-2001.jpg: PA and lateral views ...      1  \n",
       "1  CXR2093_IM-0723-2001.jpg:  PA and lateral view...      2  \n",
       "2  CXR1464_IM-0301-2001.jpg: Xray Chest PA and La...      3  \n",
       "3  CXR2419_IM-0963-2001.jpg: Xray Chest PA and La...      4  \n",
       "4  CXR1233_IM-0157-2001.jpg:  PA and lateral views.       5  \n",
       "5  CXR847_IM-2369-1002.jpg: CHEST 2V FRONTAL/LATE...      6  \n",
       "6  CXR2637_IM-1122-2001.jpg: Xray Chest PA and La...      8  \n",
       "7  CXR2197_IM-0807-2001.jpg: Chest x-XXXX, 2 view...      9  \n",
       "8  CXR2532_IM-1046-2001.jpg: Xray Chest PA and La...     10  \n",
       "9  CXR3658_IM-1819-2001.jpg: Xray Chest PA and La...     11  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting features since they are not precomputed...\n",
      "Warning: Image file not found: ./datasets/iu_xray/output/images_preprocessed/nan.png([1, 2048])\n",
      "Warning: Image file not found: ./datasets/iu_xray/output/images_preprocessed/nan.png([1, 2048])\n",
      "Warning: Image file not found: ./datasets/iu_xray/output/images_preprocessed/nan.png[1, 2048]))\n",
      "Warning: Image file not found: ./datasets/iu_xray/output/images_preprocessed/nan.png[1, 2048]))\n",
      "Warning: Image file not found: ./datasets/iu_xray/output/images_preprocessed/nan.png[1, 2048]))\n",
      "Warning: Image file not found: ./datasets/iu_xray/output/images_preprocessed/nan.png[1, 2048]))\n",
      "Warning: Image file not found: ./datasets/iu_xray/output/images_preprocessed/nan.png[1, 2048]))\n",
      "Warning: Image file not found: ./datasets/iu_xray/output/images_preprocessed/nan.png([1, 2048])\n",
      "Warning: Image file not found: ./datasets/iu_xray/output/images_preprocessed/nan.png[1, 2048]))\n",
      "Warning: Image file not found: ./datasets/iu_xray/output/images_preprocessed/nan.png([1, 2048])\n",
      "Warning: Image file not found: ./datasets/iu_xray/output/images_preprocessed/nan.png([1, 2048])\n",
      "Warning: Image file not found: ./datasets/iu_xray/output/images_preprocessed/nan.png([1, 2048])\n",
      "Warning: Image file not found: ./datasets/iu_xray/output/images_preprocessed/nan.png([1, 2048])\n",
      "Warning: Image file not found: ./datasets/iu_xray/output/images_preprocessed/nan.png([1, 2048])\n",
      "Warning: Image file not found: ./datasets/iu_xray/output/images_preprocessed/nan.png([1, 2048])\n",
      "Warning: Image file not found: ./datasets/iu_xray/output/images_preprocessed/nan.png([1, 2048])\n",
      "Warning: Image file not found: ./datasets/iu_xray/output/images_preprocessed/nan.png([1, 2048]))\n",
      "Warning: Image file not found: ./datasets/iu_xray/output/images_preprocessed/nan.png([1, 2048]))\n",
      "Warning: Image file not found: ./datasets/iu_xray/output/images_preprocessed/nan.pnge([1, 2048])\n",
      "Warning: Image file not found: ./datasets/iu_xray/output/images_preprocessed/nan.png([1, 2048]))\n",
      "Warning: Image file not found: ./datasets/iu_xray/output/images_preprocessed/nan.png([1, 2048]))\n",
      "Warning: Image file not found: ./datasets/iu_xray/output/images_preprocessed/nan.pnge([1, 2048])\n",
      "Warning: Image file not found: ./datasets/iu_xray/output/images_preprocessed/nan.png([1, 2048]))\n",
      "Warning: Image file not found: ./datasets/iu_xray/output/images_preprocessed/nan.pnge([1, 2048])\n",
      "Warning: Image file not found: ./datasets/iu_xray/output/images_preprocessed/nan.pnge([1, 2048])\n",
      "Warning: Image file not found: ./datasets/iu_xray/output/images_preprocessed/nan.png([1, 2048]))\n",
      "Warning: Image file not found: ./datasets/iu_xray/output/images_preprocessed/nan.png([1, 2048]))\n",
      "Warning: Image file not found: ./datasets/iu_xray/output/images_preprocessed/nan.pnge([1, 2048])\n",
      "Warning: Image file not found: ./datasets/iu_xray/output/images_preprocessed/nan.png([1, 2048])\n",
      "Warning: Image file not found: ./datasets/iu_xray/output/images_preprocessed/nan.pnge([1, 2048])\n",
      "Warning: Image file not found: ./datasets/iu_xray/output/images_preprocessed/nan.pnge([1, 2048])\n",
      "Warning: Image file not found: ./datasets/iu_xray/output/images_preprocessed/nan.pnge([1, 2048])\n",
      "Warning: Image file not found: ./datasets/iu_xray/output/images_preprocessed/nan.pnge([1, 2048])\n",
      "Warning: Image file not found: ./datasets/iu_xray/output/images_preprocessed/nan.png([1, 2048]))\n",
      "Warning: Image file not found: ./datasets/iu_xray/output/images_preprocessed/nan.png([1, 2048]))\n",
      "Warning: Image file not found: ./datasets/iu_xray/output/images_preprocessed/nan.pnge([1, 2048])\n",
      "Warning: Image file not found: ./datasets/iu_xray/output/images_preprocessed/nan.png([1, 2048])\n",
      "Warning: Image file not found: ./datasets/iu_xray/output/images_preprocessed/nan.pnge([1, 2048])\n",
      "Warning: Image file not found: ./datasets/iu_xray/output/images_preprocessed/nan.pnge([1, 2048])\n",
      "Warning: Image file not found: ./datasets/iu_xray/output/images_preprocessed/nan.pnge([1, 2048])\n",
      "Warning: Image file not found: ./datasets/iu_xray/output/images_preprocessed/nan.png([1, 2048]))\n",
      "Warning: Image file not found: ./datasets/iu_xray/output/images_preprocessed/nan.png[1, 2048])])\n",
      "Warning: Image file not found: ./datasets/iu_xray/output/images_preprocessed/nan.pnge([1, 2048])\n",
      "Warning: Image file not found: ./datasets/iu_xray/output/images_preprocessed/nan.pnge([1, 2048])\n",
      "Warning: Image file not found: ./datasets/iu_xray/output/images_preprocessed/nan.pnge([1, 2048])\n",
      "Warning: Image file not found: ./datasets/iu_xray/output/images_preprocessed/nan.png([1, 2048]))\n",
      "Warning: Image file not found: ./datasets/iu_xray/output/images_preprocessed/nan.pnge([1, 2048])\n",
      "Warning: Image file not found: ./datasets/iu_xray/output/images_preprocessed/nan.png([1, 2048]))\n",
      "Warning: Image file not found: ./datasets/iu_xray/output/images_preprocessed/nan.pnge([1, 2048])\n",
      "Warning: Image file not found: ./datasets/iu_xray/output/images_preprocessed/nan.pnge([1, 2048])\n",
      "Warning: Image file not found: ./datasets/iu_xray/output/images_preprocessed/nan.png([1, 2048]))\n",
      "Warning: Image file not found: ./datasets/iu_xray/output/images_preprocessed/nan.pnge([1, 2048])\n",
      "Warning: Image file not found: ./datasets/iu_xray/output/images_preprocessed/nan.png([1, 2048]))\n",
      "Warning: Image file not found: ./datasets/iu_xray/output/images_preprocessed/nan.pnge([1, 2048])\n",
      "Warning: Image file not found: ./datasets/iu_xray/output/images_preprocessed/nan.png([1, 2048]))\n",
      "Warning: Image file not found: ./datasets/iu_xray/output/images_preprocessed/nan.pnge([1, 2048])\n",
      "Warning: Image file not found: ./datasets/iu_xray/output/images_preprocessed/nan.pnge([1, 2048])\n",
      "Warning: Image file not found: ./datasets/iu_xray/output/images_preprocessed/nan.pnge([1, 2048])\n",
      "Warning: Image file not found: ./datasets/iu_xray/output/images_preprocessed/nan.pnge([1, 2048])\n",
      "Warning: Image file not found: ./datasets/iu_xray/output/images_preprocessed/nan.pnge([1, 2048])\n",
      "Warning: Image file not found: ./datasets/iu_xray/output/images_preprocessed/nan.pnge([1, 2048])\n",
      "Warning: Image file not found: ./datasets/iu_xray/output/images_preprocessed/nan.pnge([1, 2048])\n",
      "Warning: Image file not found: ./datasets/iu_xray/output/images_preprocessed/nan.pnge([1, 2048])\n",
      "Warning: Image file not found: ./datasets/iu_xray/output/images_preprocessed/nan.pnge([1, 2048])\n",
      "Warning: Image file not found: ./datasets/iu_xray/output/images_preprocessed/nan.pnge([1, 2048])\n",
      "Warning: Image file not found: ./datasets/iu_xray/output/images_preprocessed/nan.pnge([1, 2048])\n",
      "Warning: Image file not found: ./datasets/iu_xray/output/images_preprocessed/nan.pnge([1, 2048])\n",
      "Warning: Image file not found: ./datasets/iu_xray/output/images_preprocessed/nan.pnge([1, 2048])\n",
      "Warning: Image file not found: ./datasets/iu_xray/output/images_preprocessed/nan.pnge([1, 2048])\n",
      "Warning: Image file not found: ./datasets/iu_xray/output/images_preprocessed/nan.pnge([1, 2048])\n",
      "Warning: Image file not found: ./datasets/iu_xray/output/images_preprocessed/nan.pnge([1, 2048])\n",
      "Warning: Image file not found: ./datasets/iu_xray/output/images_preprocessed/nan.pnge([1, 2048])\n",
      "Warning: Image file not found: ./datasets/iu_xray/output/images_preprocessed/nan.pnge([1, 2048])\n",
      "Warning: Image file not found: ./datasets/iu_xray/output/images_preprocessed/nan.png([1, 2048]))\n",
      "Warning: Image file not found: ./datasets/iu_xray/output/images_preprocessed/nan.png([1, 2048]))\n",
      "Warning: Image file not found: ./datasets/iu_xray/output/images_preprocessed/nan.pnge([1, 2048])\n",
      "Warning: Image file not found: ./datasets/iu_xray/output/images_preprocessed/nan.png([1, 2048]))\n",
      "Warning: Image file not found: ./datasets/iu_xray/output/images_preprocessed/nan.pnge([1, 2048])\n",
      "Warning: Image file not found: ./datasets/iu_xray/output/images_preprocessed/nan.pnge([1, 2048])\n",
      "Warning: Image file not found: ./datasets/iu_xray/output/images_preprocessed/nan.png([1, 2048]))\n",
      "Warning: Image file not found: ./datasets/iu_xray/output/images_preprocessed/nan.pnge([1, 2048])\n",
      "Warning: Image file not found: ./datasets/iu_xray/output/images_preprocessed/nan.pnge([1, 2048])\n",
      "Warning: Image file not found: ./datasets/iu_xray/output/images_preprocessed/nan.pnge([1, 2048])\n",
      "Warning: Image file not found: ./datasets/iu_xray/output/images_preprocessed/nan.pnge([1, 2048])\n",
      "Warning: Image file not found: ./datasets/iu_xray/output/images_preprocessed/nan.pnge([1, 2048])\n",
      "Warning: Image file not found: ./datasets/iu_xray/output/images_preprocessed/nan.pnge([1, 2048])\n",
      "Warning: Image file not found: ./datasets/iu_xray/output/images_preprocessed/nan.pnge([1, 2048])\n",
      "Warning: Image file not found: ./datasets/iu_xray/output/images_preprocessed/nan.pnge([1, 2048])\n",
      "Warning: Image file not found: ./datasets/iu_xray/output/images_preprocessed/nan.pnge([1, 2048])\n",
      "Warning: Image file not found: ./datasets/iu_xray/output/images_preprocessed/nan.pnge([1, 2048])\n",
      "Warning: Image file not found: ./datasets/iu_xray/output/images_preprocessed/nan.pnge([1, 2048])\n",
      "Warning: Image file not found: ./datasets/iu_xray/output/images_preprocessed/nan.pnge([1, 2048])\n",
      "Warning: Image file not found: ./datasets/iu_xray/output/images_preprocessed/nan.pnge([1, 2048])\n",
      "Warning: Image file not found: ./datasets/iu_xray/output/images_preprocessed/nan.pnge([1, 2048])\n",
      "Warning: Image file not found: ./datasets/iu_xray/output/images_preprocessed/nan.png([1, 2048]))\n",
      "Warning: Image file not found: ./datasets/iu_xray/output/images_preprocessed/nan.pnge([1, 2048])\n",
      "Warning: Image file not found: ./datasets/iu_xray/output/images_preprocessed/nan.pnge([1, 2048])\n",
      "Warning: Image file not found: ./datasets/iu_xray/output/images_preprocessed/nan.pnge([1, 2048])\n",
      "Warning: Image file not found: ./datasets/iu_xray/output/images_preprocessed/nan.pnge([1, 2048])\n",
      "Warning: Image file not found: ./datasets/iu_xray/output/images_preprocessed/nan.pnge([1, 2048])\n",
      "Warning: Image file not found: ./datasets/iu_xray/output/images_preprocessed/nan.png([1, 2048]))\n",
      "Warning: Image file not found: ./datasets/iu_xray/output/images_preprocessed/nan.png([1, 2048]))\n",
      "Warning: Image file not found: ./datasets/iu_xray/output/images_preprocessed/nan.pnge([1, 2048])\n",
      "Warning: Image file not found: ./datasets/iu_xray/output/images_preprocessed/nan.pnge([1, 2048])\n",
      "Warning: Image file not found: ./datasets/iu_xray/output/images_preprocessed/nan.pnge([1, 2048])\n",
      "Warning: Image file not found: ./datasets/iu_xray/output/images_preprocessed/nan.pnge([1, 2048])\n",
      "Warning: Image file not found: ./datasets/iu_xray/output/images_preprocessed/nan.pnge([1, 2048])\n",
      "Warning: Image file not found: ./datasets/iu_xray/output/images_preprocessed/nan.pnge([1, 2048])\n",
      "Warning: Image file not found: ./datasets/iu_xray/output/images_preprocessed/nan.pngze([1, 2048])\n",
      "Warning: Image file not found: ./datasets/iu_xray/output/images_preprocessed/nan.pnge([1, 2048]))\n",
      "Warning: Image file not found: ./datasets/iu_xray/output/images_preprocessed/nan.pngze([1, 2048])\n",
      "Warning: Image file not found: ./datasets/iu_xray/output/images_preprocessed/nan.pnge([1, 2048])\n",
      "Warning: Image file not found: ./datasets/iu_xray/output/images_preprocessed/nan.pngze([1, 2048])\n",
      "Warning: Image file not found: ./datasets/iu_xray/output/images_preprocessed/nan.pngze([1, 2048])\n",
      "Warning: Image file not found: ./datasets/iu_xray/output/images_preprocessed/nan.pngze([1, 2048])\n",
      "Warning: Image file not found: ./datasets/iu_xray/output/images_preprocessed/nan.pnge([1, 2048]))\n",
      "Warning: Image file not found: ./datasets/iu_xray/output/images_preprocessed/nan.pngze([1, 2048])\n",
      "Warning: Image file not found: ./datasets/iu_xray/output/images_preprocessed/nan.pngze([1, 2048])\n",
      "Warning: Image file not found: ./datasets/iu_xray/output/images_preprocessed/nan.pngze([1, 2048])\n",
      "Warning: Image file not found: ./datasets/iu_xray/output/images_preprocessed/nan.pngze([1, 2048])\n",
      "Warning: Image file not found: ./datasets/iu_xray/output/images_preprocessed/nan.pngze([1, 2048])\n",
      "Warning: Image file not found: ./datasets/iu_xray/output/images_preprocessed/nan.pngze([1, 2048])\n",
      "Warning: Image file not found: ./datasets/iu_xray/output/images_preprocessed/nan.pngze([1, 2048])\n",
      "Warning: Image file not found: ./datasets/iu_xray/output/images_preprocessed/nan.pngze([1, 2048])\n",
      "Warning: Image file not found: ./datasets/iu_xray/output/images_preprocessed/nan.pngze([1, 2048])\n",
      "Warning: Image file not found: ./datasets/iu_xray/output/images_preprocessed/nan.pngze([1, 2048])\n",
      "Warning: Image file not found: ./datasets/iu_xray/output/images_preprocessed/nan.pnge([1, 2048]))\n",
      "Warning: Image file not found: ./datasets/iu_xray/output/images_preprocessed/nan.pngze([1, 2048])\n",
      "Warning: Image file not found: ./datasets/iu_xray/output/images_preprocessed/nan.png([1, 2048])])\n",
      "Warning: Image file not found: ./datasets/iu_xray/output/images_preprocessed/nan.pnge([1, 2048]))\n",
      "Warning: Image file not found: ./datasets/iu_xray/output/images_preprocessed/nan.pngze([1, 2048])\n",
      "Warning: Image file not found: ./datasets/iu_xray/output/images_preprocessed/nan.pngze([1, 2048])\n",
      "Warning: Image file not found: ./datasets/iu_xray/output/images_preprocessed/nan.pngze([1, 2048])\n",
      "Warning: Image file not found: ./datasets/iu_xray/output/images_preprocessed/nan.pngze([1, 2048])\n",
      "Warning: Image file not found: ./datasets/iu_xray/output/images_preprocessed/nan.pngze([1, 2048])\n",
      "Warning: Image file not found: ./datasets/iu_xray/output/images_preprocessed/nan.pngze([1, 2048])\n",
      "Warning: Image file not found: ./datasets/iu_xray/output/images_preprocessed/nan.pngze([1, 2048])\n",
      "Warning: Image file not found: ./datasets/iu_xray/output/images_preprocessed/nan.pngze([1, 2048])\n",
      "Warning: Image file not found: ./datasets/iu_xray/output/images_preprocessed/nan.pngze([1, 2048])\n",
      "Warning: Image file not found: ./datasets/iu_xray/output/images_preprocessed/nan.pngze([1, 2048])\n",
      "Warning: Image file not found: ./datasets/iu_xray/output/images_preprocessed/nan.pngze([1, 2048])\n",
      "Warning: Image file not found: ./datasets/iu_xray/output/images_preprocessed/nan.pngze([1, 2048])\n",
      "Warning: Image file not found: ./datasets/iu_xray/output/images_preprocessed/nan.pngze([1, 2048])\n",
      "Warning: Image file not found: ./datasets/iu_xray/output/images_preprocessed/nan.pngze([1, 2048])\n",
      "Warning: Image file not found: ./datasets/iu_xray/output/images_preprocessed/nan.pnge([1, 2048]))\n",
      "Warning: Image file not found: ./datasets/iu_xray/output/images_preprocessed/nan.pngze([1, 2048])\n",
      "Warning: Image file not found: ./datasets/iu_xray/output/images_preprocessed/nan.pnge([1, 2048]))\n",
      "Warning: Image file not found: ./datasets/iu_xray/output/images_preprocessed/nan.pngze([1, 2048])\n",
      "Warning: Image file not found: ./datasets/iu_xray/output/images_preprocessed/nan.pngze([1, 2048])\n",
      "Warning: Image file not found: ./datasets/iu_xray/output/images_preprocessed/nan.pngze([1, 2048])\n",
      "Warning: Image file not found: ./datasets/iu_xray/output/images_preprocessed/nan.pngze([1, 2048])\n",
      "Warning: Image file not found: ./datasets/iu_xray/output/images_preprocessed/nan.pngze([1, 2048])\n",
      "Warning: Image file not found: ./datasets/iu_xray/output/images_preprocessed/nan.pnge([1, 2048]))\n",
      "Warning: Image file not found: ./datasets/iu_xray/output/images_preprocessed/nan.pnge([1, 2048]))\n",
      "Warning: Image file not found: ./datasets/iu_xray/output/images_preprocessed/nan.pngze([1, 2048])\n",
      "Warning: Image file not found: ./datasets/iu_xray/output/images_preprocessed/nan.pnge([1, 2048]))\n",
      "Warning: Image file not found: ./datasets/iu_xray/output/images_preprocessed/nan.pngze([1, 2048])\n",
      "Warning: Image file not found: ./datasets/iu_xray/output/images_preprocessed/nan.pngze([1, 2048])\n",
      "Warning: Image file not found: ./datasets/iu_xray/output/images_preprocessed/nan.pngze([1, 2048])\n",
      "Warning: Image file not found: ./datasets/iu_xray/output/images_preprocessed/nan.pngze([1, 2048])\n",
      "Warning: Image file not found: ./datasets/iu_xray/output/images_preprocessed/nan.pnge([1, 2048]))\n",
      "Warning: Image file not found: ./datasets/iu_xray/output/images_preprocessed/nan.pnge([1, 2048]))\n",
      "Warning: Image file not found: ./datasets/iu_xray/output/images_preprocessed/nan.pngze([1, 2048])\n",
      "Warning: Image file not found: ./datasets/iu_xray/output/images_preprocessed/nan.pnge([1, 2048]))\n",
      "Warning: Image file not found: ./datasets/iu_xray/output/images_preprocessed/nan.pngze([1, 2048])\n",
      "Warning: Image file not found: ./datasets/iu_xray/output/images_preprocessed/nan.pngze([1, 2048])\n",
      "Warning: Image file not found: ./datasets/iu_xray/output/images_preprocessed/nan.pnge([1, 2048]))\n",
      "Warning: Image file not found: ./datasets/iu_xray/output/images_preprocessed/nan.pngze([1, 2048])\n",
      "Warning: Image file not found: ./datasets/iu_xray/output/images_preprocessed/nan.pnge([1, 2048]))\n",
      "Warning: Image file not found: ./datasets/iu_xray/output/images_preprocessed/nan.pngze([1, 2048])\n",
      "Warning: Image file not found: ./datasets/iu_xray/output/images_preprocessed/nan.pngze([1, 2048])\n",
      "Warning: Image file not found: ./datasets/iu_xray/output/images_preprocessed/nan.pnge([1, 2048])\n",
      "Warning: Image file not found: ./datasets/iu_xray/output/images_preprocessed/nan.pnge([1, 2048]))\n",
      "Warning: Image file not found: ./datasets/iu_xray/output/images_preprocessed/nan.pngze([1, 2048])\n",
      "Warning: Image file not found: ./datasets/iu_xray/output/images_preprocessed/nan.pngze([1, 2048])\n",
      "Warning: Image file not found: ./datasets/iu_xray/output/images_preprocessed/nan.pnge([1, 2048]))\n",
      "Warning: Image file not found: ./datasets/iu_xray/output/images_preprocessed/nan.pngze([1, 2048])\n",
      "Warning: Image file not found: ./datasets/iu_xray/output/images_preprocessed/nan.pngze([1, 2048])\n",
      "Warning: Image file not found: ./datasets/iu_xray/output/images_preprocessed/nan.pngze([1, 2048])\n",
      "Warning: Image file not found: ./datasets/iu_xray/output/images_preprocessed/nan.pngze([1, 2048])\n",
      "Warning: Image file not found: ./datasets/iu_xray/output/images_preprocessed/nan.pnge([1, 2048])\n",
      "Warning: Image file not found: ./datasets/iu_xray/output/images_preprocessed/nan.pngze([1, 2048])\n",
      "Warning: Image file not found: ./datasets/iu_xray/output/images_preprocessed/nan.pngze([1, 2048])\n",
      "Warning: Image file not found: ./datasets/iu_xray/output/images_preprocessed/nan.pnge([1, 2048]))\n",
      "Warning: Image file not found: ./datasets/iu_xray/output/images_preprocessed/nan.pngze([1, 2048])\n",
      "Warning: Image file not found: ./datasets/iu_xray/output/images_preprocessed/nan.pngze([1, 2048])\n",
      "Warning: Image file not found: ./datasets/iu_xray/output/images_preprocessed/nan.pnge([1, 2048]))\n",
      "Warning: Image file not found: ./datasets/iu_xray/output/images_preprocessed/nan.pnge([1, 2048]))\n",
      "Warning: Image file not found: ./datasets/iu_xray/output/images_preprocessed/nan.pngze([1, 2048])\n",
      "Warning: Image file not found: ./datasets/iu_xray/output/images_preprocessed/nan.pngze([1, 2048])\n",
      "Warning: Image file not found: ./datasets/iu_xray/output/images_preprocessed/nan.pngze([1, 2048])\n",
      "Warning: Image file not found: ./datasets/iu_xray/output/images_preprocessed/nan.pnge([1, 2048]))\n",
      "Warning: Image file not found: ./datasets/iu_xray/output/images_preprocessed/nan.pngze([1, 2048])\n",
      "Warning: Image file not found: ./datasets/iu_xray/output/images_preprocessed/nan.png([1, 2048])])\n",
      "Warning: Image file not found: ./datasets/iu_xray/output/images_preprocessed/nan.pngze([1, 2048])\n",
      "Warning: Image file not found: ./datasets/iu_xray/output/images_preprocessed/nan.pnge([1, 2048]))\n",
      "Warning: Image file not found: ./datasets/iu_xray/output/images_preprocessed/nan.pngze([1, 2048])\n",
      "Warning: Image file not found: ./datasets/iu_xray/output/images_preprocessed/nan.pngze([1, 2048])\n",
      "Warning: Image file not found: ./datasets/iu_xray/output/images_preprocessed/nan.pngze([1, 2048])\n",
      "Warning: Image file not found: ./datasets/iu_xray/output/images_preprocessed/nan.pngze([1, 2048])\n",
      "Warning: Image file not found: ./datasets/iu_xray/output/images_preprocessed/nan.pngze([1, 2048])\n",
      "Warning: Image file not found: ./datasets/iu_xray/output/images_preprocessed/nan.pngze([1, 2048])\n",
      "Warning: Image file not found: ./datasets/iu_xray/output/images_preprocessed/nan.pngze([1, 2048])\n",
      "Warning: Image file not found: ./datasets/iu_xray/output/images_preprocessed/nan.pngze([1, 2048])\n",
      "Warning: Image file not found: ./datasets/iu_xray/output/images_preprocessed/nan.pngze([1, 2048])\n",
      "Warning: Image file not found: ./datasets/iu_xray/output/images_preprocessed/nan.pnge([1, 2048]))\n",
      "Warning: Image file not found: ./datasets/iu_xray/output/images_preprocessed/nan.pngze([1, 2048])\n",
      "Warning: Image file not found: ./datasets/iu_xray/output/images_preprocessed/nan.pngze([1, 2048])\n",
      "Warning: Image file not found: ./datasets/iu_xray/output/images_preprocessed/nan.pngze([1, 2048])\n",
      "Warning: Image file not found: ./datasets/iu_xray/output/images_preprocessed/nan.pngze([1, 2048])\n",
      "Warning: Image file not found: ./datasets/iu_xray/output/images_preprocessed/nan.pngze([1, 2048])\n",
      "Warning: Image file not found: ./datasets/iu_xray/output/images_preprocessed/nan.pngze([1, 2048])\n",
      "Warning: Image file not found: ./datasets/iu_xray/output/images_preprocessed/nan.pngze([1, 2048])\n",
      "Warning: Image file not found: ./datasets/iu_xray/output/images_preprocessed/nan.pnge([1, 2048]))\n",
      "Warning: Image file not found: ./datasets/iu_xray/output/images_preprocessed/nan.pngze([1, 2048])\n",
      "Warning: Image file not found: ./datasets/iu_xray/output/images_preprocessed/nan.pngze([1, 2048])\n",
      "Warning: Image file not found: ./datasets/iu_xray/output/images_preprocessed/nan.pngze([1, 2048])\n",
      "Warning: Image file not found: ./datasets/iu_xray/output/images_preprocessed/nan.pngze([1, 2048])\n",
      "Warning: Image file not found: ./datasets/iu_xray/output/images_preprocessed/nan.pngze([1, 2048])\n",
      "Warning: Image file not found: ./datasets/iu_xray/output/images_preprocessed/nan.pnge([1, 2048]))\n",
      "Warning: Image file not found: ./datasets/iu_xray/output/images_preprocessed/nan.pngze([1, 2048])\n",
      "Warning: Image file not found: ./datasets/iu_xray/output/images_preprocessed/nan.pngze([1, 2048])\n",
      "Warning: Image file not found: ./datasets/iu_xray/output/images_preprocessed/nan.pngze([1, 2048])\n",
      "Warning: Image file not found: ./datasets/iu_xray/output/images_preprocessed/nan.pnge([1, 2048]))\n",
      "Warning: Image file not found: ./datasets/iu_xray/output/images_preprocessed/nan.pngze([1, 2048])\n",
      "Warning: Image file not found: ./datasets/iu_xray/output/images_preprocessed/nan.pnge([1, 2048]))\n",
      "Warning: Image file not found: ./datasets/iu_xray/output/images_preprocessed/nan.pnge([1, 2048])\n",
      "Warning: Image file not found: ./datasets/iu_xray/output/images_preprocessed/nan.pngze([1, 2048])\n",
      "Warning: Image file not found: ./datasets/iu_xray/output/images_preprocessed/nan.pngze([1, 2048])\n",
      "Warning: Image file not found: ./datasets/iu_xray/output/images_preprocessed/nan.pngze([1, 2048])\n",
      "Warning: Image file not found: ./datasets/iu_xray/output/images_preprocessed/nan.pnge([1, 2048]))\n",
      "Warning: Image file not found: ./datasets/iu_xray/output/images_preprocessed/nan.pngze([1, 2048])\n",
      "Warning: Image file not found: ./datasets/iu_xray/output/images_preprocessed/nan.pngze([1, 2048])\n",
      "Warning: Image file not found: ./datasets/iu_xray/output/images_preprocessed/nan.pngze([1, 2048])\n",
      "Warning: Image file not found: ./datasets/iu_xray/output/images_preprocessed/nan.pngze([1, 2048])\n",
      "Warning: Image file not found: ./datasets/iu_xray/output/images_preprocessed/nan.pnge([1, 2048]))\n",
      "Warning: Image file not found: ./datasets/iu_xray/output/images_preprocessed/nan.pnge([1, 2048]))\n",
      "Warning: Image file not found: ./datasets/iu_xray/output/images_preprocessed/nan.pngze([1, 2048])\n",
      "Warning: Image file not found: ./datasets/iu_xray/output/images_preprocessed/nan.pnge([1, 2048])\n",
      "Warning: Image file not found: ./datasets/iu_xray/output/images_preprocessed/nan.pngze([1, 2048])\n",
      "Warning: Image file not found: ./datasets/iu_xray/output/images_preprocessed/nan.pnge([1, 2048]))\n",
      "Warning: Image file not found: ./datasets/iu_xray/output/images_preprocessed/nan.pngze([1, 2048])\n",
      "Warning: Image file not found: ./datasets/iu_xray/output/images_preprocessed/nan.pnge([1, 2048]))\n",
      "Warning: Image file not found: ./datasets/iu_xray/output/images_preprocessed/nan.pngze([1, 2048])\n",
      "Warning: Image file not found: ./datasets/iu_xray/output/images_preprocessed/nan.pnge([1, 2048]))\n",
      "Warning: Image file not found: ./datasets/iu_xray/output/images_preprocessed/nan.pngze([1, 2048])\n",
      "Warning: Image file not found: ./datasets/iu_xray/output/images_preprocessed/nan.pnge([1, 2048])\n",
      "Warning: Image file not found: ./datasets/iu_xray/output/images_preprocessed/nan.pnge([1, 2048]))\n",
      "Warning: Image file not found: ./datasets/iu_xray/output/images_preprocessed/nan.pngze([1, 2048])\n",
      "Warning: Image file not found: ./datasets/iu_xray/output/images_preprocessed/nan.pngze([1, 2048])\n",
      "Warning: Image file not found: ./datasets/iu_xray/output/images_preprocessed/nan.pnge([1, 2048]))\n",
      "Warning: Image file not found: ./datasets/iu_xray/output/images_preprocessed/nan.pngze([1, 2048])\n",
      "Warning: Image file not found: ./datasets/iu_xray/output/images_preprocessed/nan.pnge([1, 2048]))\n",
      "Warning: Image file not found: ./datasets/iu_xray/output/images_preprocessed/nan.pngze([1, 2048])\n",
      "Warning: Image file not found: ./datasets/iu_xray/output/images_preprocessed/nan.pnge([1, 2048]))\n",
      "Warning: Image file not found: ./datasets/iu_xray/output/images_preprocessed/nan.pngze([1, 2048])\n",
      "Warning: Image file not found: ./datasets/iu_xray/output/images_preprocessed/nan.pngze([1, 2048])\n",
      "Warning: Image file not found: ./datasets/iu_xray/output/images_preprocessed/nan.pngze([1, 2048])\n",
      "Warning: Image file not found: ./datasets/iu_xray/output/images_preprocessed/nan.pngze([1, 2048])\n",
      "Warning: Image file not found: ./datasets/iu_xray/output/images_preprocessed/nan.pngze([1, 2048])\n",
      "Warning: Image file not found: ./datasets/iu_xray/output/images_preprocessed/nan.pngze([1, 2048])\n",
      "Warning: Image file not found: ./datasets/iu_xray/output/images_preprocessed/nan.pngze([1, 2048])\n",
      "Warning: Image file not found: ./datasets/iu_xray/output/images_preprocessed/nan.pnge([1, 2048]))\n",
      "Warning: Image file not found: ./datasets/iu_xray/output/images_preprocessed/nan.pngze([1, 2048])\n",
      "Warning: Image file not found: ./datasets/iu_xray/output/images_preprocessed/nan.pngze([1, 2048])\n",
      "Warning: Image file not found: ./datasets/iu_xray/output/images_preprocessed/nan.pngze([1, 2048])\n",
      "Warning: Image file not found: ./datasets/iu_xray/output/images_preprocessed/nan.pngze([1, 2048])\n",
      "Warning: Image file not found: ./datasets/iu_xray/output/images_preprocessed/nan.pngze([1, 2048])\n",
      "Warning: Image file not found: ./datasets/iu_xray/output/images_preprocessed/nan.pngze([1, 2048])\n",
      "Warning: Image file not found: ./datasets/iu_xray/output/images_preprocessed/nan.pngze([1, 2048])\n",
      "Warning: Image file not found: ./datasets/iu_xray/output/images_preprocessed/nan.pngze([1, 2048])\n",
      "Warning: Image file not found: ./datasets/iu_xray/output/images_preprocessed/nan.pngze([1, 2048])\n",
      "Warning: Image file not found: ./datasets/iu_xray/output/images_preprocessed/nan.pngze([1, 2048])\n",
      "Warning: Image file not found: ./datasets/iu_xray/output/images_preprocessed/nan.pngze([1, 2048])\n",
      "Warning: Image file not found: ./datasets/iu_xray/output/images_preprocessed/nan.pngze([1, 2048])\n",
      "Warning: Image file not found: ./datasets/iu_xray/output/images_preprocessed/nan.pngze([1, 2048])\n",
      "Warning: Image file not found: ./datasets/iu_xray/output/images_preprocessed/nan.pngze([1, 2048])\n",
      "Warning: Image file not found: ./datasets/iu_xray/output/images_preprocessed/nan.pnge([1, 2048])\n",
      "Warning: Image file not found: ./datasets/iu_xray/output/images_preprocessed/nan.pngze([1, 2048])\n",
      "Warning: Image file not found: ./datasets/iu_xray/output/images_preprocessed/nan.pngze([1, 2048])\n",
      "Warning: Image file not found: ./datasets/iu_xray/output/images_preprocessed/nan.pnge([1, 2048]))\n",
      "Warning: Image file not found: ./datasets/iu_xray/output/images_preprocessed/nan.pngze([1, 2048])\n",
      "Warning: Image file not found: ./datasets/iu_xray/output/images_preprocessed/nan.pngze([1, 2048])\n",
      "Warning: Image file not found: ./datasets/iu_xray/output/images_preprocessed/nan.pngze([1, 2048])\n",
      "Warning: Image file not found: ./datasets/iu_xray/output/images_preprocessed/nan.pngze([1, 2048])\n",
      "Warning: Image file not found: ./datasets/iu_xray/output/images_preprocessed/nan.pnge([1, 2048]))\n",
      "Warning: Image file not found: ./datasets/iu_xray/output/images_preprocessed/nan.pngze([1, 2048])\n",
      "Warning: Image file not found: ./datasets/iu_xray/output/images_preprocessed/nan.pnge([1, 2048]))\n",
      "Warning: Image file not found: ./datasets/iu_xray/output/images_preprocessed/nan.pngze([1, 2048])\n",
      "Warning: Image file not found: ./datasets/iu_xray/output/images_preprocessed/nan.pnge([1, 2048]))\n",
      "Warning: Image file not found: ./datasets/iu_xray/output/images_preprocessed/nan.pngze([1, 2048])\n",
      "Warning: Image file not found: ./datasets/iu_xray/output/images_preprocessed/nan.pngze([1, 2048])\n",
      "Warning: Image file not found: ./datasets/iu_xray/output/images_preprocessed/nan.pnge([1, 2048]))\n",
      "Warning: Image file not found: ./datasets/iu_xray/output/images_preprocessed/nan.png([1, 2048])])\n",
      "Warning: Image file not found: ./datasets/iu_xray/output/images_preprocessed/nan.pngze([1, 2048])\n",
      "Warning: Image file not found: ./datasets/iu_xray/output/images_preprocessed/nan.pngze([1, 2048])\n",
      "Warning: Image file not found: ./datasets/iu_xray/output/images_preprocessed/nan.pngze([1, 2048])\n",
      "Warning: Image file not found: ./datasets/iu_xray/output/images_preprocessed/nan.pngze([1, 2048])\n",
      "Warning: Image file not found: ./datasets/iu_xray/output/images_preprocessed/nan.pngze([1, 2048])\n",
      "Warning: Image file not found: ./datasets/iu_xray/output/images_preprocessed/nan.pngze([1, 2048])\n",
      "Warning: Image file not found: ./datasets/iu_xray/output/images_preprocessed/nan.pngze([1, 2048])\n",
      "Warning: Image file not found: ./datasets/iu_xray/output/images_preprocessed/nan.pngze([1, 2048])\n",
      "Warning: Image file not found: ./datasets/iu_xray/output/images_preprocessed/nan.pnge([1, 2048]))\n",
      "Warning: Image file not found: ./datasets/iu_xray/output/images_preprocessed/nan.pngze([1, 2048])\n",
      "Warning: Image file not found: ./datasets/iu_xray/output/images_preprocessed/nan.pngze([1, 2048])\n",
      "Warning: Image file not found: ./datasets/iu_xray/output/images_preprocessed/nan.pngze([1, 2048])\n",
      "Warning: Image file not found: ./datasets/iu_xray/output/images_preprocessed/nan.pngze([1, 2048])\n",
      "Warning: Image file not found: ./datasets/iu_xray/output/images_preprocessed/nan.pngze([1, 2048])\n",
      "Warning: Image file not found: ./datasets/iu_xray/output/images_preprocessed/nan.pngze([1, 2048])\n",
      "Warning: Image file not found: ./datasets/iu_xray/output/images_preprocessed/nan.pnge([1, 2048]))\n",
      "Warning: Image file not found: ./datasets/iu_xray/output/images_preprocessed/nan.pnge([1, 2048]))\n",
      "Warning: Image file not found: ./datasets/iu_xray/output/images_preprocessed/nan.pngze([1, 2048])\n",
      "Warning: Image file not found: ./datasets/iu_xray/output/images_preprocessed/nan.pnge([1, 2048]))\n",
      "Warning: Image file not found: ./datasets/iu_xray/output/images_preprocessed/nan.pngze([1, 2048])\n",
      "Warning: Image file not found: ./datasets/iu_xray/output/images_preprocessed/nan.pngze([1, 2048])\n",
      "Warning: Image file not found: ./datasets/iu_xray/output/images_preprocessed/nan.pngze([1, 2048])\n",
      "Warning: Image file not found: ./datasets/iu_xray/output/images_preprocessed/nan.pnge([1, 2048])\n",
      "Warning: Image file not found: ./datasets/iu_xray/output/images_preprocessed/nan.pngze([1, 2048])\n",
      "Warning: Image file not found: ./datasets/iu_xray/output/images_preprocessed/nan.pngze([1, 2048])\n",
      "Warning: Image file not found: ./datasets/iu_xray/output/images_preprocessed/nan.pnge([1, 2048]))\n",
      "Patch Features Shape: torch.Size([2700, 4096]), 2048]), Avg Features Shape: torch.Size([1, 2048])\n",
      "Average Features Shape: torch.Size([2700, 2048])\n",
      "Final Embedding Shape: torch.Size([2700, 6144])\n"
     ]
    }
   ],
   "source": [
    "'''Visual Extractor to Extract Data from Image and Encode it Accordingly'''\n",
    "\n",
    "# Define paths for saving features\n",
    "patch_feats_file = os.path.join(output_directory, 'patch_feats.pt')\n",
    "avg_feats_file = os.path.join(output_directory, 'avg_feats.pt')\n",
    "final_embeddings_file = os.path.join(output_directory, 'final_embeddings.pt')\n",
    "iu_xray_images_preprocessed = os.path.join(output_directory, \"images_preprocessed\")\n",
    "\n",
    "\n",
    "# Define the transform for image preprocessing\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)), \n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  \n",
    "])\n",
    "\n",
    "\n",
    "# Define VisualExtractor class\n",
    "class VisualExtractor(nn.Module):\n",
    "    def __init__(self, visual_extractor='resnet101', pretrained=True):\n",
    "        super(VisualExtractor, self).__init__()\n",
    "\n",
    "        model = getattr(models, visual_extractor)(pretrained=pretrained)\n",
    "        \n",
    "        # Remove the last fully connected layer\n",
    "        modules = list(model.children())[:-2]  \n",
    "        self.model = nn.Sequential(*modules)\n",
    "        \n",
    "        # Average pooling and a fully connected layer to transform the features\n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.fc_layer = nn.Linear(model.fc.in_features, 2048)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.model(x)\n",
    "        avg_feats = self.avg_pool(x).view(x.size(0), -1)\n",
    "        patch_feats = self.fc_layer(avg_feats)\n",
    "        return patch_feats, avg_feats\n",
    "\n",
    "    \n",
    "# Load images function that handles the new DataFrame structure\n",
    "def load_images(report_row, img_folder):\n",
    "    images = []\n",
    "    for i in range(1, 3):  # image_1 and image_2\n",
    "        image = str(report_row[f'image_{i}']).strip()\n",
    "        img_filename = image.split('.')[0] + '.png'  \n",
    "        img_path = os.path.join(img_folder, img_filename)\n",
    "        \n",
    "        # Check if the image file exists before attempting to open it\n",
    "        if os.path.exists(img_path):\n",
    "            img = Image.open(img_path).convert(\"RGB\")\n",
    "            images.append(transform(img))\n",
    "        else:\n",
    "            print(f\"Warning: Image file not found: {img_path}\")  # Warning if file not found\n",
    "\n",
    "    # If only one image was loaded, duplicate it\n",
    "    if len(images) == 1:\n",
    "        images.append(images[0].clone())  # Duplicate the single available image\n",
    "    \n",
    "    return torch.stack(images) if images else torch.tensor([])  # Return empty tensor if no images loaded\n",
    "\n",
    "\n",
    "# Initialize visual extractor\n",
    "visual_extractor = VisualExtractor()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "visual_extractor.to(device)\n",
    "\n",
    "\n",
    "# Set learning rates and other parameters\n",
    "learning_rate = 5e-5  \n",
    "other_parameter = 1e-4\n",
    "\n",
    "\n",
    "# Create an optimizer\n",
    "optimizer = optim.Adam(visual_extractor.parameters(), lr=learning_rate, weight_decay=other_parameter)\n",
    "\n",
    "\n",
    "# function to extract features\n",
    "def extract_features(data_loader):\n",
    "    patch_feats, avg_feats, final_embeddings = [], [], []\n",
    "    \n",
    "    for idx, row in data_loader.iterrows():\n",
    "        images = load_images(row, iu_xray_images_preprocessed).to(device)  # Load up to 2 images\n",
    "        \n",
    "        if images.numel() == 0:  # Skip if no images were loaded\n",
    "            continue\n",
    "        \n",
    "        report_patch_feats, report_avg_feats = [], []\n",
    "        \n",
    "        # Iterate directly over the images tensor\n",
    "        for image in images:  # Process each image in the loaded images tensor\n",
    "            image = image.unsqueeze(0).to(device)  # Move single image to device\n",
    "            pf, af = visual_extractor(image)  # Extract patch_feats, avg_feats\n",
    "            print(f\"{row['index']}, {row['pmc_id']} Patch Features Shape: {pf.shape}, Avg Features Shape: {af.shape}\", end=\"\\r\")\n",
    "            \n",
    "            # Store the output temporarily, releasing earlier references\n",
    "            report_patch_feats.append(pf.detach())\n",
    "            report_avg_feats.append(af.detach())\n",
    "\n",
    "        # Concatenate patch features and average the avg_feats\n",
    "        concatenated_patch_feats = torch.cat(report_patch_feats, dim=1)  # Concatenate along feature dimension\n",
    "        averaged_avg_feats = torch.mean(torch.stack(report_avg_feats), dim=0)  # Average the avg_feats\n",
    "\n",
    "        # Combine avg_feats with patch_feats to create final embedding\n",
    "        final_embedding = torch.cat((averaged_avg_feats, concatenated_patch_feats), dim=1)\n",
    "        patch_feats.append(concatenated_patch_feats)\n",
    "        avg_feats.append(averaged_avg_feats)\n",
    "        final_embeddings.append(final_embedding)\n",
    "        \n",
    "    return torch.stack(patch_feats), torch.stack(avg_feats), torch.stack(final_embeddings)\n",
    "\n",
    "\n",
    "# Functions to load and save features\n",
    "def load_features(file_path):\n",
    "    return torch.load(file_path)\n",
    "\n",
    "def save_features(file_path, features):\n",
    "    torch.save(features, file_path)\n",
    "\n",
    "    \n",
    "# Load data\n",
    "report_data_path = os.path.join(output_directory, 'filtered_train_data.csv')\n",
    "iu_xray_reports_df = pd.read_csv(report_data_path)\n",
    "display(iu_xray_reports_df.head(10))\n",
    "\n",
    "\n",
    "# Check if features are already saved; if not, extract and save them\n",
    "if not os.path.exists(patch_feats_file) and os.path.exists(avg_feats_file) and os.path.exists(final_embeddings_file):\n",
    "    print(\"All features are already precomputed and will be loaded.\")\n",
    "    patch_feats = load_features(patch_feats_file)\n",
    "    avg_feats = load_features(avg_feats_file)\n",
    "    final_embeddings = load_features(final_embeddings_file)\n",
    "else:\n",
    "    print(\"Extracting features since they are not precomputed...\")\n",
    "    \n",
    "    visual_extractor.train()  \n",
    "    \n",
    "    patch_feats, avg_feats, final_embeddings = extract_features(iu_xray_reports_df)\n",
    "    patch_feats = patch_feats.squeeze(1)  # Shape: (num_reports, 4096)\n",
    "    avg_feats = avg_feats.squeeze(1)      # Shape: (num_reports, 2048)\n",
    "    final_embeddings = final_embeddings.squeeze(1)  # Shape: (num_reports, 6144)\n",
    "\n",
    "    save_features(patch_feats_file, patch_feats)\n",
    "    save_features(avg_feats_file, avg_feats)\n",
    "    save_features(final_embeddings_file, final_embeddings)\n",
    "\n",
    "    \n",
    "# Displaying the shapes of the feature dataframes\n",
    "print(\"Patch Features Shape:\", patch_feats.shape)\n",
    "print(\"Average Features Shape:\", avg_feats.shape)\n",
    "print(\"Final Embedding Shape:\", final_embeddings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-06T01:12:16.862758Z",
     "iopub.status.busy": "2024-11-06T01:12:16.862290Z",
     "iopub.status.idle": "2024-11-06T01:12:16.907682Z",
     "shell.execute_reply": "2024-11-06T01:12:16.906668Z",
     "shell.execute_reply.started": "2024-11-06T01:12:16.862718Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Patch Features Shape: torch.Size([2700, 4096])\n",
      "Average Features Shape: torch.Size([2700, 2048])\n",
      "Final Embedding Shape: torch.Size([2700, 6144])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_30/2547253832.py:13: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  patch_feats = torch.load(patch_feats_file, map_location=torch.device('cpu'))\n",
      "/tmp/ipykernel_30/2547253832.py:20: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  avg_feats = torch.load(avg_feats_file, map_location=torch.device('cpu'))\n",
      "/tmp/ipykernel_30/2547253832.py:27: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  final_embeddings = torch.load(final_embeddings_file, map_location=torch.device('cpu'))\n"
     ]
    }
   ],
   "source": [
    "'''Displaying Tensor Shapes for All Embeddings'''\n",
    "\n",
    "# Define paths for the saved features\n",
    "patch_feats_file = os.path.join(output_directory, 'patch_feats.pt')\n",
    "avg_feats_file = os.path.join(output_directory, 'avg_feats.pt')\n",
    "final_embeddings_file = os.path.join(output_directory, 'final_embeddings.pt')\n",
    "\n",
    "\n",
    "# Function to load features and print their shapes\n",
    "def print_tensor_shapes():\n",
    "    # Load patch features\n",
    "    if os.path.exists(patch_feats_file):\n",
    "        patch_feats = torch.load(patch_feats_file, map_location=torch.device('cpu'))\n",
    "        print(\"Patch Features Shape:\", patch_feats.shape)\n",
    "    else:\n",
    "        print(f\"Patch features file not found: {patch_feats_file}\")\n",
    "\n",
    "    # Load average features\n",
    "    if os.path.exists(avg_feats_file):\n",
    "        avg_feats = torch.load(avg_feats_file, map_location=torch.device('cpu'))\n",
    "        print(\"Average Features Shape:\", avg_feats.shape)\n",
    "    else:\n",
    "        print(f\"Average features file not found: {avg_feats_file}\")\n",
    "\n",
    "    # Load final embeddings\n",
    "    if os.path.exists(final_embeddings_file):\n",
    "        final_embeddings = torch.load(final_embeddings_file, map_location=torch.device('cpu'))\n",
    "        print(\"Final Embedding Shape:\", final_embeddings.shape)\n",
    "    else:\n",
    "        print(f\"Final embeddings file not found: {final_embeddings_file}\")\n",
    "\n",
    "        \n",
    "# Call the function to print shapes\n",
    "print_tensor_shapes()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\"><b>>>>>>>> remote</b></span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D2kBZTiCMmsE"
   },
   "source": [
    "### **Text Encoder**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Text Encoder'''\n",
    "\n",
    "# function to embed text\n",
    "def embed_text(text_dataloader, model):\n",
    "    all_embeddings = []\n",
    "    \n",
    "    try:\n",
    "        for batch in text_dataloader:\n",
    "            if isinstance(batch, str):\n",
    "                inputs = tokenizer([batch], return_tensors=\"pt\", padding=True, truncation=True).to(device)\n",
    "            elif isinstance(batch, list):\n",
    "                inputs = tokenizer(batch, return_tensors=\"pt\", padding=True, truncation=True).to(device)\n",
    "            else:\n",
    "                raise ValueError(\"Batch must be of type str or List[str]\")\n",
    "\n",
    "            outputs = model(**inputs)\n",
    "            embeddings = outputs.last_hidden_state.mean(dim=1)\n",
    "            all_embeddings.append(embeddings)\n",
    "\n",
    "        if all_embeddings: \n",
    "            return torch.cat(all_embeddings, dim=0)\n",
    "        else:\n",
    "            if isinstance(model, SentenceTransformer):\n",
    "                return torch.empty(0, model.get_sentence_embedding_dimension()).to(device)\n",
    "            else:\n",
    "                return torch.empty(0, model.config.hidden_size).to(device)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error in embedding: {e}\")\n",
    "        if isinstance(model, SentenceTransformer):\n",
    "            return torch.empty(0, model.get_sentence_embedding_dimension()).to(device)\n",
    "        else:\n",
    "            return torch.empty(0, model.config.hidden_size).to(device)\n",
    "\n",
    "\n",
    "# function to compute cosine similarity\n",
    "def compute_cosine_similarity(embeddings1, embeddings2):\n",
    "    embeddings1 = F.normalize(embeddings1, p=2, dim=1)\n",
    "    embeddings2 = F.normalize(embeddings2, p=2, dim=1)\n",
    "    cosine = torch.mm(embeddings1, embeddings2.t())\n",
    "    return cosine\n",
    "\n",
    "\n",
    "# defining device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "\n",
    "# loading tokenizer, bert model and sentence-bert model\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "bert_model = BertModel.from_pretrained('bert-base-uncased').to(device)\n",
    "sentence_model = SentenceTransformer('all-MiniLM-L6-v2').to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Text Encoder - Medical Knowledge encoder using TextEncoder'''\n",
    "\n",
    "# defining radiology dictionary\n",
    "radiology_dictionary = {\n",
    "    \"pleural\": ['hemithorax', 'effusion', 'pneumothorax', 'parenchymal'],\n",
    "    \"lung\": ['lungs', 'pulmonary', 'hilar', 'lobe', 'consolidation', 'atelectasis', 'edema', 'opacity', 'pneumonia'],\n",
    "    \"mediastinal\": ['mediastinum', 'diaphragm', 'hemidiaphragm'],\n",
    "    \"cardiac\": ['heart', 'cardiomegaly', 'cardiomediastinal', 'atrium', 'ventricle', 'retrocardiac'],\n",
    "    \"vascular\": ['aorta', 'venous', 'jugular', 'aortic', 'vasculature', 'cabg'],\n",
    "    \"osseous\": ['rib', 'sternal', 'subclavian', 'thoracic'],\n",
    "    \"trachea\": ['endotrachea'],\n",
    "    \"stomach\": [],\n",
    "    \"abdomen\": [],\n",
    "    \"tube\": ['clips'],\n",
    "    \"spine\": ['vertebral', 'degenerative'],\n",
    "    \"nodule\": ['mass'],\n",
    "    \"chest\": ['small', 'enlarged', 'unchanged', 'stable', 'silhouette', 'contours', 'size', 'focal', 'mild', 'acute']\n",
    "}\n",
    "\n",
    "\n",
    "# define the filename and full path\n",
    "filename = 'radiology_terms.csv'\n",
    "dictionary_csv = os.path.join(output_directory, filename) \n",
    "\n",
    "\n",
    "# Open the CSV file in write mode using the full path\n",
    "if not os.path.exists(dictionary_csv):    \n",
    "    with open(dictionary_csv, 'w', newline='') as file:  \n",
    "        writer = csv.writer(file)  \n",
    "        writer.writerow(['Category', 'Term']) \n",
    "    \n",
    "        for category, terms in radiology_dictionary.items():\n",
    "            for term in terms:\n",
    "                if term: \n",
    "                    writer.writerow([category, term]) \n",
    "                    \n",
    "    print(f\"Dictionary saved to {dictionary_csv}\")  \n",
    "else:     \n",
    "    print(f\"File already exists at {dictionary_csv}. No changes made.\")\n",
    "\n",
    "text_list = []\n",
    "with open(dictionary_csv, 'r') as file:\n",
    "    reader = csv.reader(file)\n",
    "    next(reader) \n",
    "    for row in reader:\n",
    "        text_list.append(row[1]) \n",
    "\n",
    "        \n",
    "# load the SentenceTransformer model\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# Generate embeddings\n",
    "embeddings = model.encode(text_list)\n",
    "\n",
    "# Display the shape of the embeddings\n",
    "print(f\"Embeddings Shape: {embeddings.shape}\")\n",
    "\n",
    "dictionary_pt = os.path.join(output_directory, 'dictionary_embeddings.pt')\n",
    "if not os.path.exists(dictionary_pt):    \n",
    "    torch.save(embeddings, dictionary_pt)\n",
    "    print(f\"Saved at : {dictionary_pt}\")\n",
    "else : print(f\"Already saved at path : {dictionary_pt}\")\n",
    "\n",
    "    \n",
    "# # embedding dictionary and reports using bert, and historical medical reports using sentence transformer\n",
    "# dictionary_dataloader = load_preprocessed_texts(radiology_dictionary, tokenizer)\n",
    "# dictionary_embeddings = embed_text(dictionary_dataloader, bert_model)\n",
    "\n",
    "\n",
    "# # saving embeddings\n",
    "# embeddings_file_path = os.path.join(output_directory, 'dictionary_embeddings.pt')\n",
    "# if not os.path.exists(embeddings_file_path):\n",
    "#     torch.save(dictionary_embeddings.cpu(), embeddings_file_path)\n",
    "#     print(f\"Dictionary embeddings saved to {embeddings_file_path}\")\n",
    "\n",
    "\n",
    "# # displaying shape of embeddings\n",
    "# print(f\"Dictionary Embeddings Shape: {dictionary_embeddings.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# '''Text Encoder - Medical History encoding using sentence encoder'''\n",
    "\n",
    "# # reading and preprocessing medical history reports\n",
    "# iu_xray_reports_preprocessed_df_path = os.path.join(output_directory, 'iu_xray_reports_preprocessed_df.csv')\n",
    "# medical_history = pd.read_csv(iu_xray_reports_preprocessed_df_path)[\"findings\"].dropna().tolist()\n",
    "# medical_history = [str(report) for report in medical_history if isinstance(report, str) or pd.notna(report)]\n",
    "\n",
    "# batch_size = 32  # Set your desired batch size\n",
    "# tokenizer = AutoTokenizer.from_pretrained('all-MiniLM-L6-v2')  # Ensure you have your tokenizer defined\n",
    "\n",
    "# # Load the data into a DataLoader\n",
    "# medical_reports_dataloader = load_preprocessed_texts(medical_history, tokenizer, batch_size)\n",
    "\n",
    "# # Step 3: Print the shapes of the batches\n",
    "# for batch in medical_reports_dataloader:\n",
    "#     # Printing shapes of input tensors\n",
    "#     print({key: tensor.shape for key, tensor in batch.items()})\n",
    "#     break  # Remove this break to see all batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# '''Text Encoder -  Sentence-Bert Encoder using Medical History'''\n",
    "\n",
    "# # reading and preprocessing medical history reports\n",
    "# iu_xray_reports_preprocessed_df_path = os.path.join(output_directory, 'iu_xray_reports_preprocessed_df.csv')\n",
    "# medical_history = pd.read_csv(iu_xray_reports_preprocessed_df_path)[\"findings\"].dropna().tolist()\n",
    "# medical_history = [str(report) for report in medical_history if isinstance(report, str) or pd.notna(report)]\n",
    "\n",
    "# invalid_entries = [report for report in medical_history if not isinstance(report, (str, list))]\n",
    "\n",
    "# # Print if there are any invalid entries\n",
    "# if invalid_entries:\n",
    "#     print(f\"Found {len(invalid_entries)} invalid entries: {invalid_entries}\")\n",
    "# else:\n",
    "#     print(\"No invalid entries found.\")\n",
    "\n",
    "# # encoding medical history using Sentence-BERT\n",
    "# bert_model = BertModel.from_pretrained('bert-base-uncased').to(device)\n",
    "# tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "# historical_dataloader_b = load_preprocessed_texts(medical_history, tokenizer)\n",
    "# historical_embeddings_b = embed_text(historical_dataloader, bert_model)\n",
    "\n",
    "# # # Check for invalid entries after batching\n",
    "# # invalid_entries_after_batches = [\n",
    "# #     report for batch in historical_dataloader_b for report in batch.values() if not isinstance(report, (str, list))\n",
    "# # ]\n",
    "\n",
    "# # for batch in historical_dataloader:\n",
    "# #     print(type(batch))  # Check the type\n",
    "# #     print(batch)  # Print the batch content\n",
    "# #     break\n",
    "\n",
    "\n",
    "\n",
    "# # saving embeddings\n",
    "# historical_pt_b = os.path.join(output_directory, 'historical_embeddings_b.pt')\n",
    "# if not os.path.exists(historical_pt_b):\n",
    "#     torch.save(historical_embeddings_b.cpu(), historical_pt_b)\n",
    "#     print(f\"Historical embeddings saved to {historical_pt_b}\")\n",
    "# else : print(f\"Already at {historical_pt_b}\")\n",
    "\n",
    "\n",
    "# # displaying shape of embeddings\n",
    "# print(f\"Historical Embeddings Shape: {historical_embeddings_b.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# '''Text Encoder - Finding Reports Similar to Current Report'''\n",
    "\n",
    "# # current report embedding\n",
    "# current_report = 'Heart size pulmonary vascularity appear within normal limits mild tortuosity descending thoracic aorta lungs free focal airspace disease pleural effusion pneumothorax seen discrete nodules adenopathy noted degenerative changes present spine'\n",
    "# current_embeddings = embed_text(current_report, sentence_model)\n",
    "\n",
    "\n",
    "# # computing cosine similarity\n",
    "# historical_embeddings_normalized = historical_embeddings / historical_embeddings.norm(dim=1, keepdim=True)\n",
    "# current_embeddings_normalized = current_embeddings / current_embeddings.norm(dim=1, keepdim=True)\n",
    "\n",
    "# similarity_matrix = compute_cosine_similarity(dictionary_embeddings, historical_embeddings)\n",
    "# print(f\"Similarity Matrix Shape: {similarity_matrix.shape}\")\n",
    "\n",
    "\n",
    "# # finding top-k relevant entries\n",
    "# k = 5\n",
    "# top_k_indices = similarity_matrix.topk(k=k, dim=1).indices\n",
    "\n",
    "\n",
    "# # preparing relevant entries based on indices\n",
    "# relevant_entries = []\n",
    "# for row in top_k_indices:\n",
    "#     relevant_entries.append(medical_history[row.item()])\n",
    "\n",
    "\n",
    "# # printing relevant entries for each report\n",
    "# print(f\"Relevant Entries for the Current Report: {relevant_entries}\")\n",
    "\n",
    "\n",
    "# # update the historical embeddingx\n",
    "# historical_embeddings.append(current_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import pandas as pd\n",
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# import torch.nn.functional as F\n",
    "# from torch.utils.data import DataLoader, Dataset\n",
    "# from transformers import BertTokenizer, BertModel\n",
    "# import pickle\n",
    "\n",
    "# # Check if GPU is available and set the device accordingly\n",
    "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# # Function to convert findings to list of lists\n",
    "# def get_findings_list(df):\n",
    "#     findings_list = df['findings'].fillna(\"\").tolist()  # Replace NaN with empty string\n",
    "#     return [[finding] for finding in findings_list]  # Convert each finding into a list of strings\n",
    "\n",
    "# # Dataset class to load the findings from the reports\n",
    "# class FindingsDataset(Dataset):\n",
    "#     def __init__(self, findings_list):\n",
    "#         self.findings_list = findings_list\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return len(self.findings_list)\n",
    "\n",
    "#     def __getitem__(self, idx):\n",
    "#         return self.findings_list[idx]\n",
    "\n",
    "# # Define the Sentence Encoder (trainable)\n",
    "# class SentenceEncoder(nn.Module):\n",
    "#     def __init__(self, hidden_size=768):\n",
    "#         super(SentenceEncoder, self).__init__()\n",
    "#         self.encoder = nn.Linear(hidden_size, hidden_size)\n",
    "    \n",
    "#     def forward(self, x):\n",
    "#         return self.encoder(x)\n",
    "\n",
    "# # Cosine similarity loss based on the image equation\n",
    "# def cosine_similarity_loss(H, H_b):\n",
    "#     cos_sim_H = F.cosine_similarity(H, H.unsqueeze(1))\n",
    "#     cos_sim_H_b = F.cosine_similarity(H_b, H_b.unsqueeze(1))\n",
    "#     loss = torch.mean((cos_sim_H_b - cos_sim_H) ** 2)\n",
    "#     return loss\n",
    "\n",
    "# # Load pre-trained BERT model and tokenizer\n",
    "# tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "# bert_model = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# # Move the model to the appropriate device\n",
    "# bert_model.to(device)\n",
    "\n",
    "# # Load your dataframe\n",
    "# # iu_xray_reports_preprocessed_df_path = '/kaggle/input/preprocessed-text/iu_xray_reports_df_preprocessed.csv'\n",
    "# iu_xray_reports_preprocessed_df_path = os.path.join(output_directory, 'iu_xray_reports_preprocessed_df.csv')\n",
    "# medical_history = pd.read_csv(iu_xray_reports_preprocessed_df_path)\n",
    "# findings_list = get_findings_list(medical_history)\n",
    "\n",
    "# # Custom collate function to tokenize the findings in batches\n",
    "# def collate_fn(batch):\n",
    "#     batch = [item[0] for item in batch]  # Flatten the batch list\n",
    "#     return tokenizer(batch, padding=True, truncation=True, return_tensors=\"pt\").to(device)  # Move to GPU\n",
    "\n",
    "# # Create dataset and dataloader\n",
    "# findings_dataset = FindingsDataset(findings_list)\n",
    "# dataloader = DataLoader(findings_dataset, batch_size=16, collate_fn=collate_fn)\n",
    "\n",
    "# # Initialize Sentence Encoder and move to device\n",
    "# sentence_encoder = SentenceEncoder().to(device)\n",
    "\n",
    "# # Historical Knowledge Storage\n",
    "# #historical_knowledge_file = 'historical_knowledge.pkl'  # File to save the historical encodings\n",
    "# historical_knowledge = []  # List to store historical encodings\n",
    "\n",
    "# historical_knowledge_file = os.path.join(output_directory, 'historical_knowledge.pkl')\n",
    "# # if not os.path.exists(historical_pt_b):\n",
    "# #     torch.save(historical_embeddings_b.cpu(), historical_pt_b)\n",
    "# #     print(f\"Historical embeddings saved to {historical_pt_b}\")\n",
    "\n",
    "# # Load historical knowledge if it exists\n",
    "# if os.path.exists(historical_knowledge_file):\n",
    "#     with open(historical_knowledge_file, 'rb') as f:\n",
    "#         historical_knowledge = pickle.load(f)\n",
    "\n",
    "# # Optimizer\n",
    "# optimizer = torch.optim.Adam(sentence_encoder.parameters(), lr=1e-4)\n",
    "# num_epochs = 10  # Set the number of epochs\n",
    "\n",
    "# # Training loop\n",
    "# for epoch in range(num_epochs):\n",
    "#     for batch in dataloader:\n",
    "#         input_ids = batch['input_ids'].to(device)  # Move input_ids to GPU\n",
    "#         attention_mask = batch['attention_mask'].to(device)  # Move attention_mask to GPU\n",
    "\n",
    "#         # Get historical encodings from BERT\n",
    "#         with torch.no_grad():\n",
    "#             bert_output = bert_model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "#             H_b = bert_output.last_hidden_state.mean(dim=1).to(device)  # Average pooling of BERT embeddings and move to GPU\n",
    "\n",
    "#         # Get encodings from the trainable Sentence Encoder\n",
    "#         H = sentence_encoder(H_b)  # Feeding the pre-trained embeddings to the trainable encoder\n",
    "\n",
    "#         # final_H = H.detach().cpu()\n",
    "#         # Store the current encodings in historical knowledge\n",
    "#         historical_knowledge.append(H.detach().cpu())  # Detach tensor and move to CPU\n",
    "\n",
    "#         # Calculate cosine similarity loss\n",
    "#         loss = cosine_similarity_loss(H, H_b)\n",
    "\n",
    "#         # Backpropagation and optimization\n",
    "#         optimizer.zero_grad()\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "\n",
    "#     print(f\"Epoch {epoch + 1}, Loss: {loss.item()}\")\n",
    "\n",
    "# # final_H_file = 'final_H.pkl'\n",
    "# # with open(final_H_file, 'wb') as f:\n",
    "# #     pickle.dump(final_H.numpy(), f)\n",
    "\n",
    "\n",
    "# # Now you can utilize historical_knowledge for further inference or report generation.\n",
    "# with open(historical_knowledge_file, 'wb') as f:\n",
    "#     # Convert tensors to NumPy arrays before saving\n",
    "#     historical_knowledge_np = [h.numpy() for h in historical_knowledge]\n",
    "#     pickle.dump(historical_knowledge_np, f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Sentence Encoder**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Sentence Encoder - Classes'''\n",
    "\n",
    "# class for Sentence Encoder\n",
    "class SentenceEncoder(nn.Module):\n",
    "    def __init__(self, input_dim=768, hidden_dim=512, output_dim=512):\n",
    "        super(SentenceEncoder, self).__init__()\n",
    "        # Encoder network\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(hidden_dim, hidden_dim)\n",
    "        )\n",
    "        \n",
    "        # Networks to generate mean and variance as mentioned in paper\n",
    "        self.mean_layer = nn.Linear(hidden_dim, output_dim)\n",
    "        self.logvar_layer = nn.Linear(hidden_dim, output_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        encoded = self.encoder(x)\n",
    "        mean = self.mean_layer(encoded)\n",
    "        logvar = self.logvar_layer(encoded)\n",
    "        return mean, logvar\n",
    "        \n",
    "    def encode(self, x):\n",
    "        \"\"\"Get only the mean for inference\"\"\"\n",
    "        mean, _ = self.forward(x)\n",
    "        return mean\n",
    "\n",
    "\n",
    "# class for Sentence BERT\n",
    "class SentenceBERT:\n",
    "    def __init__(self):\n",
    "        # Initialize the BERT tokenizer and model\n",
    "        self.tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "        self.bert_model = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "    def encode_reports(self, reports):\n",
    "        embeddings = []\n",
    "        for report in reports:\n",
    "            # Tokenize and encode the report\n",
    "            inputs = self.tokenizer(report, return_tensors='pt', padding=True, truncation=True)\n",
    "\n",
    "            # Get BERT outputs\n",
    "            with torch.no_grad():  # Disable gradient calculation for inference\n",
    "                outputs = self.bert_model(**inputs)\n",
    "                \n",
    "            # Use the pooled output as the embedding\n",
    "            embeddings.append(outputs.pooler_output)\n",
    "\n",
    "        # Stack all embeddings into a single tensor\n",
    "        return torch.stack(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Sentence Encoder - Generating Sentence Embeddings'''\n",
    "\n",
    "# initialising paths\n",
    "input_train_path = os.path.join(output_directory, 'filtered_train_data.csv')\n",
    "input_test_path = os.path.join(output_directory, 'filtered_test_data.csv')\n",
    "\n",
    "\n",
    "# reading csv files\n",
    "train_df = pd.read_csv(input_train_path)\n",
    "test_df = pd.read_csv(input_test_path)\n",
    "\n",
    "\n",
    "# initialize models\n",
    "sentence_bert = SentenceBERT()\n",
    "sentence_encoder = SentenceEncoder()\n",
    "\n",
    "\n",
    "# Parameters for batching\n",
    "df = train_df\n",
    "batch_size = 64\n",
    "num_batches = (len(df) + batch_size - 1) // batch_size  \n",
    "\n",
    "\n",
    "# Initialize matrices to store embeddings\n",
    "sbert_matrix = []\n",
    "sentence_encoder_matrix = []\n",
    "\n",
    "\n",
    "# Loop through batches with progress bar\n",
    "for i in tqdm(range(num_batches), desc=\"Processing Batches\", unit=\"batch\"):\n",
    "    batch_reports = df['findings'].iloc[i * batch_size:(i + 1) * batch_size].tolist()\n",
    "    \n",
    "    # Compute SBERT embeddings\n",
    "    sbert_embeddings = sentence_bert.encode_reports(batch_reports)\n",
    "    \n",
    "    # Compute Sentence Encoder embeddings\n",
    "    sentence_embeddings = sentence_encoder.encode(sbert_embeddings)\n",
    "    \n",
    "    # Store embeddings in the matrices\n",
    "    sbert_matrix.append(sbert_embeddings)\n",
    "    sentence_encoder_matrix.append(sentence_embeddings)\n",
    "\n",
    "\n",
    "# Stack the matrices to create final output tensors\n",
    "sbert_matrix = torch.cat(sbert_matrix, dim=0).squeeze(1)\n",
    "sentence_encoder_matrix = torch.cat(sentence_encoder_matrix, dim=0).squeeze(1)\n",
    "\n",
    "\n",
    "# Print shapes of the final matrices\n",
    "print(f\"SBERT Matrix Shape: {sbert_matrix.shape}\")\n",
    "print(f\"Sentence Encoder Matrix Shape: {sentence_encoder_matrix.shape}\")\n",
    "\n",
    "\n",
    "# define file paths for each embedding\n",
    "embeddings_dir = output_directory\n",
    "sentence_encoder_embeddings_path = os.path.join(embeddings_dir, \"sentence_encoder_embeddings.pt\")\n",
    "sbert_embeddings_path = os.path.join(embeddings_dir, \"sbert_embeddings.pt\")\n",
    "\n",
    "# save the embeddings\n",
    "torch.save(sentence_encoder_matrix, sentence_encoder_embeddings_path)\n",
    "torch.save(sbert_matrix, sbert_embeddings_path)\n",
    "print(\"Embeddings saved successfully.\")\n",
    "\n",
    "\n",
    "# load the embeddings\n",
    "loaded_sentence_encoder_matrix = torch.load(sentence_encoder_embeddings_path)\n",
    "loaded_sbert_matrix = torch.load(sbert_embeddings_path)\n",
    "print(\"Embeddings loaded successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Sentence Encoder - Training the Sentence Encoder using Sentence BERT'''\n",
    "\n",
    "# function to compute kl divergence loss\n",
    "def kl_divergence_loss(mean1: torch.Tensor, logvar1: torch.Tensor, mean2: torch.Tensor, logvar2: torch.Tensor) -> torch.Tensor:\n",
    "    normal1 = dist.Normal(mean1, torch.exp(0.5 * logvar1))  # Standard deviation is sqrt of variance\n",
    "    normal2 = dist.Normal(mean2, torch.exp(0.5 * logvar2))\n",
    "    kl_loss = dist.kl.kl_divergence(normal1, normal2).mean()\n",
    "    return kl_loss\n",
    "\n",
    "\n",
    "# function to train the model based on embeddings\n",
    "def train_embeddings(sentence_encoder, projection_layer, sentence_encoder_matrix, sbert_matrix, \n",
    "                    optimizer, batch_size, num_epochs):\n",
    "    \"\"\"\n",
    "    Training loop with explicit memory management and graph cleanup\n",
    "    \"\"\"\n",
    "    device = next(sentence_encoder.parameters()).device\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss = 0\n",
    "        sentence_encoder.train()\n",
    "        projection_layer.train()\n",
    "        \n",
    "        n_batches = sentence_encoder_matrix.size(0) // batch_size\n",
    "        \n",
    "        with tqdm(total=n_batches, desc=f\"Epoch {epoch + 1}/{num_epochs}\", unit=\"batch\") as pbar:\n",
    "            for i in range(0, sentence_encoder_matrix.size(0), batch_size):\n",
    "                try:\n",
    "                    # Clear gradients\n",
    "                    optimizer.zero_grad(set_to_none=True)  # More efficient than zero_grad()\n",
    "                    \n",
    "                    # Get batch data with detached copies\n",
    "                    batch_embedding1 = sentence_encoder_matrix[i:i+batch_size].to(device).detach().clone()\n",
    "                    batch_embedding2 = sbert_matrix[i:i+batch_size].to(device).detach().clone()\n",
    "                    \n",
    "                    # Project SBERT embeddings\n",
    "                    with torch.set_grad_enabled(True):\n",
    "                        batch_embedding2_projected = projection_layer(batch_embedding2)\n",
    "                        \n",
    "                        # Compute means and variances\n",
    "                        mean1 = batch_embedding1.mean(dim=0, keepdim=True)\n",
    "                        var1 = batch_embedding1.var(dim=0, keepdim=True, unbiased=False)\n",
    "                        mean2 = batch_embedding2_projected.mean(dim=0, keepdim=True)\n",
    "                        var2 = batch_embedding2_projected.var(dim=0, keepdim=True, unbiased=False)\n",
    "                        \n",
    "                        # KL divergence loss\n",
    "                        kl_loss = kl_divergence_loss(mean1, var1, mean2, var2)\n",
    "                        \n",
    "                        # Similarity loss\n",
    "                        similarities = F.cosine_similarity(batch_embedding1, batch_embedding2_projected, dim=1)\n",
    "                        sim_loss = 1 - similarities.mean()\n",
    "                        \n",
    "                        # Combined loss\n",
    "                        loss = kl_loss + sim_loss\n",
    "                    \n",
    "                    # Backward pass with no graph retention\n",
    "                    loss.backward(retain_graph=False)\n",
    "                    \n",
    "                    # Clip gradients\n",
    "                    torch.nn.utils.clip_grad_norm_(sentence_encoder.parameters(), max_norm=1.0)\n",
    "                    torch.nn.utils.clip_grad_norm_(projection_layer.parameters(), max_norm=1.0)\n",
    "                    \n",
    "                    # Optimizer step\n",
    "                    optimizer.step()\n",
    "                    \n",
    "                    # Update metrics\n",
    "                    current_loss = loss.item()\n",
    "                    total_loss += current_loss\n",
    "                    \n",
    "                    # Clean up tensors explicitly\n",
    "                    del batch_embedding1, batch_embedding2, batch_embedding2_projected\n",
    "                    del mean1, var1, mean2, var2\n",
    "                    del kl_loss, similarities, sim_loss, loss\n",
    "                    torch.cuda.empty_cache() if torch.cuda.is_available() else None\n",
    "                    \n",
    "                    # Update progress bar\n",
    "                    pbar.update(1)\n",
    "                    pbar.set_postfix(loss=f\"{current_loss:.4f}\")\n",
    "                    \n",
    "                except RuntimeError as e:\n",
    "                    print(f\"\\nError in batch {i//batch_size}:\")\n",
    "                    print(str(e))\n",
    "                    # Try to recover\n",
    "                    torch.cuda.empty_cache() if torch.cuda.is_available() else None\n",
    "                    continue\n",
    "        \n",
    "        # Compute average loss for the epoch\n",
    "        avg_loss = total_loss / n_batches\n",
    "        print(f\"\\nEpoch {epoch + 1}/{num_epochs}, Average Loss: {avg_loss:.4f}\")\n",
    "\n",
    "\n",
    "# Set up training\n",
    "batch_size = 64\n",
    "num_epochs = 20\n",
    "\n",
    "\n",
    "# Initialize optimizer with parameters from both models\n",
    "optimizer = optim.Adam([\n",
    "    {'params': sentence_encoder.parameters()},\n",
    "    {'params': projection_layer.parameters()}\n",
    "], lr=1e-3)\n",
    "\n",
    "\n",
    "# Move models to the same device if using GPU\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "    sentence_encoder = sentence_encoder.to(device)\n",
    "    projection_layer = projection_layer.to(device)\n",
    "    sentence_encoder_matrix = sentence_encoder_matrix.to(device)\n",
    "    sbert_matrix = sbert_matrix.to(device)\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "\n",
    "\n",
    "# Run training\n",
    "train_embeddings(\n",
    "    sentence_encoder=sentence_encoder,\n",
    "    projection_layer=projection_layer,\n",
    "    sentence_encoder_matrix=sentence_encoder_matrix,\n",
    "    sbert_matrix=sbert_matrix,\n",
    "    optimizer=optimizer,\n",
    "    batch_size=batch_size,\n",
    "    num_epochs=num_epochs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Sentence Encoder - Compute Similarity Scores'''\n",
    "\n",
    "# function to compute cosine similarity\n",
    "def compute_cosine_similarities(batch_embeddings: torch.Tensor, all_embeddings: torch.Tensor) -> torch.Tensor:\n",
    "    # Normalize embeddings\n",
    "    batch_norm = F.normalize(batch_embeddings, p=2, dim=1)\n",
    "    all_norm = F.normalize(all_embeddings, p=2, dim=1)\n",
    "    \n",
    "    # Compute similarities\n",
    "    similarities = torch.mm(batch_norm, all_norm.t())\n",
    "    return similarities\n",
    "\n",
    "\n",
    "# Compute similarity matrices\n",
    "sbert_similarities = compute_cosine_similarities(sbert_matrix, sbert_matrix)\n",
    "sentence_encoder_similarities = compute_cosine_similarities(sentence_encoder_matrix, sentence_encoder_matrix)\n",
    "\n",
    "\n",
    "# Print similarity matrix shapes\n",
    "print(f\"SBERT Similarity Matrix Shape: {sbert_similarities.shape}\")  # Should be [2705, 2705]\n",
    "print(f\"Sentence Encoder Similarity Matrix Shape: {sentence_encoder_similarities.shape}\")  # Should be [2705, 2705]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Sentence Encoder - Saving/Loading our Model'''\n",
    "\n",
    "# define model path\n",
    "model_path = os.path.join(output_directory, \"model.pt\")  \n",
    "\n",
    "\n",
    "# save `sentence_encoder` model instance\n",
    "torch.save(sentence_encoder.state_dict(), model_path)\n",
    "print(f\"Model saved to {model_path}\")\n",
    "\n",
    "\n",
    "# load model instance\n",
    "sentence_encoder = SentenceEncoder()\n",
    "sentence_encoder.load_state_dict(torch.load(model_path))\n",
    "sentence_encoder.eval()\n",
    "print(\"Model loaded successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Sentence Encoder - Finding Top K Similar Reports for each Report'''\n",
    "\n",
    "# function to compute similarity score\n",
    "def compute_similarity_score(sim1, sim2):\n",
    "    return torch.mean((sim1 - sim2) ** 2)\n",
    "\n",
    "\n",
    "# function to find top k similar reports\n",
    "def find_top_k_similar_reports(custom_similarities, sbert_similarities, k):\n",
    "    num_reports = custom_similarities.shape[0]\n",
    "    \n",
    "    similarity_scores = []\n",
    "\n",
    "    for i in range(num_reports):\n",
    "        for j in range(i + 1, num_reports):\n",
    "            custom_score = compute_similarity_score(custom_similarities[i], custom_similarities[j])\n",
    "            sbert_score = compute_similarity_score(sbert_similarities[i], sbert_similarities[j])\n",
    "            combined_score = (custom_score**2 - sbert_score**2)\n",
    "\n",
    "            similarity_scores.append((combined_score.item(), i, j))\n",
    "\n",
    "    similarity_scores.sort(key=lambda x: x[0], reverse=True)\n",
    "\n",
    "    return similarity_scores[:k]\n",
    "\n",
    "\n",
    "# find top k similar reports for each report \n",
    "k_most_similar_reports = find_top_k_similar_reports(sentence_encoder_similarities, sbert_similarities, 7)\n",
    "\n",
    "\n",
    "# store and display the results\n",
    "ans = []\n",
    "for report_id, data in k_most_similar_reports.items():\n",
    "    ans.append(data['most_similar_reports'])\n",
    "    print(data['most_similar_reports'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Sentence Encoder - Finding Top N Similar Indices'''\n",
    "\n",
    "# convert to numpy array\n",
    "ans = np.array(ans)\n",
    "print(type(ans))\n",
    "\n",
    "\n",
    "# function to return n similar indexes\n",
    "def return_n_sim_indexes(a, n):\n",
    "    indexes = []\n",
    "    flattened_array = a.flatten()\n",
    "    count = Counter(flattened_array)\n",
    "    most_common_numbers = count.most_common(n)\n",
    "    indexes = [num for num, _ in most_common_numbers]\n",
    "    \n",
    "    return indexes\n",
    "\n",
    "\n",
    "# display n similar indexes\n",
    "print(return_n_sim_indexes(ans, 7))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gIYzXGtsMqmQ"
   },
   "source": [
    "### **Multilevel Alignment**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Multilevel Alignment based on BLIP Architecture'''\n",
    "\n",
    "# function to extract image embeddings of a given file name\n",
    "def extract_image_embeddings(image_name):\n",
    "    image_embeddings = torch.load(output_directory + \"image_embeddings.pt\")\n",
    "    if image_name in image_embeddings:\n",
    "        return image_embeddings[image_name]\n",
    "    else:\n",
    "        return None\n",
    "    \n",
    "\n",
    "# function to extract text embeddings of a given text\n",
    "def extract_historical_text_embeddings(text):\n",
    "    historical_text_embeddings = torch.load(output_directory + \"historical_embeddings.pt\")\n",
    "    if text in historical_text_embeddings:\n",
    "        return historical_text_embeddings[text]\n",
    "    else :\n",
    "        return None\n",
    "\n",
    "def extract_dictionary_text_embeddings(text):\n",
    "    dictionary_text_embeddings = torch.load(output_directory + \"dictionary_embeddings.pt\")\n",
    "    if text in dictionary_text_embeddings:\n",
    "        return dictionary_text_embeddings[text]\n",
    "    else :\n",
    "        return None\n",
    "    \n",
    "\n",
    "# function to find cosine similarity between a text and an image \n",
    "def cosine_similarity(embedding1, embedding2):\n",
    "    embedding1 = np.array(embedding1)\n",
    "    embedding2 = np.array(embedding2)\n",
    "\n",
    "    dot_product = np.dot(embedding1, embedding2)\n",
    "    norm_embedding1 = np.linalg.norm(embedding1)\n",
    "    norm_embedding2 = np.linalg.norm(embedding2)\n",
    "    \n",
    "    if norm_embedding1 == 0 or norm_embedding2 == 0:\n",
    "        return None \n",
    "    \n",
    "    return dot_product / (norm_embedding1 * norm_embedding2)\n",
    "    \n",
    "\n",
    "# function to compute Image-Text-Contrastive Loss\n",
    "def batch_itc_loss(image_embeddings, text_embeddings, temperature=0.1):\n",
    "    image_embeddings = F.normalize(image_embeddings, dim=1)\n",
    "    text_embeddings = F.normalize(text_embeddings, dim=1)\n",
    "\n",
    "    similarity_matrix = torch.matmul(image_embeddings, text_embeddings.T) / temperature\n",
    "\n",
    "    labels = torch.arange(len(image_embeddings), device=image_embeddings.device)\n",
    "\n",
    "    loss = F.cross_entropy(similarity_matrix, labels)\n",
    "    return labels, loss\n",
    "\n",
    "\n",
    "# function to compute Image-Text-Matching loss\n",
    "def batch_itm_loss(image_embeddings, text_embeddings, match_labels):\n",
    "    logits = torch.matmul(image_embeddings, text_embeddings.t())\n",
    "    probabilities = torch.sigmoid(logits) \n",
    "    positive_probs = probabilities[torch.arange(len(match_labels)), match_labels]\n",
    "\n",
    "    loss = -torch.log(positive_probs + 1e-12).mean()\n",
    "    return loss\n",
    "\n",
    "\n",
    "# function to get embeddings()\n",
    "def get_embeddings(file_path):\n",
    "    df = pd.read_csv(file_path)\n",
    "    report_embeddings = []\n",
    "    image_embeddings = []\n",
    "\n",
    "    for _, row in df.iterrows():\n",
    "        report_embedding = extract_historical_text_embeddings(row['findings'])\n",
    "\n",
    "        for i in range(1, 6):\n",
    "            image_col = f'image_{i}'\n",
    "            if image_col in row and pd.notnull(row[image_col]):\n",
    "                image_embedding = extract_image_embeddings(row[image_col])\n",
    "                report_embeddings.append(report_embedding)\n",
    "                image_embeddings.append(image_embedding)\n",
    "\n",
    "    return report_embeddings, image_embeddings\n",
    "\n",
    "\n",
    "# function to do training step\n",
    "def train_step(file, optimizer):\n",
    "    blip_model.train()\n",
    "    text_embeddings, image_embeddings = get_embeddings(file_path)\n",
    "    \n",
    "    match_labels, itc_loss_value = batch_itc_loss(image_embeddings, text_embeddings)\n",
    "    itm_loss_value = batch_itm_loss(image_embeddings, text_embeddings, match_labels)\n",
    "    \n",
    "    total_loss = itc_loss_value + itm_loss_value\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    total_loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    return total_loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Multilevel Alignment based on BLIP Architecture - Coarse Grained Alignment (Image Text Contrastive and Image Text Matching)'''\n",
    "\n",
    "# BLIP architecture\n",
    "processor = BlipProcessor.from_pretrained(\"Salesforce/blip-base\")\n",
    "blip_model = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-base\")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "blip_model.to(device)\n",
    "blip_model.eval()\n",
    "\n",
    "\n",
    "# training loop\n",
    "num_epochs = 10  \n",
    "optimizer = torch.optim.Adam(blip_model.parameters(), lr=1e-5)\n",
    "\n",
    "\n",
    "# using a data loader defined that provides (image, text) pairs\n",
    "for epoch in range(num_epochs):\n",
    "    loss = train_step(file_path, optimizer)\n",
    "    print(f\"Epoch {epoch}, Loss: {loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Multilevel Alignment based on BLIP Architecture - Fine Grained Alignment'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KL Loss for batch 0: 2.50960111618042\n",
      "KL Loss for batch 1: 2.5075905323028564\n",
      "KL Loss for batch 2: 2.5028076171875\n",
      "KL Loss for batch 3: 2.509035348892212\n",
      "KL Loss for batch 4: 2.510709762573242\n",
      "KL Loss for batch 5: 2.5108795166015625\n",
      "KL Loss for batch 6: 2.510974645614624\n",
      "KL Loss for batch 7: 2.5042688846588135\n",
      "KL Loss for batch 8: 2.5072593688964844\n",
      "KL Loss for batch 9: 2.500169277191162\n",
      "KL Loss for batch 10: 2.5004351139068604\n",
      "KL Loss for batch 11: 2.4989662170410156\n",
      "KL Loss for batch 12: 2.512904167175293\n",
      "KL Loss for batch 13: 2.506892442703247\n",
      "KL Loss for batch 14: 2.5084269046783447\n",
      "KL Loss for batch 15: 2.507992744445801\n",
      "KL Loss for batch 16: 2.505155086517334\n",
      "KL Loss for batch 17: 2.511310577392578\n",
      "KL Loss for batch 18: 2.5095150470733643\n",
      "KL Loss for batch 19: 2.513171434402466\n",
      "KL Loss for batch 20: 2.5062851905822754\n",
      "KL Loss for batch 21: 2.5028235912323\n",
      "KL Loss for batch 22: 2.510256767272949\n",
      "KL Loss for batch 23: 2.5104334354400635\n",
      "KL Loss for batch 24: 2.507218599319458\n",
      "KL Loss for batch 25: 2.501281499862671\n",
      "KL Loss for batch 26: 2.503661632537842\n",
      "KL Loss for batch 27: 2.5047688484191895\n",
      "KL Loss for batch 28: 2.506333112716675\n",
      "KL Loss for batch 29: 2.508028030395508\n",
      "KL Loss for batch 30: 2.51633620262146\n",
      "KL Loss for batch 31: 2.5105669498443604\n",
      "KL Loss for batch 32: 2.513197183609009\n",
      "KL Loss for batch 33: 2.5067684650421143\n",
      "KL Loss for batch 34: 2.511516571044922\n",
      "KL Loss for batch 35: 2.506542205810547\n",
      "KL Loss for batch 36: 2.5034873485565186\n",
      "KL Loss for batch 37: 2.5068066120147705\n",
      "KL Loss for batch 38: 2.5047945976257324\n",
      "KL Loss for batch 39: 2.513479709625244\n",
      "KL Loss for batch 40: 2.5084846019744873\n",
      "KL Loss for batch 41: 2.501063823699951\n",
      "KL Loss for batch 42: 2.506246328353882\n",
      "KL Loss for batch 43: 2.507786512374878\n",
      "KL Loss for batch 44: 2.501591444015503\n",
      "KL Loss for batch 45: 2.510796070098877\n",
      "KL Loss for batch 46: 2.511389970779419\n",
      "KL Loss for batch 47: 2.5080790519714355\n",
      "KL Loss for batch 48: 2.5058295726776123\n",
      "KL Loss for batch 49: 2.511479139328003\n",
      "KL Loss for batch 50: 2.5079152584075928\n",
      "KL Loss for batch 51: 2.499704360961914\n",
      "KL Loss for batch 52: 2.50091552734375\n",
      "KL Loss for batch 53: 2.510094165802002\n",
      "KL Loss for batch 54: 2.5117909908294678\n",
      "KL Loss for batch 55: 2.508760929107666\n",
      "KL Loss for batch 56: 2.5107779502868652\n",
      "KL Loss for batch 57: 2.5105721950531006\n",
      "KL Loss for batch 58: 2.503929615020752\n",
      "KL Loss for batch 59: 2.5082528591156006\n",
      "KL Loss for batch 60: 2.5114340782165527\n",
      "KL Loss for batch 61: 2.5063869953155518\n",
      "KL Loss for batch 62: 2.5096898078918457\n",
      "KL Loss for batch 63: 2.5104615688323975\n",
      "KL Loss for batch 64: 2.5056991577148438\n",
      "KL Loss for batch 65: 2.5050108432769775\n",
      "KL Loss for batch 66: 2.5085630416870117\n",
      "KL Loss for batch 67: 2.50586199760437\n",
      "KL Loss for batch 68: 2.50588059425354\n",
      "KL Loss for batch 69: 2.5022077560424805\n",
      "KL Loss for batch 70: 2.5127222537994385\n",
      "KL Loss for batch 71: 2.507448673248291\n",
      "KL Loss for batch 72: 2.5087318420410156\n",
      "KL Loss for batch 73: 2.5097687244415283\n",
      "KL Loss for batch 74: 2.5120129585266113\n",
      "KL Loss for batch 75: 2.517369031906128\n",
      "KL Loss for batch 76: 2.511927604675293\n",
      "KL Loss for batch 77: 2.5044169425964355\n",
      "KL Loss for batch 78: 2.5116493701934814\n",
      "KL Loss for batch 79: 2.5117175579071045\n",
      "KL Loss for batch 80: 2.507162570953369\n",
      "KL Loss for batch 81: 2.5023293495178223\n",
      "KL Loss for batch 82: 2.510446548461914\n",
      "KL Loss for batch 83: 2.508938789367676\n",
      "KL Loss for batch 84: 2.511141777038574\n",
      "KL Loss for batch 85: 2.508793354034424\n",
      "KL Loss for batch 86: 2.5095067024230957\n",
      "KL Loss for batch 87: 2.5086934566497803\n",
      "KL Loss for batch 88: 2.5044374465942383\n",
      "KL Loss for batch 89: 2.5041604042053223\n",
      "KL Loss for batch 90: 2.510058879852295\n",
      "KL Loss for batch 91: 2.5066325664520264\n",
      "KL Loss for batch 92: 2.5060036182403564\n",
      "KL Loss for batch 93: 2.509967803955078\n",
      "KL Loss for batch 94: 2.509882926940918\n",
      "KL Loss for batch 95: 2.510976791381836\n",
      "KL Loss for batch 96: 2.512115240097046\n",
      "KL Loss for batch 97: 2.5054404735565186\n",
      "KL Loss for batch 98: 2.511502742767334\n",
      "KL Loss for batch 99: 2.505554437637329\n",
      "KL Loss for batch 100: 2.5085034370422363\n",
      "KL Loss for batch 101: 2.508915424346924\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import os\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.distributions as dist\n",
    "\n",
    "\n",
    "output_aligned_features_dir = output_directory\n",
    "if(os.path.exists(output_aligned_features_dir + \"aligned_outputs.pt\")):\n",
    "    print(f\"Already at {historical_pt_b}\")\n",
    "\n",
    "batch_size = 32 \n",
    "\n",
    "final_embeddings = torch.load(output_directory + \"/final_embeddings.pt\")  # Shape: [7470, 50, 2048]\n",
    "#patch_features = torch.load(output_directory + 'patch_feats.pt')          # Shape: [7470, 49, 2048]\n",
    "dictionary_embeddings = torch.load(output_directory + \"/dictionary_embeddings.pt\")  # Shape: [47, 384]\n",
    "dictionary_embeddings = torch.tensor(dictionary_embeddings)\n",
    "\n",
    "# Project dictionary embeddings (V) to match the dimensionality of the image embeddings (2048)\n",
    "projection_layer = nn.Linear(384, 2048)\n",
    "V_projected = projection_layer(dictionary_embeddings)  # Shape: [47, 2048]\n",
    "# V_projected = F.normalize(V_projected, p=2, dim=1)\n",
    "\n",
    "# Normalize final embeddings (this is already being done)\n",
    "# I_prime = F.normalize(final_embeddings, p=2, dim=1)\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.d_k = d_model // num_heads\n",
    "        \n",
    "        # weight matrices for query, key, value\n",
    "        self.W_q = nn.Linear(d_model, d_model)\n",
    "        self.W_k = nn.Linear(d_model, d_model)\n",
    "        self.W_v = nn.Linear(d_model, d_model)\n",
    "        self.W_o = nn.Linear(d_model, d_model)\n",
    "\n",
    "    def scaled_dot_product_attention(self, Q, K, V):\n",
    "        # Compute QK^T / sqrt(d_k)\n",
    "        d_k = Q.size(-1)\n",
    "        scores = torch.matmul(Q, K.transpose(-2, -1)) / torch.sqrt(torch.tensor(d_k).float())\n",
    "        attn_weights = F.softmax(scores, dim=-1)\n",
    "        output = torch.matmul(attn_weights, V)\n",
    "        return output, attn_weights\n",
    "\n",
    "    def forward(self, V, I_prime):\n",
    "        # V:  projected dictionary embeddings\n",
    "        # I_prime: final_embeddings\n",
    "        \n",
    "        Q = self.W_q(V)  # Dictionary embeddings\n",
    "        K = self.W_k(I_prime)  # Image embeddings \n",
    "        V = self.W_v(I_prime)\n",
    "\n",
    "        Q = Q.view(Q.size(0), Q.size(1), self.num_heads, self.d_k).transpose(1, 2)\n",
    "        K = K.view(K.size(0), K.size(1), self.num_heads, self.d_k).transpose(1, 2)\n",
    "        V = V.view(V.size(0), V.size(1), self.num_heads, self.d_k).transpose(1, 2)\n",
    "       \n",
    "        attn_output, attn_weights = self.scaled_dot_product_attention(Q, K, V) # apply scaled dot-product attention\n",
    "\n",
    "        attn_output = attn_output.transpose(1, 2).contiguous().view(V.size(0), -1, self.d_model) # concatenate heads and apply final linear transformation\n",
    "        output = self.W_o(attn_output)\n",
    "        \n",
    "        return output, attn_weights\n",
    "\n",
    "# Feed-forward network\n",
    "class FeedForwardNetwork(nn.Module):\n",
    "    def __init__(self, d_model, d_ff):\n",
    "        super(FeedForwardNetwork, self).__init__()\n",
    "        self.fc1 = nn.Linear(d_model, d_ff)\n",
    "        self.fc2 = nn.Linear(d_ff, d_model)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc2(self.relu(self.fc1(x)))\n",
    "\n",
    "# KL Divergence Loss (for training)\n",
    "# def kl_divergence(mu1, logvar1, mu2, logvar2):\n",
    "#     kl_loss = 0.5 * torch.sum(logvar2 - logvar1 + (torch.exp(logvar1) + (mu1 - mu2)**2) / torch.exp(logvar2) - 1)\n",
    "#     return kl_loss\n",
    "def kl_divergence(mu1, logvar1, mu2, logvar2):\n",
    "    # kl_loss = 0.5 * torch.sum(\n",
    "    #     logvar2 - logvar1 +\n",
    "    #     (torch.exp(logvar1) + (mu1 - mu2) ** 2) / torch.exp(logvar2) - 1\n",
    "    # )\n",
    "    # def compute_kl_loss(mu1, logvar1, mu2, logvar2):\n",
    "    # Create normal distributions from the means (mu) and log-variances (logvar)\n",
    "    normal1 = dist.Normal(mu1, torch.exp(0.5 * logvar1))  # exp(0.5 * logvar) gives standard deviation\n",
    "    normal2 = dist.Normal(mu2, torch.exp(0.5 * logvar2))\n",
    "    \n",
    "    # Compute the KL divergence between the two distributions\n",
    "    kl_loss = dist.kl.kl_divergence(normal1, normal2).mean()  # Mean over the batch\n",
    "    return kl_loss\n",
    "\n",
    "    # return kl_loss\n",
    "    \n",
    "# Model setup\n",
    "d_model = 2048\n",
    "num_heads = 8\n",
    "d_ff = 4096  \n",
    "\n",
    "mha = MultiHeadAttention(d_model, num_heads)\n",
    "ffn = FeedForwardNetwork(d_model, d_ff)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "mha = mha.to(device)\n",
    "ffn = ffn.to(device)\n",
    "V_projected = V_projected.to(device)\n",
    "final_embeddings = final_embeddings.to(device)\n",
    "#patch_features = patch_features.to(device)\n",
    "\n",
    "dataset_size = final_embeddings.size(0)  # 7470\n",
    "aligned_outputs = []\n",
    "\n",
    "params = list(mha.parameters()) + list(ffn.parameters())  \n",
    "optimizer = torch.optim.Adam(params, lr=1e-4)  \n",
    "\n",
    "for i in range(0, dataset_size, batch_size):\n",
    "    \n",
    "    batch_final_embeddings = final_embeddings[i:i+batch_size]  # Shape: [batch_size, 50, 2048]\n",
    "    #batch_patch_features = patch_features[i:i+batch_size]  # Shape: [batch_size, 49, 2048]\n",
    "\n",
    "    #batch_I_prime = torch.cat((batch_final_embeddings, batch_patch_features), dim=1)  # Shape: [batch_size, 99, 2048]\n",
    "    batch_I_prime = batch_final_embeddings\n",
    "    batch_V_repeated = V_projected.unsqueeze(0).repeat(batch_final_embeddings.size(0), 1, 1)  # Shape: [batch_size, 47, 2048]\n",
    "\n",
    "    batch_I_prime = batch_I_prime.to(device)\n",
    "    batch_V_repeated = batch_V_repeated.to(device)\n",
    "\n",
    "    aligned_output, _ = mha(batch_V_repeated, batch_I_prime) #multi-head attention with dictionary embeddings and image embeddings\n",
    "\n",
    "    aligned_output_ffn = ffn(aligned_output) #feed-forward network\n",
    "\n",
    "    mu1, logvar1 = torch.randn_like(aligned_output_ffn,requires_grad=True), torch.randn_like(aligned_output_ffn,requires_grad=True)  # Priors for V'\n",
    "    mu2, logvar2 = torch.randn_like(batch_V_repeated,requires_grad=True), torch.randn_like(batch_V_repeated,requires_grad=True)  # Priors for V_label\n",
    "    kl_loss = kl_divergence(mu1, logvar1, mu2, logvar2) # KL divergence\n",
    "\n",
    "    print(f\"KL Loss for batch {i // batch_size}: {kl_loss.item()}\")\n",
    "\n",
    "    # If training, accumulate gradients and perform optimization steps\n",
    "    optimizer.zero_grad()\n",
    "    kl_loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    aligned_outputs.append(aligned_output_ffn.cpu()) \n",
    "\n",
    "#single tensor\n",
    "aligned_outputs_tensor = torch.cat(aligned_outputs, dim=0)  # Shape: [7470, 50, 2048]\n",
    "\n",
    "torch.save(aligned_outputs_tensor, os.path.join(output_aligned_features_dir, \"aligned_outputs.pt\"))\n",
    "\n",
    "print(\"Aligned features saved successfully.\")\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.distributions as dist\n",
    "from transformers import BertModel, BertTokenizer\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "class TextEncoder(nn.Module):\n",
    "    def __init__(self, bert_model=\"bert-base-uncased\", output_dim=384):\n",
    "        super(TextEncoder, self).__init__()\n",
    "        self.bert = BertModel.from_pretrained(bert_model)\n",
    "        self.tokenizer = BertTokenizer.from_pretrained(bert_model)\n",
    "        self.projection = nn.Linear(self.bert.config.hidden_size, output_dim)\n",
    "        \n",
    "    def encode_dictionary(self, dictionary):\n",
    "        \"\"\"\n",
    "        Encodes dictionary entries using BERT and combines key-value pairs\n",
    "        dictionary: Dict with medical terms as keys and list of related terms as values\n",
    "        Returns: Tensor of shape [num_entries, output_dim]\n",
    "        \"\"\"\n",
    "        encoded_entries = []\n",
    "        \n",
    "        for key, values in dictionary.items():\n",
    "            # Combine key with its values into a single text\n",
    "            if values:  # If values list is not empty\n",
    "                text = key + \": \" + \", \".join(values)\n",
    "            else:\n",
    "                text = key\n",
    "                \n",
    "            # Tokenize and encode\n",
    "            inputs = self.tokenizer(text, \n",
    "                                  padding=True, \n",
    "                                  truncation=True, \n",
    "                                  return_tensors=\"pt\")\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                outputs = self.bert(**inputs)\n",
    "                # Use [CLS] token embedding\n",
    "                embedding = outputs.last_hidden_state[:, 0, :]\n",
    "                \n",
    "            # Project to desired dimension\n",
    "            projected = self.projection(embedding)\n",
    "            encoded_entries.append(projected)\n",
    "            \n",
    "        return torch.cat(encoded_entries, dim=0)\n",
    "\n",
    "# Your original alignment code\n",
    "output_aligned_features_dir = output_directory\n",
    "if(os.path.exists(output_aligned_features_dir + \"aligned_outputs.pt\")):\n",
    "    print(f\"Already at {historical_pt_b}\")\n",
    "\n",
    "batch_size = 32 \n",
    "\n",
    "# Load the medical dictionary\n",
    "medical_dict = {\n",
    "    \"pleural\": [\"hemithorax\", \"effusion\", \"pneumothorax\", \"parenchymal\"],\n",
    "    \"lung\": [\"lungs\", \"pulmonary\", \"hilar\", \"lobe\", \"consolidation\", \n",
    "             \"atelectasis\", \"edema\", \"opacity\", \"pneumonia\"],\n",
    "    \"mediastinal\": [\"mediastinum\", \"diaphragm\", \"hemidiaphragm\"],\n",
    "    \"cardiac\": [\"heart\", \"cardiomegaly\", \"cardiomediastinal\", \"atrium\",\n",
    "                \"ventricle\", \"retrocardiac\"],\n",
    "    \"vascular\": [\"aorta\", \"venous\", \"jugular\", \"aortic\", \"vasculature\", \"cabg\"],\n",
    "    \"osseous\": [\"rib\", \"sternal\", \"subclavian\", \"thoracic\"],\n",
    "    \"trachea\": [\"endotrachea\"],\n",
    "    \"stomach\": [],\n",
    "    \"abdomen\": [],\n",
    "    \"tube\": [\"clips\"],\n",
    "    \"spine\": [\"vertebral\", \"degenerative\"],\n",
    "    \"nodule\": [\"mass\"],\n",
    "    \"chest\": [\"small\", \"enlarged\", \"unchanged\", \"stable\", \"silhouette\",\n",
    "              \"contours\", \"size\", \"focal\", \"mild\", \"acute\"]\n",
    "}\n",
    "\n",
    "# Initialize text encoder and encode dictionary\n",
    "text_encoder = TextEncoder()\n",
    "dictionary_embeddings = text_encoder.encode_dictionary(medical_dict)\n",
    "\n",
    "# final_embeddings = torch.load(output_directory + \"/final_embeddings.pt\")\n",
    "final_embeddings = torch.load(output_directory + \"/final_embeddings.pt\", map_location=torch.device('cpu'))# Shape: [7470, 50, 2048]\n",
    "dictionary_embeddings = dictionary_embeddings.to(torch.float32)  # Convert to float32\n",
    "\n",
    "# Project dictionary embeddings (V) to match the dimensionality of the image embeddings (2048)\n",
    "projection_layer = nn.Linear(384, 6144)\n",
    "V_projected = projection_layer(dictionary_embeddings)  # Shape: [47, 2048]\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.d_k = d_model // num_heads\n",
    "        \n",
    "        # weight matrices for query, key, value\n",
    "        self.W_q = nn.Linear(d_model, d_model)\n",
    "        self.W_k = nn.Linear(d_model, d_model)\n",
    "        self.W_v = nn.Linear(d_model, d_model)\n",
    "        self.W_o = nn.Linear(d_model, d_model)\n",
    "\n",
    "    def scaled_dot_product_attention(self, Q, K, V):\n",
    "        # Compute QK^T / sqrt(d_k)\n",
    "        d_k = Q.size(-1)\n",
    "        scores = torch.matmul(Q, K.transpose(-2, -1)) / torch.sqrt(torch.tensor(d_k).float())\n",
    "        attn_weights = F.softmax(scores, dim=-1)\n",
    "        output = torch.matmul(attn_weights, V)\n",
    "        return output, attn_weights\n",
    "\n",
    "    def forward(self, V, I_prime):\n",
    "        # V:  projected dictionary embeddings\n",
    "        # I_prime: final_embeddings\n",
    "        # print(\"Shape of I_prime:\", I_prime.shape)\n",
    "        I_prime = I_prime.unsqueeze(1) \n",
    "        Q = self.W_q(V)  # Dictionary embeddings\n",
    "        K = self.W_k(I_prime)  # Image embeddings \n",
    "        V = self.W_v(I_prime)\n",
    "        batch_size, seq_len, d_model = K.size()\n",
    "        Q = Q.view(Q.size(0), Q.size(1), self.num_heads, self.d_k).transpose(1, 2)\n",
    "        K = K.view(K.size(0), K.size(1), self.num_heads, self.d_k).transpose(1, 2)\n",
    "        V = V.view(V.size(0), V.size(1), self.num_heads, self.d_k).transpose(1, 2)\n",
    "       \n",
    "        attn_output, attn_weights = self.scaled_dot_product_attention(Q, K, V)\n",
    "        attn_output = attn_output.transpose(1, 2).contiguous().view(V.size(0), -1, self.d_model)\n",
    "        output = self.W_o(attn_output)\n",
    "        \n",
    "        return output, attn_weights\n",
    "\n",
    "class FeedForwardNetwork(nn.Module):\n",
    "    def __init__(self, d_model, d_ff):\n",
    "        super(FeedForwardNetwork, self).__init__()\n",
    "        self.fc1 = nn.Linear(d_model, d_ff)\n",
    "        self.fc2 = nn.Linear(d_ff, d_model)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc2(self.relu(self.fc1(x)))\n",
    "\n",
    "def kl_divergence(mu1, logvar1, mu2, logvar2):\n",
    "    normal1 = dist.Normal(mu1, torch.exp(0.5 * logvar1))\n",
    "    normal2 = dist.Normal(mu2, torch.exp(0.5 * logvar2))\n",
    "    kl_loss = dist.kl.kl_divergence(normal1, normal2).mean()\n",
    "    return kl_loss\n",
    "\n",
    "class Piror(nn.Module):\n",
    "    \"\"\"Fully connected layer to convert encodings to mean and variance\"\"\"\n",
    "    def __init__(self, input_dim=6144, hidden_dim=512):\n",
    "        super(Piror, self).__init__()\n",
    "        self.fc_mu = nn.Linear(input_dim, hidden_dim)\n",
    "        self.fc_var = nn.Linear(input_dim, hidden_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Generate mean and log variance\n",
    "        mu = self.fc_mu(x)\n",
    "        logvar = self.fc_var(x)\n",
    "        return mu, logvar\n",
    "\n",
    "# def compute_kl_loss(mu1, logvar1, mu2, logvar2):\n",
    "#     \"\"\"\n",
    "#     Compute KL divergence between two normal distributions N1(mu1, sigma1) and N2(mu2, sigma2)\n",
    "#     as per equation (12) in the paper\n",
    "#     \"\"\"\n",
    "#     # Convert log variance to variance\n",
    "#     var1 = torch.exp(logvar1)\n",
    "#     var2 = torch.exp(logvar2)\n",
    "    \n",
    "#     # Compute KL divergence according to equation (12)\n",
    "#     kl_div = 0.5 * torch.sum(\n",
    "#         logvar2 - logvar1 + \n",
    "#         (var1 + (mu1 - mu2).pow(2)) / var2 - 1\n",
    "#     )\n",
    "    \n",
    "#     return kl_div    \n",
    "# Model setup\n",
    "d_model = 6144\n",
    "num_heads = 8\n",
    "d_ff = 4096  \n",
    "\n",
    "mha = MultiHeadAttention(d_model, num_heads)\n",
    "ffn = FeedForwardNetwork(d_model, d_ff)\n",
    "piror = Piror(d_model)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "mha = mha.to(device)\n",
    "ffn = ffn.to(device)\n",
    "V_projected = V_projected.to(device)\n",
    "final_embeddings = final_embeddings.to(device)\n",
    "piror = piror.to(device)\n",
    "dataset_size = final_embeddings.size(0)\n",
    "aligned_outputs = []\n",
    "\n",
    "params = list(mha.parameters()) + list(ffn.parameters())  \n",
    "optimizer = torch.optim.Adam(params, lr=1e-4)  \n",
    "\n",
    "for i in range(0, dataset_size, batch_size):\n",
    "    optimizer.zero_grad()\n",
    "    batch_final_embeddings = final_embeddings[i:i+batch_size]\n",
    "    batch_I_prime = batch_final_embeddings\n",
    "    batch_V_repeated = V_projected.unsqueeze(0).repeat(batch_final_embeddings.size(0), 1, 1)\n",
    "\n",
    "    batch_I_prime = batch_I_prime.to(device)\n",
    "    batch_V_repeated = batch_V_repeated.to(device)\n",
    "\n",
    "    aligned_output, _ = mha(batch_V_repeated, batch_I_prime)\n",
    "    aligned_output_ffn = ffn(aligned_output)\n",
    "    V_prime = aligned_output_ffn\n",
    "    V_label = batch_V_repeated \n",
    "    mu1, logvar1 = piror(V_prime)\n",
    "    mu2, logvar2 = piror(V_label)\n",
    "    \n",
    "    # Compute KL divergence loss according to equation (12)\n",
    "    # kl_loss = compute_kl_loss(mu1, logvar1, mu2, logvar2)\n",
    "\n",
    "    # print(f\"KL Loss for batch {i // batch_size}: {kl_loss.item()}\")\n",
    "\n",
    "    #     mu1, logvar1 = torch.randn_like(aligned_output_ffn,requires_grad=True), torch.randn_like(aligned_output_ffn,requires_grad=True)\n",
    "    # mu2, logvar2 = torch.randn_like(batch_V_repeated,requires_grad=True), torch.randn_like(batch_V_repeated,requires_grad=True)\n",
    "    kl_loss = kl_divergence(mu1, logvar1, mu2, logvar2)\n",
    "\n",
    "    print(f\"KL Loss for batch {i // batch_size}: {kl_loss.item()}\")\n",
    "\n",
    "    \n",
    "    kl_loss.backward(retain_graph=True)\n",
    "    optimizer.step()\n",
    "    \n",
    "    aligned_outputs.append(aligned_output_ffn.cpu())\n",
    "\n",
    "aligned_outputs_tensor = torch.cat(aligned_outputs, dim=0)\n",
    "torch.save(aligned_outputs_tensor, os.path.join(output_aligned_features_dir, \"aligned_outputs.pt\"))\n",
    "# Saving each model's state_dict separately\n",
    "torch.save(mha.state_dict(), \"mha.pth\")\n",
    "torch.save(ffn.state_dict(), \"ffn.pth\")\n",
    "torch.save(piror.state_dict(), \"piror.pth\")\n",
    "\n",
    "# Or saving all models together\n",
    "torch.save({\n",
    "    'mha': mha.state_dict(),\n",
    "    'ffn': ffn.state_dict(),\n",
    "    'piror': piror.state_dict(),\n",
    "}, \"model_parameters.pth\")\n",
    "\n",
    "print(\"Aligned features saved successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Uz4pEhW2MuBF"
   },
   "source": [
    "### **Report Generator**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z_WeRBlDMwX2"
   },
   "source": [
    "### **Complete Model**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NZjhDeJxMzgg"
   },
   "source": [
    "## **Training**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7Uk_G0K8M2H9"
   },
   "source": [
    "### **Training**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OXhkaJVvM3Uu"
   },
   "source": [
    "## **Testing**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bzTZZUlQM401"
   },
   "source": [
    "### **Testing**"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "MlOl6I0rMa68",
    "PsELnlXpMjGX",
    "D2kBZTiCMmsE",
    "gIYzXGtsMqmQ",
    "Uz4pEhW2MuBF",
    "Z_WeRBlDMwX2",
    "7Uk_G0K8M2H9",
    "OXhkaJVvM3Uu"
   ],
   "gpuType": "T4",
   "provenance": []
  },
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [],
   "dockerImageVersionId": 30787,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
