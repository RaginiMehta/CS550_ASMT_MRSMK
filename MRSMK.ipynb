{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Bnpd7Zh0NPl2"
   },
   "source": [
    "## **Medical Report Summarisation using Medical Knowledge**\n",
    "\n",
    "### **References**\n",
    "\n",
    "**Main Reference**\n",
    "- Radiology report generation with medical knowledge and multilevel image-report alignment: A new method and its verification\n",
    "https://www.sciencedirect.com/science/article/pii/S0933365723002282#bib1\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oKdcyk1sMEtD"
   },
   "source": [
    "## **Data Collection**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p81rfgckMR9a"
   },
   "source": [
    "### **Collect Datasets**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AYR_B6u1ojJt",
    "outputId": "faaac1ed-b54b-4cfd-b3e4-cca51372318e"
   },
   "outputs": [],
   "source": [
    "'''Libraries Installation and Import'''\n",
    "\n",
    "# installling necessary libraries\n",
    "!pip -q install requests numpy pandas matplotlib tqdm Pillow opencv-python nltk pyspellchecker torch torchvision transformers scikit-learn\n",
    "\n",
    "# importing required libraries\n",
    "import os\n",
    "import re\n",
    "import requests\n",
    "import tarfile\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import xml.etree.ElementTree as ET\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "import cv2\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from spellchecker import SpellChecker\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.models as models\n",
    "from torchvision import datasets\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from transformers import BertTokenizer, BertModel\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "49-0xiXplwH8",
    "outputId": "ea63eda3-908a-4f5f-eeec-a4019130e57f"
   },
   "outputs": [],
   "source": [
    "'''Setup - Generalized'''\n",
    "\n",
    "# setup to download the IU X-Ray Dataset\n",
    "dataset = 'iu_xray/'\n",
    "download_path = os.path.join('./datasets', dataset)\n",
    "\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')\n",
    "# download_path = os.path.join('/content/drive/Othercomputers/My Laptop/CS550_ASMT_MRSMK/datasets', dataset)\n",
    "# download_path = os.path.join('/content/drive/MyDrive/Academics/CS550 Machine Learning/CS550 ASMT MRSMK/datasets', dataset)\n",
    "\n",
    "images_dir = os.path.join(download_path, \"images\")\n",
    "reports_dir = os.path.join(download_path, \"reports\")\n",
    "\n",
    "images_url = \"https://openi.nlm.nih.gov/imgs/collections/NLMCXR_png.tgz\"\n",
    "reports_url = \"https://openi.nlm.nih.gov/imgs/collections/NLMCXR_reports.tgz\"\n",
    "\n",
    "\n",
    "# function to check the file size of a given URL\n",
    "def get_file_size(url):\n",
    "    response = requests.head(url)\n",
    "    size_in_bytes = int(response.headers.get('Content-Length', 0))\n",
    "    size_in_mb = size_in_bytes / (1024 * 1024)\n",
    "    return size_in_mb\n",
    "\n",
    "\n",
    "# function to download and extract from a given url to a given directory\n",
    "def download_and_extract(url, save_dir):\n",
    "    file_name = url.split('/')[-1]\n",
    "    file_path = os.path.join(save_dir, file_name)\n",
    "\n",
    "    response = requests.get(url, stream=True)\n",
    "    total_size = int(response.headers.get('Content-Length', 0))\n",
    "    downloaded_size = 0\n",
    "\n",
    "    with open(file_path, 'wb') as file:\n",
    "        for chunk in response.iter_content(chunk_size=1024):\n",
    "            if chunk:\n",
    "                file.write(chunk)\n",
    "                downloaded_size += len(chunk)\n",
    "                percent_complete = (downloaded_size / total_size) * 100\n",
    "                print(f\"Downloaded {downloaded_size / (1024*1024):.2f} MB out of {total_size / (1024*1024):.2f} MB: {percent_complete:.2f}% complete\")\n",
    "\n",
    "    print(\"\\nDownload complete!\")\n",
    "\n",
    "    with tarfile.open(file_path, 'r:gz') as tar:\n",
    "        members = tar.getmembers()\n",
    "        total_files = len(members)\n",
    "\n",
    "        for idx, member in enumerate(members, start=1):\n",
    "            tar.extract(member, path=save_dir)\n",
    "            print(f\"Extracting File {idx} out of {total_files}: {member.name}\")\n",
    "\n",
    "    os.remove(file_path)\n",
    "\n",
    "\n",
    "# downloading  IU X-Ray dataset\n",
    "if not os.path.exists(images_dir):\n",
    "    images_size = get_file_size(images_url)\n",
    "    print(f\"Downloading {images_url} to: {images_dir} ({images_size:.2f} MB)\")\n",
    "    os.makedirs(images_dir, exist_ok=True)\n",
    "    download_and_extract(images_url, images_dir)\n",
    "    print(f\"Downloaded {images_url} to: {images_dir}\")\n",
    "else:\n",
    "    print(f\"{images_url} already exists at: {images_dir}\")\n",
    "\n",
    "if not os.path.exists(reports_dir):\n",
    "    reports_size = get_file_size(reports_url)\n",
    "    print(f\"Downloading {reports_url} to: {reports_dir} ({reports_size:.2f} MB)\")\n",
    "    os.makedirs(reports_dir, exist_ok=True)\n",
    "    download_and_extract(reports_url, reports_dir)\n",
    "    print(f\"Downloaded {reports_url} to: {reports_dir}\")\n",
    "else:\n",
    "    print(f\"{reports_url} already exists at: {reports_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5ZSA8Jyowaoh",
    "outputId": "dffa4899-00a7-4d49-a118-8dd85400ba06"
   },
   "outputs": [],
   "source": [
    "'''Exploring the IU X-Ray Dataset Contents'''\n",
    "\n",
    "# displaying directory and subdirectory contents\n",
    "iu_xray = download_path\n",
    "print(\"\\nPath: \", iu_xray)\n",
    "print(f\"Directory Contents: {os.listdir(iu_xray)}\")\n",
    "\n",
    "iu_xray_images = images_dir\n",
    "print(\"\\nPath: \", iu_xray_images)\n",
    "print(f\"Directory Contents: {len(os.listdir(iu_xray_images))} Images\")\n",
    "\n",
    "iu_xray_reports = os.path.join(reports_dir, 'ecgen-radiology')\n",
    "print(\"\\nPath: \", iu_xray_reports)\n",
    "print(f\"Directory Contents: {len(os.listdir(iu_xray_reports))} Reports\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 822
    },
    "id": "qGczexPLUaN5",
    "outputId": "8c7528a1-7064-4802-c276-edef004f0d5b"
   },
   "outputs": [],
   "source": [
    "'''Processing Textual Data from each .xml Report File and Storing it in a .csv File'''\n",
    "\n",
    "# function to iterate through all .xml report files and storing them in a dataframe\n",
    "def save_images_df():\n",
    "    data = []\n",
    "    cnt = 0\n",
    "    for file in os.listdir(iu_xray_reports):\n",
    "        if file.endswith(\".xml\"):\n",
    "            cnt += 1\n",
    "            print(f\"Processing .xml File {cnt} out of {len(os.listdir(iu_xray_reports))}: {file}\")\n",
    "\n",
    "            file_path = os.path.join(iu_xray_reports, file)\n",
    "            try:\n",
    "                tree = ET.parse(file_path)\n",
    "                root = tree.getroot()\n",
    "\n",
    "                pmc_id = root.find('.//pmcId').attrib.get('id')\n",
    "\n",
    "                comparison = indication = findings = impression = None\n",
    "\n",
    "                for abstract in root.findall('.//AbstractText'):\n",
    "                    if abstract.attrib.get('Label') == 'COMPARISON':\n",
    "                        comparison = abstract.text\n",
    "                    elif abstract.attrib.get('Label') == 'INDICATION':\n",
    "                        indication = abstract.text\n",
    "                    elif abstract.attrib.get('Label') == 'FINDINGS':\n",
    "                        findings = abstract.text\n",
    "                    elif abstract.attrib.get('Label') == 'IMPRESSION':\n",
    "                        impression = abstract.text\n",
    "\n",
    "                for parent_image in root.findall('parentImage'):\n",
    "                    image_file = parent_image.attrib['id'] + \".png\"\n",
    "                    image_path = os.path.join(iu_xray_images, image_file)\n",
    "                    image = cv2.imread(image_path)\n",
    "\n",
    "                    if image is not None:\n",
    "                        height, width, channels = image.shape\n",
    "                        caption = parent_image.find('caption').text if parent_image.find('caption') is not None else None\n",
    "                        data.append([pmc_id, image_file, caption, comparison, indication, findings, impression, height, width])\n",
    "                    else:\n",
    "                        print(f\"Warning: Unable to read image {image_path}\")\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing file {file}: {e}\")\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "# creating a dataframe and saving it as .csv\n",
    "iu_xray_images_df_path = os.path.join(iu_xray, 'iu_xray_images_df.csv')\n",
    "if not os.path.exists(iu_xray_images_df_path):\n",
    "    data = save_images_df()\n",
    "    columns = ['pmc_id', 'image_filename', 'caption', 'comparison', 'indication', 'findings', 'impression', 'height', 'width']\n",
    "    iu_xray_images_df = pd.DataFrame(data, columns=columns)\n",
    "    iu_xray_images_df.to_csv(iu_xray_images_df_path, index=False)\n",
    "    print(f\"Dataframe saved to {iu_xray_images_df_path}\")\n",
    "else:\n",
    "    print(f\"Dataframe already exists at {iu_xray_images_df_path}\")\n",
    "    iu_xray_images_df = pd.read_csv(iu_xray_images_df_path)\n",
    "\n",
    "\n",
    "# displaying the stored dataframe\n",
    "print(\"\\n\\nDataframe Shape:\", iu_xray_images_df.shape)\n",
    "\n",
    "print(\"\\n\\nDataframe Information:\\n\")\n",
    "display(iu_xray_images_df.info())\n",
    "\n",
    "print(\"\\n\\nDisplaying Dataframe:\\n\")\n",
    "display(iu_xray_images_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 893
    },
    "id": "vYnfzXXT0O6w",
    "outputId": "d2692046-ebe0-45ac-8291-37641f6d5958"
   },
   "outputs": [],
   "source": [
    "'''Processing Textual Data from each .xml Report File and Storing it in a .csv File'''\n",
    "\n",
    "# function to iterate through all .xml report files and storing them in a dataframe\n",
    "def save_reports_df():\n",
    "    data = []\n",
    "    cnt = 0\n",
    "    for file in os.listdir(iu_xray_reports):\n",
    "        if file.endswith(\".xml\"):\n",
    "            cnt += 1\n",
    "            print(f\"Processing .xml File {cnt} out of {len(os.listdir(iu_xray_reports))}: {file}\")\n",
    "\n",
    "            file_path = os.path.join(iu_xray_reports, file)\n",
    "            try:\n",
    "                tree = ET.parse(file_path)\n",
    "                root = tree.getroot()\n",
    "\n",
    "                pmc_id = root.find('.//pmcId').attrib.get('id')\n",
    "\n",
    "                comparison = indication = findings = impression = None\n",
    "\n",
    "                for abstract in root.findall('.//AbstractText'):\n",
    "                    if abstract.attrib.get('Label') == 'COMPARISON':\n",
    "                        comparison = abstract.text\n",
    "                    elif abstract.attrib.get('Label') == 'INDICATION':\n",
    "                        indication = abstract.text\n",
    "                    elif abstract.attrib.get('Label') == 'FINDINGS':\n",
    "                        findings = abstract.text\n",
    "                    elif abstract.attrib.get('Label') == 'IMPRESSION':\n",
    "                        impression = abstract.text\n",
    "\n",
    "                report_data = {\n",
    "                    'pmc_id': pmc_id,\n",
    "                    'findings': findings,\n",
    "                    'impression': impression,\n",
    "                    'comparison': comparison,\n",
    "                    'indication': indication,\n",
    "                }\n",
    "\n",
    "                parent_images = root.findall('parentImage')\n",
    "                report_data['image_count'] = len(parent_images)\n",
    "\n",
    "                for i, parent_image in enumerate(parent_images, start=1):\n",
    "                    image_file = parent_image.attrib['id'] + \".jpg\"\n",
    "                    caption = parent_image.find('caption').text if parent_image.find('caption') is not None else None\n",
    "                    report_data[f'image_{i}'] = f\"{image_file}: {caption}\" if caption else image_file\n",
    "\n",
    "                data.append(report_data)\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing file {file}: {e}\")\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "# creating a dataframe and saving it as .csv\n",
    "iu_xray_reports_df_path = os.path.join(iu_xray, 'iu_xray_reports_df.csv')\n",
    "if not os.path.exists(iu_xray_reports_df_path):\n",
    "    data = save_reports_df()\n",
    "    iu_xray_reports_df = pd.DataFrame(data)\n",
    "    iu_xray_reports_df.to_csv(iu_xray_reports_df_path, index=False)\n",
    "    print(f\"Dataframe saved to {iu_xray_reports_df_path}\")\n",
    "else:\n",
    "    print(f\"Dataframe already exists at {iu_xray_reports_df_path}\")\n",
    "    iu_xray_reports_df = pd.read_csv(iu_xray_reports_df_path)\n",
    "\n",
    "\n",
    "# displaying the stored dataframe\n",
    "print(\"\\n\\nDataframe Shape:\", iu_xray_reports_df.shape)\n",
    "\n",
    "print(\"\\n\\nDataframe Information:\\n\")\n",
    "display(iu_xray_reports_df.info())\n",
    "\n",
    "print(\"\\n\\nDisplaying Dataframe:\\n\")\n",
    "display(iu_xray_reports_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 310
    },
    "id": "bw4Ylfa94M1o",
    "outputId": "5ef503a0-265c-423b-8cae-1d36b136134d"
   },
   "outputs": [],
   "source": [
    "'''Displaying the Number of Images per Report'''\n",
    "\n",
    "# displaying the distribution of number of images per report\n",
    "reports_count = iu_xray_reports_df['image_count'].value_counts().rename_axis('images_qty').reset_index(name='reports_count')\n",
    "print(\"\\n\\nNumber of Images per Report:\\n\")\n",
    "display(reports_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6UouFxQwMNeo"
   },
   "source": [
    "## **Data Preprocessing**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-cGf99_CMV47"
   },
   "source": [
    "### **Preprocess Images**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JTClOSxeSCOM",
    "outputId": "811cbc93-70e7-4d60-f213-26238488c44d"
   },
   "outputs": [],
   "source": [
    "'''Preprocessing Images - Resizing, Tensor Conversion and Normalization'''\n",
    "\n",
    "# function to preprocess and save images\n",
    "def preprocess_images(input_dir, output_dir):\n",
    "    preprocess = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ])\n",
    "\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    cnt = 0\n",
    "    for filename in os.listdir(input_dir):\n",
    "        if filename.endswith('.png'):\n",
    "            cnt += 1\n",
    "            print(f\"Preprocessing File {cnt} out of {len(os.listdir(input_dir))}: {filename}\")\n",
    "\n",
    "            image_path = os.path.join(input_dir, filename)\n",
    "            image = Image.open(image_path).convert('RGB')\n",
    "            processed_image = preprocess(image)\n",
    "\n",
    "            processed_image_path = os.path.join(output_dir, filename)\n",
    "\n",
    "            processed_image_pil = transforms.ToPILImage()(processed_image)\n",
    "            processed_image_pil.save(processed_image_path)\n",
    "\n",
    "\n",
    "# preprocessing images\n",
    "iu_xray_images_preprocessed = os.path.join(iu_xray, 'images_preprocessed')\n",
    "if not os.path.exists(iu_xray_images_preprocessed):\n",
    "    print(f\"Preprocessing Images to: {iu_xray_images_preprocessed}\")\n",
    "    preprocess_images(iu_xray_images, iu_xray_images_preprocessed)\n",
    "    print(f\"Preprocessed Images saved to: {iu_xray_images_preprocessed}\")\n",
    "else:\n",
    "    print(f\"Preprocessed Images already exist at: {iu_xray_images_preprocessed}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lvdRoNqXMZlN",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### **Preprocess Text**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "M8sW8uz8-plE"
   },
   "outputs": [],
   "source": [
    "'''Preprocessing Text - Lowercasing, Decontracting, Punctuation Removal, Number Removal, Two-Letter Word Removal, Stop Word Removal, Spell Checking, Extra Space Removal'''\n",
    "\n",
    "# downloading nltk resources and initializing spell checker\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "spell = SpellChecker()\n",
    "\n",
    "\n",
    "# function to convert text to lowercase\n",
    "def lowercase(text):\n",
    "    return text.lower() if isinstance(text, str) else text\n",
    "\n",
    "\n",
    "# function to decontract words\n",
    "def decontracted(text):\n",
    "    if not isinstance(text, str):\n",
    "        return text\n",
    "    contractions = {\n",
    "        \"won't\": \"will not\", \"can't\": \"can not\", \"couldn't\": \"could not\",\n",
    "        \"shouldn't\": \"should not\", \"wouldn't\": \"would not\", \"n't\": \" not\",\n",
    "        \"'re\": \" are\", \"'s\": \" is\", \"'d\": \" would\", \"'ll\": \" will\",\n",
    "        \"'t\": \" not\", \"'ve\": \" have\", \"'m\": \" am\"\n",
    "    }\n",
    "    for contraction, full_form in contractions.items():\n",
    "        text = text.replace(contraction, full_form)\n",
    "    return text\n",
    "\n",
    "\n",
    "# function to remove punctuations\n",
    "def rem_punctuations(text):\n",
    "    return re.sub(r'[^\\w\\s]', ' ', text) if isinstance(text, str) else text\n",
    "\n",
    "\n",
    "# function to remove numbers\n",
    "def rem_numbers(text):\n",
    "    return re.sub(r'\\d+', ' ', text) if isinstance(text, str) else text\n",
    "\n",
    "\n",
    "# function to remove two-letter words except \"no\" and \"ct\"\n",
    "def rem_two_letter_words(text):\n",
    "    if not isinstance(text, str):\n",
    "        return text\n",
    "    return ' '.join(word for word in text.split() if len(word) > 2 or word in [\"no\", \"ct\"])\n",
    "\n",
    "\n",
    "# function to remove stop words\n",
    "def rem_stop_words(text):\n",
    "    if not isinstance(text, str):\n",
    "        return text\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    return ' '.join(word for word in text.split() if word not in stop_words)\n",
    "\n",
    "\n",
    "# function to correct spelling\n",
    "def correct_spelling(text):\n",
    "    if not isinstance(text, str):\n",
    "        return text\n",
    "    corrected = []\n",
    "    for word in text.split():\n",
    "        corrected_word = list(spell.candidates(word))[0] if spell.candidates(word) else word\n",
    "        corrected.append(corrected_word)\n",
    "    return ' '.join(corrected)\n",
    "\n",
    "\n",
    "# function to remove extra spaces\n",
    "def rem_extra_spaces(text):\n",
    "    return ' '.join(text.split()) if isinstance(text, str) else text\n",
    "\n",
    "\n",
    "# function to preprocess text\n",
    "def preprocess_text(data):\n",
    "    preprocessed = []\n",
    "    for sentence in tqdm(data.values):\n",
    "        sentence = str(sentence)\n",
    "        sentence = lowercase(sentence)\n",
    "        sentence = decontracted(sentence)\n",
    "        sentence = rem_punctuations(sentence)\n",
    "        sentence = rem_numbers(sentence)\n",
    "        sentence = rem_two_letter_words(sentence)\n",
    "        sentence = rem_stop_words(sentence)\n",
    "        sentence = correct_spelling(sentence)\n",
    "        sentence = rem_extra_spaces(sentence)\n",
    "        \n",
    "        preprocessed.append(sentence)\n",
    "\n",
    "    return preprocessed\n",
    "\n",
    "\n",
    "# setting path to the preprocessed dataframe\n",
    "iu_xray_reports_preprocessed_df_path = os.path.join(iu_xray, 'iu_xray_reports_preprocessed_df.csv')\n",
    "iu_xray_reports_preprocessed_df = iu_xray_reports_df.copy()\n",
    "\n",
    "\n",
    "# preprocessing text columns in the dataframe\n",
    "print(f\"Preprocessing Text of DataFrame {iu_xray_reports_df_path} to: {iu_xray_reports_preprocessed_df_path}\")\n",
    "\n",
    "preprocess_caption = True\n",
    "preprocess_comparison = True\n",
    "preprocess_indication = True\n",
    "preprocess_findings = True\n",
    "preprocess_impression = True\n",
    "\n",
    "if preprocess_caption and 'caption' in iu_xray_reports_preprocessed_df.columns:\n",
    "    print(\"Preprocessing Column: caption\")\n",
    "    iu_xray_reports_preprocessed_df['caption'] = iu_xray_reports_preprocessed_df['caption'].fillna('unknown').astype(str)\n",
    "    iu_xray_reports_preprocessed_df['caption'] = preprocess_text(iu_xray_reports_preprocessed_df['caption'])\n",
    "    iu_xray_reports_preprocessed_df.to_csv(iu_xray_reports_preprocessed_df_path, index=False)\n",
    "    print(f\"Saved preprocessed 'caption' column to: {iu_xray_reports_preprocessed_df_path}\")\n",
    "\n",
    "if preprocess_comparison and 'comparison' in iu_xray_reports_preprocessed_df.columns:\n",
    "    print(\"Preprocessing Column: comparison\")\n",
    "    iu_xray_reports_preprocessed_df['comparison'] = iu_xray_reports_preprocessed_df['comparison'].fillna('none').astype(str)\n",
    "    iu_xray_reports_preprocessed_df['comparison'] = preprocess_text(iu_xray_reports_preprocessed_df['comparison'])\n",
    "    iu_xray_reports_preprocessed_df.to_csv(iu_xray_reports_preprocessed_df_path, index=False)\n",
    "    print(f\"Saved preprocessed 'comparison' column to: {iu_xray_reports_preprocessed_df_path}\")\n",
    "\n",
    "if preprocess_indication and 'indication' in iu_xray_reports_preprocessed_df.columns:\n",
    "    print(\"Preprocessing Column: indication\")\n",
    "    iu_xray_reports_preprocessed_df['indication'] = iu_xray_reports_preprocessed_df['indication'].fillna('none').astype(str)\n",
    "    iu_xray_reports_preprocessed_df['indication'] = preprocess_text(iu_xray_reports_preprocessed_df['indication'])\n",
    "    iu_xray_reports_preprocessed_df.to_csv(iu_xray_reports_preprocessed_df_path, index=False)\n",
    "    print(f\"Saved preprocessed 'indication' column to: {iu_xray_reports_preprocessed_df_path}\")\n",
    "\n",
    "if preprocess_findings and 'findings' in iu_xray_reports_preprocessed_df.columns:\n",
    "    print(\"Preprocessing Column: findings\")\n",
    "    iu_xray_reports_preprocessed_df['findings'] = iu_xray_reports_preprocessed_df['findings'].fillna('none').astype(str)\n",
    "    iu_xray_reports_preprocessed_df['findings'] = preprocess_text(iu_xray_reports_preprocessed_df['findings'])\n",
    "    iu_xray_reports_preprocessed_df.to_csv(iu_xray_reports_preprocessed_df_path, index=False)\n",
    "    print(f\"Saved preprocessed 'findings' column to: {iu_xray_reports_preprocessed_df_path}\")\n",
    "\n",
    "if preprocess_impression and 'impression' in iu_xray_reports_preprocessed_df.columns:\n",
    "    print(\"Preprocessing Column: impression\")\n",
    "    iu_xray_reports_preprocessed_df['impression'] = iu_xray_reports_preprocessed_df['impression'].fillna('none').astype(str)\n",
    "    iu_xray_reports_preprocessed_df['impression'] = preprocess_text(iu_xray_reports_preprocessed_df['impression'])\n",
    "    iu_xray_reports_preprocessed_df.to_csv(iu_xray_reports_preprocessed_df_path, index=False)\n",
    "    print(f\"Saved preprocessed 'impression' column to: {iu_xray_reports_preprocessed_df_path}\")\n",
    "\n",
    "\n",
    "# displaying the preprocessed dataframe\n",
    "iu_xray_reports_preprocessed_df = pd.read_csv(iu_xray_reports_preprocessed_df_path)\n",
    "display(iu_xray_reports_preprocessed_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MlOl6I0rMa68"
   },
   "source": [
    "### **Create Data Loaders**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Data Loaders to Supply Dataset to Model in Batches'''\n",
    "\n",
    "# classes in dataset\n",
    "class CustomImageDataset(Dataset):\n",
    "    def __init__(self, image_dir, transform=None):\n",
    "        self.image_dir = image_dir\n",
    "        self.transform = transform\n",
    "        self.image_files = [f for f in os.listdir(image_dir) if f.endswith(('.png', '.jpg', '.jpeg'))]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.image_dir, self.image_files[idx])\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image\n",
    "\n",
    "\n",
    "# function to use CustomImageDataset\n",
    "def load_preprocessed_images(image_dir, batch_size=32):\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),  \n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ])\n",
    "\n",
    "    dataset = CustomImageDataset(image_dir, transform=transform)\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n",
    "    return dataloader\n",
    "\n",
    "\n",
    "# loading images from iu_xray_images_preprocessed directory\n",
    "image_dir = iu_xray_images_preprocessed\n",
    "dataloader = load_preprocessed_images(image_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wQ0H-X3HMgS1"
   },
   "source": [
    "## **Model Implementation**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PsELnlXpMjGX"
   },
   "source": [
    "### **Visual Extractor**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "'''Visual Extractor to Extract Data from Image and Encode it Accordingly'''\n",
    "\n",
    "# defining device for gpu support\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "\n",
    "# defining the visual extractor model using ResNet101 \n",
    "class VisualExtractor(nn.Module):\n",
    "    def __init__(self, args):\n",
    "        super(VisualExtractor, self).__init__()\n",
    "        self.visual_extractor = args.visual_extractor\n",
    "        self.pretrained = args.visual_extractor_pretrained\n",
    "        model = getattr(models, self.visual_extractor)(pretrained=self.pretrained)\n",
    "        modules = list(model.children())[:-2]  \n",
    "        self.model = nn.Sequential(*modules)\n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.fc_layer = nn.Linear(model.fc.in_features, 2048) \n",
    "        \n",
    "    def forward(self, images):\n",
    "        patch_feats = self.model(images)\n",
    "        avg_feats = self.avg_pool(patch_feats).squeeze() \n",
    "        avg_feats = self.fc_layer(avg_feats)\n",
    "\n",
    "        batch_size, feat_size, _, _ = patch_feats.shape\n",
    "        patch_feats = patch_feats.view(batch_size, feat_size, -1).permute(0, 2, 1)\n",
    "        \n",
    "        final_embedding = torch.cat((avg_feats.unsqueeze(1), patch_feats), dim=1) \n",
    "        \n",
    "        return patch_feats, avg_feats, final_embedding\n",
    "\n",
    "\n",
    "# arguments for the visual extractor\n",
    "class Args:\n",
    "    visual_extractor = 'resnet101' \n",
    "    visual_extractor_pretrained = True\n",
    "\n",
    "\n",
    "# initializing the model\n",
    "args = Args()\n",
    "visual_extractor = VisualExtractor(args).to(device)\n",
    "\n",
    "\n",
    "# function to extract features from images\n",
    "def extract_features(dataloader):\n",
    "    all_patch_feats = []\n",
    "    all_avg_feats = []\n",
    "    all_final_embeddings = []\n",
    "\n",
    "    visual_extractor.eval() \n",
    "    with torch.no_grad():\n",
    "        for images in tqdm(dataloader):\n",
    "            patch_feats, avg_feats, final_embedding = visual_extractor(images)\n",
    "            all_patch_feats.append(patch_feats.cpu()) \n",
    "            all_avg_feats.append(avg_feats.cpu())\n",
    "            all_final_embeddings.append(final_embedding.cpu())\n",
    "\n",
    "    all_patch_feats = torch.cat(all_patch_feats, dim=0)\n",
    "    all_avg_feats = torch.cat(all_avg_feats, dim=0)\n",
    "    all_final_embeddings = torch.cat(all_final_embeddings, dim=0)\n",
    "\n",
    "    return all_patch_feats, all_avg_feats, all_final_embeddings\n",
    "\n",
    "\n",
    "# function to save extracted features\n",
    "def save_features(file_path, features):\n",
    "    print(f\"Saving features to {file_path}\")\n",
    "    torch.save(features, file_path)\n",
    "\n",
    "\n",
    "# function to lead the extracted features\n",
    "def load_features(file_path):\n",
    "    if os.path.exists(file_path):\n",
    "        print(f\"Loading features from {file_path}\")\n",
    "        return torch.load(file_path)\n",
    "    return None\n",
    "\n",
    "\n",
    "# initializing paths\n",
    "feature_dir = iu_xray\n",
    "patch_feats_file = os.path.join(feature_dir, 'patch_feats.pt')\n",
    "avg_feats_file = os.path.join(feature_dir, 'avg_feats.pt')\n",
    "final_embeddings_file = os.path.join(feature_dir, 'final_embeddings.pt')\n",
    "\n",
    "\n",
    "# extracting and saving the extracted features\n",
    "if os.path.exists(patch_feats_file) and os.path.exists(avg_feats_file) and os.path.exists(final_embeddings_file):\n",
    "    print(\"All features are already precomputed and will be loaded.\")\n",
    "    patch_feats = load_features(patch_feats_file)\n",
    "    avg_feats = load_features(avg_feats_file)\n",
    "    final_embeddings = load_features(final_embeddings_file)\n",
    "else:\n",
    "    print(\"Extracting features since they are not precomputed...\")\n",
    "    patch_feats, avg_feats, final_embeddings = extract_features(dataloader) \n",
    "        \n",
    "    os.makedirs(feature_dir, exist_ok=True)\n",
    "    save_features(patch_feats_file, patch_feats)\n",
    "    save_features(avg_feats_file, avg_feats)\n",
    "    save_features(final_embeddings_file, final_embeddings)\n",
    "\n",
    "\n",
    "# displaying sizes of the feature dataframes\n",
    "print(\"Patch Features Shape:\", patch_feats.shape)\n",
    "print(\"Average Features Shape:\", avg_feats.shape)\n",
    "print(\"Final Embedding Shape:\", final_embeddings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Visualizing Extracted Features using Plots'''\n",
    "\n",
    "# function to visualize features using PCA and t-SNE, including K-Means clustering\n",
    "def visualize_features(features, title):\n",
    "    pca = PCA(n_components=2)\n",
    "    pca_result = pca.fit_transform(features)\n",
    "\n",
    "    tsne = TSNE(n_components=2)\n",
    "    tsne_result = tsne.fit_transform(features)\n",
    "\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.scatter(pca_result[:, 0], pca_result[:, 1], alpha=0.5)\n",
    "    plt.title(f\"PCA - {title}\")\n",
    "    plt.xlabel(\"PCA Component 1\")\n",
    "    plt.ylabel(\"PCA Component 2\")\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.scatter(tsne_result[:, 0], tsne_result[:, 1], alpha=0.5)\n",
    "    plt.title(f\"t-SNE - {title}\")\n",
    "    plt.xlabel(\"t-SNE Component 1\")\n",
    "    plt.ylabel(\"t-SNE Component 2\")\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "    num_clusters = 3\n",
    "    kmeans = KMeans(n_clusters=num_clusters)\n",
    "    labels = kmeans.fit_predict(features)\n",
    "    \n",
    "    plt.figure(figsize=(8, 8))\n",
    "    plt.scatter(tsne_result[:, 0], tsne_result[:, 1], c=labels, cmap='viridis', alpha=0.5)\n",
    "    plt.title('t-SNE Result with K-Means Clusters')\n",
    "    plt.colorbar()\n",
    "    plt.show()\n",
    "    plt.figure(figsize=(8, 8))\n",
    "    plt.scatter(pca_result[:, 0], pca_result[:, 1], c=labels, cmap='viridis', alpha=0.5)\n",
    "    plt.title('PCA Result with K-Means Clusters')\n",
    "    plt.colorbar()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# function to visualize features using PCA and t-SNE, without clustering\n",
    "def visualize_features_2(features, title):\n",
    "    print(f\"Original feature shape: {features.shape}\")\n",
    "    \n",
    "    flattened_features = features.reshape(features.shape[0], -1) \n",
    "    \n",
    "    pca = PCA(n_components=2)\n",
    "    pca_result = pca.fit_transform(flattened_features)\n",
    "\n",
    "    tsne = TSNE(n_components=2)\n",
    "    tsne_result = tsne.fit_transform(flattened_features)\n",
    "\n",
    "    plt.scatter(pca_result[:, 0], pca_result[:, 1], alpha=0.7)\n",
    "    plt.title(f'PCA: {title}')\n",
    "    plt.show()\n",
    "    \n",
    "    plt.scatter(tsne_result[:, 0], tsne_result[:, 1], alpha=0.7)\n",
    "    plt.title(f't-SNE: {title}')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# visualizing average features\n",
    "visualize_features(avg_feats.numpy(), \"Average Features\")\n",
    "visualize_features(patch_feats.numpy(), \"Patch Features\")\n",
    "visualize_features_2(final_embeddings.numpy(), \"Final Embeddings\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D2kBZTiCMmsE"
   },
   "source": [
    "### **Text Encoder**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Text Encoder - Encoding Dictionary, Encoding Medical History and Finding Cosine Similarity Between Them '''\n",
    "\n",
    "# reading medical reports collected and validating\n",
    "iu_xray_reports_preprocessed_df_path = os.path.join(download_path, 'iu_xray/iu_xray_reports_preprocessed_df.csv')\n",
    "medical_history = pd.read_csv(iu_xray_reports_preprocessed_df_path)\n",
    "medical_history = medical_history[\"findings\"].dropna().tolist()  \n",
    "\n",
    "medical_history = [str(report) for report in medical_history if isinstance(report, str) or pd.notna(report)]\n",
    "\n",
    "\n",
    "# creating dictionary with key as organ: chest\n",
    "radiology_dictionary = {\n",
    "    \"chest\": [\"small\", \"enlarged\", \"unchanged\", \"stable\", \"silhouette\", \"contours\", \"size\", \"focal\", \"mild\", \"acute\"]\n",
    "}\n",
    "\n",
    "\n",
    "# using pre-trained model to embed into numerical space\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "\n",
    "# function to embed text\n",
    "def embed_text(texts):\n",
    "    inputs = tokenizer(texts, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "    outputs = model(**inputs)\n",
    "    return outputs.last_hidden_state.mean(dim=1)\n",
    "\n",
    "\n",
    "# embedding process\n",
    "dictionary_embeddings = embed_text(radiology_dictionary[\"chest\"] )\n",
    "report_embeddings = embed_text(medical_history) \n",
    "\n",
    "\n",
    "# calculating cosine similarities\n",
    "similarities = torch.mm(report_embeddings, dictionary_embeddings.T)\n",
    "\n",
    "\n",
    "# finding top k relevant entries\n",
    "k = 5\n",
    "top_k_indices = similarities.topk(k=k, dim=1).indices\n",
    "relevant_entries = [[radiology_dictionary[idx] for idx in row] for row in top_k_indices]\n",
    "\n",
    "\n",
    "# printing relevant entries of each report\n",
    "for i, entries in enumerate(relevant_entries):\n",
    "    print(f\"Report {i+1}: Relevant Entries: {entries}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gIYzXGtsMqmQ"
   },
   "source": [
    "### **Multilevel Alignment**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Uz4pEhW2MuBF"
   },
   "source": [
    "### **Report Generator**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z_WeRBlDMwX2"
   },
   "source": [
    "### **Complete Model**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NZjhDeJxMzgg"
   },
   "source": [
    "## **Training**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7Uk_G0K8M2H9"
   },
   "source": [
    "### **Training**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OXhkaJVvM3Uu"
   },
   "source": [
    "## **Testing**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bzTZZUlQM401"
   },
   "source": [
    "### **Testing**"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "MlOl6I0rMa68",
    "PsELnlXpMjGX",
    "D2kBZTiCMmsE",
    "gIYzXGtsMqmQ",
    "Uz4pEhW2MuBF",
    "Z_WeRBlDMwX2",
    "7Uk_G0K8M2H9",
    "OXhkaJVvM3Uu"
   ],
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
