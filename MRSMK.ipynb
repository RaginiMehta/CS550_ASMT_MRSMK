{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bnpd7Zh0NPl2"
      },
      "source": [
        "## **Medical Report Summarisation using Medical Knowledge**\n",
        "\n",
        "### **References**\n",
        "\n",
        "**Main Reference**\n",
        "- Radiology report generation with medical knowledge and multilevel image-report alignment: A new method and its verification\n",
        "https://www.sciencedirect.com/science/article/pii/S0933365723002282#bib1\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oKdcyk1sMEtD"
      },
      "source": [
        "## **Data Collection**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p81rfgckMR9a"
      },
      "source": [
        "### **Collect Datasets**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AYR_B6u1ojJt"
      },
      "outputs": [],
      "source": [
        "'''Libraries Installation and Import'''\n",
        "\n",
        "# install necessary libraries\n",
        "!pip install Pillow\n",
        "!pip install torchvision\n",
        "!pip install nltk\n",
        "!pip install pyspellchecker\n",
        "!pip install tqdm\n",
        "!pip install opencv-python\n",
        "\n",
        "# importing required libraries\n",
        "import os\n",
        "import requests\n",
        "import tarfile\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import xml.etree.ElementTree as ET\n",
        "from tqdm import tqdm\n",
        "from PIL import Image\n",
        "import torchvision.transforms as transforms\n",
        "import cv2\n",
        "import re\n",
        "import nltk\n",
        "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
        "from nltk.corpus import stopwords\n",
        "from spellchecker import SpellChecker"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "49-0xiXplwH8"
      },
      "outputs": [],
      "source": [
        "'''Setup - Generalized'''\n",
        "\n",
        "# setup to download the IU X-Ray Dataset\n",
        "dataset = 'iu_xray/'\n",
        "download_path = os.path.join('./datasets', dataset)\n",
        "\n",
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')\n",
        "# download_path = os.path.join('/content/drive/MyDrive/Academics/CS550 Machine Learning/CS550 ASMT MRSMK/datasets', dataset)\n",
        "\n",
        "images_dir = os.path.join(download_path, \"images\")\n",
        "reports_dir = os.path.join(download_path, \"reports\")\n",
        "\n",
        "images_url = \"https://openi.nlm.nih.gov/imgs/collections/NLMCXR_png.tgz\"\n",
        "reports_url = \"https://openi.nlm.nih.gov/imgs/collections/NLMCXR_reports.tgz\"\n",
        "\n",
        "\n",
        "# function to check the file size of a given URL\n",
        "def get_file_size(url):\n",
        "    response = requests.head(url)\n",
        "    size_in_bytes = int(response.headers.get('Content-Length', 0))\n",
        "    size_in_mb = size_in_bytes / (1024 * 1024)\n",
        "    return size_in_mb\n",
        "\n",
        "\n",
        "# function to download and extract from a given url to a given directory\n",
        "def download_and_extract(url, save_dir):\n",
        "    file_name = url.split('/')[-1]\n",
        "    file_path = os.path.join(save_dir, file_name)\n",
        "\n",
        "    response = requests.get(url, stream=True)\n",
        "    total_size = int(response.headers.get('Content-Length', 0))\n",
        "    downloaded_size = 0\n",
        "\n",
        "    with open(file_path, 'wb') as file:\n",
        "        for chunk in response.iter_content(chunk_size=1024):\n",
        "            if chunk:\n",
        "                file.write(chunk)\n",
        "                downloaded_size += len(chunk)\n",
        "                percent_complete = (downloaded_size / total_size) * 100\n",
        "                print(f\"Downloaded {downloaded_size / (1024*1024):.2f} MB out of {total_size / (1024*1024):.2f} MB: {percent_complete:.2f}% complete\")\n",
        "\n",
        "    print(\"\\nDownload complete!\")\n",
        "\n",
        "    with tarfile.open(file_path, 'r:gz') as tar:\n",
        "        members = tar.getmembers()\n",
        "        total_files = len(members)\n",
        "\n",
        "        for idx, member in enumerate(members, start=1):\n",
        "            tar.extract(member, path=save_dir)\n",
        "            print(f\"Extracting File {idx} out of {total_files}: {member.name}\")\n",
        "\n",
        "    os.remove(file_path)\n",
        "\n",
        "\n",
        "# downloading  IU X-Ray dataset\n",
        "if not os.path.exists(images_dir):\n",
        "    images_size = get_file_size(images_url)\n",
        "    print(f\"Downloading {images_url} to: {images_dir} ({images_size:.2f} MB)\")\n",
        "    os.makedirs(images_dir, exist_ok=True)\n",
        "    download_and_extract(images_url, images_dir)\n",
        "    print(f\"Downloaded {images_url} to: {images_dir}\")\n",
        "else:\n",
        "    print(f\"{images_url} already exists at: {images_dir}\")\n",
        "\n",
        "if not os.path.exists(reports_dir):\n",
        "    reports_size = get_file_size(reports_url)\n",
        "    print(f\"Downloading {reports_url} to: {reports_dir} ({reports_size:.2f} MB)\")\n",
        "    os.makedirs(reports_dir, exist_ok=True)\n",
        "    download_and_extract(reports_url, reports_dir)\n",
        "    print(f\"Downloaded {reports_url} to: {reports_dir}\")\n",
        "else:\n",
        "    print(f\"{reports_url} already exists at: {reports_dir}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5ZSA8Jyowaoh"
      },
      "outputs": [],
      "source": [
        "'''Exploring the IU X-Ray Dataset Contents'''\n",
        "\n",
        "# displaying directory and subdirectory contents\n",
        "iu_xray = download_path\n",
        "print(\"\\nPath: \", iu_xray)\n",
        "print(f\"Directory Contents: {os.listdir(iu_xray)}\")\n",
        "\n",
        "iu_xray_images = images_dir\n",
        "print(\"\\nPath: \", iu_xray_images)\n",
        "print(f\"Directory Contents: {len(os.listdir(iu_xray_images))} Images\")\n",
        "\n",
        "iu_xray_reports = os.path.join(reports_dir, 'ecgen-radiology')\n",
        "print(\"\\nPath: \", iu_xray_reports)\n",
        "print(f\"Directory Contents: {len(os.listdir(iu_xray_reports))} Reports\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qGczexPLUaN5"
      },
      "outputs": [],
      "source": [
        "'''Processing Textual Data from each .xml Report File and Storing it in a .csv File'''\n",
        "\n",
        "# function to iterate through all .xml report files and storing them in a dataframe\n",
        "def save_images_df():\n",
        "    data = []\n",
        "    cnt = 0\n",
        "    for file in os.listdir(iu_xray_reports):\n",
        "        if file.endswith(\".xml\"):\n",
        "            cnt += 1\n",
        "            print(f\"Processing .xml File {cnt} out of {len(os.listdir(iu_xray_reports))}: {file}\")\n",
        "\n",
        "            file_path = os.path.join(iu_xray_reports, file)\n",
        "            try:\n",
        "                tree = ET.parse(file_path)\n",
        "                root = tree.getroot()\n",
        "\n",
        "                pmc_id = root.find('.//pmcId').attrib.get('id')\n",
        "\n",
        "                comparison = indication = findings = impression = None\n",
        "\n",
        "                for abstract in root.findall('.//AbstractText'):\n",
        "                    if abstract.attrib.get('Label') == 'COMPARISON':\n",
        "                        comparison = abstract.text\n",
        "                    elif abstract.attrib.get('Label') == 'INDICATION':\n",
        "                        indication = abstract.text\n",
        "                    elif abstract.attrib.get('Label') == 'FINDINGS':\n",
        "                        findings = abstract.text\n",
        "                    elif abstract.attrib.get('Label') == 'IMPRESSION':\n",
        "                        impression = abstract.text\n",
        "\n",
        "                for parent_image in root.findall('parentImage'):\n",
        "                    image_file = parent_image.attrib['id'] + \".png\"\n",
        "                    image_path = os.path.join(iu_xray_images, image_file)\n",
        "                    image = cv2.imread(image_path)\n",
        "\n",
        "                    if image is not None:\n",
        "                        height, width, channels = image.shape\n",
        "                        caption = parent_image.find('caption').text if parent_image.find('caption') is not None else None\n",
        "                        data.append([pmc_id, image_file, caption, comparison, indication, findings, impression, height, width])\n",
        "                    else:\n",
        "                        print(f\"Warning: Unable to read image {image_path}\")\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"Error processing file {file}: {e}\")\n",
        "\n",
        "    return data\n",
        "\n",
        "\n",
        "# create a dataframe and save it as csv\n",
        "iu_xray_images_df_path = os.path.join(iu_xray, 'iu_xray_images_df.csv')\n",
        "if not os.path.exists(iu_xray_images_df_path):\n",
        "    data = save_images_df()\n",
        "    columns = ['pmc_id', 'image_filename', 'caption', 'comparison', 'indication', 'findings', 'impression', 'height', 'width']\n",
        "    iu_xray_images_df = pd.DataFrame(data, columns=columns)\n",
        "    iu_xray_images_df.to_csv(iu_xray_images_df_path, index=False)\n",
        "    print(f\"Dataframe saved to {iu_xray_images_df_path}\")\n",
        "else:\n",
        "    print(f\"Dataframe already exists at {iu_xray_images_df_path}\")\n",
        "    iu_xray_images_df = pd.read_csv(iu_xray_images_df_path)\n",
        "\n",
        "\n",
        "# display the stored dataframe\n",
        "print(\"\\n\\nDataframe Shape:\", iu_xray_images_df.shape)\n",
        "\n",
        "print(\"\\n\\nDataframe Information:\\n\")\n",
        "display(iu_xray_images_df.info())\n",
        "\n",
        "print(\"\\n\\nDisplaying Dataframe:\\n\")\n",
        "display(iu_xray_images_df.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vYnfzXXT0O6w"
      },
      "outputs": [],
      "source": [
        "'''Processing Textual Data from each .xml Report File and Storing it in a .csv File'''\n",
        "\n",
        "# function to iterate through all .xml report files and storing them in a dataframe\n",
        "def save_reports_df():\n",
        "    data = []\n",
        "    cnt = 0\n",
        "    for file in os.listdir(iu_xray_reports):\n",
        "        if file.endswith(\".xml\"):\n",
        "            cnt += 1\n",
        "            print(f\"Processing .xml File {cnt} out of {len(os.listdir(iu_xray_reports))}: {file}\")\n",
        "\n",
        "            file_path = os.path.join(iu_xray_reports, file)\n",
        "            try:\n",
        "                tree = ET.parse(file_path)\n",
        "                root = tree.getroot()\n",
        "\n",
        "                pmc_id = root.find('.//pmcId').attrib.get('id')\n",
        "\n",
        "                comparison = indication = findings = impression = None\n",
        "\n",
        "                for abstract in root.findall('.//AbstractText'):\n",
        "                    if abstract.attrib.get('Label') == 'COMPARISON':\n",
        "                        comparison = abstract.text\n",
        "                    elif abstract.attrib.get('Label') == 'INDICATION':\n",
        "                        indication = abstract.text\n",
        "                    elif abstract.attrib.get('Label') == 'FINDINGS':\n",
        "                        findings = abstract.text\n",
        "                    elif abstract.attrib.get('Label') == 'IMPRESSION':\n",
        "                        impression = abstract.text\n",
        "\n",
        "                report_data = {\n",
        "                    'pmc_id': pmc_id,\n",
        "                    'findings': findings,\n",
        "                    'impression': impression,\n",
        "                    'comparison': comparison,\n",
        "                    'indication': indication,\n",
        "                }\n",
        "\n",
        "                parent_images = root.findall('parentImage')\n",
        "                report_data['image_count'] = len(parent_images)\n",
        "\n",
        "                for i, parent_image in enumerate(parent_images, start=1):\n",
        "                    image_file = parent_image.attrib['id'] + \".jpg\"\n",
        "                    caption = parent_image.find('caption').text if parent_image.find('caption') is not None else None\n",
        "                    report_data[f'image_{i}'] = f\"{image_file}: {caption}\" if caption else image_file\n",
        "\n",
        "                data.append(report_data)\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"Error processing file {file}: {e}\")\n",
        "\n",
        "    return data\n",
        "\n",
        "\n",
        "# create a dataframe and save it as csv\n",
        "iu_xray_reports_df_path = os.path.join(iu_xray, 'iu_xray_reports_df.csv')\n",
        "if not os.path.exists(iu_xray_reports_df_path):\n",
        "    data = save_reports_df()\n",
        "    iu_xray_reports_df = pd.DataFrame(data)\n",
        "    iu_xray_reports_df.to_csv(iu_xray_reports_df_path, index=False)\n",
        "    print(f\"Dataframe saved to {iu_xray_reports_df_path}\")\n",
        "else:\n",
        "    print(f\"Dataframe already exists at {iu_xray_reports_df_path}\")\n",
        "    iu_xray_reports_df = pd.read_csv(iu_xray_reports_df_path)\n",
        "\n",
        "\n",
        "# display the stored dataframe\n",
        "print(\"\\n\\nDataframe Shape:\", iu_xray_reports_df.shape)\n",
        "\n",
        "print(\"\\n\\nDataframe Information:\\n\")\n",
        "display(iu_xray_reports_df.info())\n",
        "\n",
        "print(\"\\n\\nDisplaying Dataframe:\\n\")\n",
        "display(iu_xray_reports_df.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bw4Ylfa94M1o"
      },
      "outputs": [],
      "source": [
        "'''Displaying the Number of Images per Report'''\n",
        "\n",
        "# displaying the distribution of number of images per report\n",
        "reports_count = iu_xray_reports_df['image_count'].value_counts().rename_axis('images_qty').reset_index(name='reports_count')\n",
        "print(\"\\n\\nNumber of Images per Report:\\n\")\n",
        "display(reports_count)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6UouFxQwMNeo"
      },
      "source": [
        "## **Data Preprocessing**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-cGf99_CMV47"
      },
      "source": [
        "### **Preprocess Images**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JTClOSxeSCOM"
      },
      "outputs": [],
      "source": [
        "'''Preprocessing Images - Resizing, Tensor Conversion and Normalization'''\n",
        "\n",
        "# function to find minimum dimensions of given set of images\n",
        "def find_min_dimensions(image_dir):\n",
        "    min_width = float('inf')\n",
        "    min_height = float('inf')\n",
        "\n",
        "    for filename in os.listdir(image_dir):\n",
        "        if filename.endswith('.png'):\n",
        "            img_path = os.path.join(image_dir, filename)\n",
        "            with Image.open(img_path) as img:\n",
        "                width, height = img.size\n",
        "                min_width = min(min_width, width)\n",
        "                min_height = min(min_height, height)\n",
        "\n",
        "    return min_width, min_height\n",
        "\n",
        "\n",
        "# function to preprocess and save images\n",
        "def preprocess_images(input_dir, output_dir):\n",
        "    min_width, min_height = find_min_dimensions(iu_xray_images)\n",
        "    print(f'Minimum Width: {min_width}, Minimum Height: {min_height}\\n')\n",
        "\n",
        "    preprocess = transforms.Compose([\n",
        "        transforms.Resize((min_width, min_height)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "    ])\n",
        "\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "    cnt = 0\n",
        "    for filename in os.listdir(input_dir):\n",
        "        if filename.endswith('.png'):\n",
        "            cnt += 1\n",
        "            print(f\"Preprocessing File {cnt} out of {len(os.listdir(input_dir))}: {filename}\")\n",
        "\n",
        "            image_path = os.path.join(input_dir, filename)\n",
        "            image = Image.open(image_path).convert('RGB')\n",
        "            processed_image = preprocess(image)\n",
        "\n",
        "            processed_image_path = os.path.join(output_dir, filename)\n",
        "\n",
        "            processed_image_pil = transforms.ToPILImage()(processed_image)\n",
        "            processed_image_pil.save(processed_image_path)\n",
        "\n",
        "\n",
        "# preprocessing images\n",
        "iu_xray_images_preprocessed = os.path.join(iu_xray, 'images_preprocessed')\n",
        "if not os.path.exists(iu_xray_images_preprocessed):\n",
        "    print(f\"Preprocessing Images to: {iu_xray_images_preprocessed}\")\n",
        "    preprocess_images(iu_xray_images, iu_xray_images_preprocessed)\n",
        "    print(f\"Preprocessed Images saved to: {iu_xray_images_preprocessed}\")\n",
        "else:\n",
        "    print(f\"Preprocessed Images already exist at: {iu_xray_images_preprocessed}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lvdRoNqXMZlN"
      },
      "source": [
        "### **Preprocess Text**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4plVKo-vXewj"
      },
      "outputs": [],
      "source": [
        "'''Preprocessing Text - Lowercasing, Decontracting, Punctuation Removal, Number Removal, Two-Letter Word Removal, Stop Word Removal, Negation Handling, Spell Checking, Extra Space Removal'''\n",
        "\n",
        "# download nltk resources and initialize spell checker\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('stopwords')\n",
        "spell = SpellChecker()\n",
        "\n",
        "\n",
        "# function to convert text to lowercase\n",
        "def lowercase(text):\n",
        "    return text.lower() if isinstance(text, str) else text\n",
        "\n",
        "\n",
        "# function to decontract words\n",
        "def decontracted(text):\n",
        "    if not isinstance(text, str):\n",
        "        return text\n",
        "    contractions = {\n",
        "        \"won't\": \"will not\", \"can't\": \"can not\", \"couldn't\": \"could not\",\n",
        "        \"shouldn't\": \"should not\", \"wouldn't\": \"would not\", \"n't\": \" not\",\n",
        "        \"'re\": \" are\", \"'s\": \" is\", \"'d\": \" would\", \"'ll\": \" will\",\n",
        "        \"'t\": \" not\", \"'ve\": \" have\", \"'m\": \" am\"\n",
        "    }\n",
        "    for contraction, full_form in contractions.items():\n",
        "        text = text.replace(contraction, full_form)\n",
        "    return text\n",
        "\n",
        "\n",
        "# function to remove punctuations\n",
        "def rem_punctuations(text):\n",
        "    return re.sub(r'[^\\w\\s]', '', text) if isinstance(text, str) else text\n",
        "\n",
        "\n",
        "# function to remove numbers\n",
        "def rem_numbers(text):\n",
        "    return re.sub(r'\\d+', '', text) if isinstance(text, str) else text\n",
        "\n",
        "\n",
        "# function to remove two-letter words except \"no\" and \"ct\"\n",
        "def rem_two_letter_words(text):\n",
        "    if not isinstance(text, str):\n",
        "        return text\n",
        "    return ' '.join(word for word in text.split() if len(word) > 2 or word in [\"no\", \"ct\"])\n",
        "\n",
        "\n",
        "# function to remove stop words\n",
        "def rem_stop_words(text):\n",
        "    if not isinstance(text, str):\n",
        "        return text\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    return ' '.join(word for word in text.split() if word not in stop_words)\n",
        "\n",
        "\n",
        "# function to handle negations\n",
        "def handle_negations(text):\n",
        "    if not isinstance(text, str):\n",
        "        return text\n",
        "    negations = {\"no\": \"not\", \"not\": \"not\"}\n",
        "    return ' '.join(negations.get(word, word) for word in text.split())\n",
        "\n",
        "\n",
        "# function to correct spelling\n",
        "def correct_spelling(text):\n",
        "    if not isinstance(text, str):\n",
        "        return text\n",
        "    corrected = []\n",
        "    for word in text.split():\n",
        "        corrected_word = list(spell.candidates(word))[0] if spell.candidates(word) else word\n",
        "        corrected.append(corrected_word)\n",
        "    return ' '.join(corrected)\n",
        "\n",
        "\n",
        "# function to remove extra spaces\n",
        "def rem_extra_spaces(text):\n",
        "    return ' '.join(text.split()) if isinstance(text, str) else text\n",
        "\n",
        "\n",
        "# function to preprocess text\n",
        "def preprocess_text(data):\n",
        "    preprocessed = []\n",
        "    for sentence in tqdm(data.values):\n",
        "        sentence = str(sentence) \n",
        "        sentence = lowercase(sentence)\n",
        "        sentence = decontracted(sentence)\n",
        "        sentence = rem_punctuations(sentence)\n",
        "        sentence = rem_numbers(sentence)\n",
        "        sentence = rem_two_letter_words(sentence)\n",
        "        sentence = rem_stop_words(sentence)\n",
        "        sentence = handle_negations(sentence)\n",
        "        sentence = correct_spelling(sentence)\n",
        "        sentence = rem_extra_spaces(sentence)\n",
        "\n",
        "        preprocessed.append(sentence)\n",
        "\n",
        "    return preprocessed\n",
        "\n",
        "\n",
        "# function to preprocess text and save the corresponding dataframe\n",
        "def preprocess_and_save_dataframe(dataframe, path):\n",
        "    columns_to_preprocess = {\n",
        "        'caption': 'unknown',\n",
        "        'comparison': 'no comparison',\n",
        "        'indication': 'no indication',\n",
        "        'findings': 'no findings',\n",
        "        'impression': 'no impression'\n",
        "    }\n",
        "\n",
        "    for column, fill_value in columns_to_preprocess.items():\n",
        "        if column in dataframe.columns:\n",
        "            print(f\"Preprocessing Column: {column}\")\n",
        "            dataframe.loc[:, column] = dataframe[column].fillna(fill_value).astype(str)\n",
        "            dataframe.loc[:, column] = preprocess_text(dataframe[column])\n",
        "\n",
        "    dataframe.to_csv(path, index=False)\n",
        "\n",
        "    return dataframe\n",
        "\n",
        "\n",
        "# save and display the preprocessed dataframe for images\n",
        "# iu_xray_images_preprocessed_df_path = os.path.join(iu_xray, 'iu_xray_images_preprocessed_df.csv')\n",
        "# if not os.path.exists(iu_xray_images_preprocessed_df_path):\n",
        "#     print(f\"Preprocessing Text DataFrame {iu_xray_images_df_path} to: {iu_xray_images_preprocessed_df_path}\")\n",
        "#     iu_xray_images_preprocessed_df = preprocess_and_save_dataframe(iu_xray_images_df, iu_xray_images_preprocessed_df_path)\n",
        "#     print(f\"Preprocessed Text DataFrame {iu_xray_images_df_path} saved to: {iu_xray_images_preprocessed_df_path}\")\n",
        "# else:\n",
        "#     print(f\"Preprocessed Text DataFrame {iu_xray_images_df_path} already exists at: {iu_xray_images_preprocessed_df_path}\")\n",
        "#     iu_xray_images_preprocessed_df = pd.read_csv(iu_xray_images_preprocessed_df_path)\n",
        "# display(iu_xray_images_preprocessed_df.head())\n",
        "\n",
        "\n",
        "# save and display the preprocessed dataframe for reports\n",
        "iu_xray_reports_preprocessed_df_path = os.path.join(iu_xray, 'iu_xray_reports_preprocessed_df.csv')\n",
        "if not os.path.exists(iu_xray_reports_preprocessed_df_path):\n",
        "    print(f\"Preprocessing Text DataFrame {iu_xray_reports_df_path} to: {iu_xray_reports_preprocessed_df_path}\")\n",
        "    iu_xray_reports_preprocessed_df = preprocess_and_save_dataframe(iu_xray_reports_df, iu_xray_reports_preprocessed_df_path)\n",
        "    print(f\"Preprocessed Text DataFrame {iu_xray_reports_df_path} saved to: {iu_xray_reports_preprocessed_df_path}\")\n",
        "else:\n",
        "    print(f\"Preprocessed Text DataFrame {iu_xray_reports_df_path} already exists at: {iu_xray_reports_preprocessed_df_path}\")\n",
        "    iu_xray_reports_preprocessed_df = pd.read_csv(iu_xray_reports_preprocessed_df_path)\n",
        "display(iu_xray_reports_preprocessed_df.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MlOl6I0rMa68"
      },
      "source": [
        "### **Create Data Loaders**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wQ0H-X3HMgS1"
      },
      "source": [
        "## **Model Implementation**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PsELnlXpMjGX"
      },
      "source": [
        "### **Visual Extractor**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D2kBZTiCMmsE"
      },
      "source": [
        "### **Text Encoder**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gIYzXGtsMqmQ"
      },
      "source": [
        "### **Multilevel Alignment**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uz4pEhW2MuBF"
      },
      "source": [
        "### **Report Generator**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z_WeRBlDMwX2"
      },
      "source": [
        "### **Complete Model**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NZjhDeJxMzgg"
      },
      "source": [
        "## **Training**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Uk_G0K8M2H9"
      },
      "source": [
        "### **Training**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OXhkaJVvM3Uu"
      },
      "source": [
        "## **Testing**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bzTZZUlQM401"
      },
      "source": [
        "### **Testing**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_jlINFMp9H_a"
      },
      "source": [
        "## **Dataset Download as Zip File**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aIYBkyBlmm_l"
      },
      "outputs": [],
      "source": [
        "'''Downloading Dataset Directory with all Changes'''\n",
        "\n",
        "# importing required libraries\n",
        "import shutil\n",
        "import os\n",
        "from google.colab import files\n",
        "\n",
        "\n",
        "# zipping and downloading the archive\n",
        "zip_filename = 'IUXR.zip'\n",
        "shutil.make_archive(zip_filename[:-4], 'zip', download_path)\n",
        "files.download(zip_filename)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "MlOl6I0rMa68",
        "PsELnlXpMjGX",
        "D2kBZTiCMmsE",
        "gIYzXGtsMqmQ",
        "Uz4pEhW2MuBF",
        "Z_WeRBlDMwX2",
        "7Uk_G0K8M2H9",
        "OXhkaJVvM3Uu"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
